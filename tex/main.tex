%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[acmsmall]{acmart}\settopmatter{}

%% Journal information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmJournal{PACMPL}
\acmVolume{1}
\acmNumber{POPL} % CONF = POPL or ICFP or OOPSLA
\acmArticle{1}
\acmYear{2021}
\acmMonth{1}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%% Note: author/year citations are required for papers published as an
%% issue of PACMPL.
\citestyle{acmauthoryear}   %% For author/year citations


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from PACMPL format to traditional
%% SIGPLAN proceedings format must update the '\documentclass' and
%% topmatter commands above; see 'acmart-sigplanproc-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\input{macros}
\input{examples}
\input{example1}

\newcommand{\powerset}{\raisebox{.15\baselineskip}{\Large\ensuremath{\wp}}}

\begin{document}

%% Title information
\title{Stack Safety is a Security Property}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
%                                        %% can be repeated if necessary;
%                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
%                                        %% can be repeated if necessary;
%                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
%\author{First1 Last1}
%\authornote{with author1 note}          %% \authornote is optional;
%                                        %% can be repeated if necessary
%\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
%\affiliation{
%  \position{Position1}
%  \department{Department1}              %% \department is recommended
%  \institution{Institution1}            %% \institution is required
%  \streetaddress{Street1 Address1}
%  \city{City1}
%  \state{State1}
%  \postcode{Post-Code1}
%  \country{Country1}                    %% \country is recommended
%}
%\email{first1.last1@inst1.edu}          %% \email is recommended
%
%%% Author with two affiliations and emails.
%\author{First2 Last2}
%\authornote{with author2 note}          %% \authornote is optional;
%                                        %% can be repeated if necessary
%\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
%\affiliation{
%  \position{Position2a}
%  \department{Department2a}             %% \department is recommended
%  \institution{Institution2a}           %% \institution is required
%  \streetaddress{Street2a Address2a}
%  \city{City2a}
%  \state{State2a}
%  \postcode{Post-Code2a}
%  \country{Country2a}                   %% \country is recommended
%}
%\email{first2.last2@inst2a.com}         %% \email is recommended
%\affiliation{
%  \position{Position2b}
%  \department{Department2b}             %% \department is recommended
%  \institution{Institution2b}           %% \institution is required
%  \streetaddress{Street3b Address2b}
%  \city{City2b}
%  \state{State2b}
%  \postcode{Post-Code2b}
%  \country{Country2b}                   %% \country is recommended
%}
%\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
\apt{maybe: ``Stack safety is a testable security property''?}\bcp{That
  would sacrifice a lot of oomph for a little bit of precision, IMO.  Prefer
  to leave it out of the title but hit it strongly in the abstract.}%
What is stack safety? The phrase is associated with a variety of compiler,
run-time, and hardware mechanisms for guarding against corruption of stack
memory.  But these mechanisms mostly lack rigorous specifications, appealing
either to intuitive expectations of how good stack-using programs should
behave or to specific examples of bad behavior.

We propose a formal criterion for stack safety, phrased in terms of
language-based security: a combination of an integrity property (``each
caller's stack frame is invariant by the callee'') and a confidentiality
property (``the callee's behavior is insensitive to differences in the
caller's stack frame''). These combine with a straightforward
well-bracketedness condition on the program's control flow that ensures that
callers and callees remain meaningfully distinct.\bcp{I am still not clear
  how well-bracketedness fits into the story.  (For me, it is the bracketing
  that {\em defines} where calls and returns happen, so there is something
  tautological about saying that `` well-bracketedness condition ensures
  that callers and callees remain meaningfully distinct''!)}

Our criterion is inspired by hardware tagging mechanisms for stack
protection, in particular the stack-safety ``micro-policies''
of~\citet{DBLP:conf/sp/RoesslerD18}, but its core reflects general
principles of stack safety\bcp{I wish we did not have to be so apologetic
  about being tied to micro-policies (and it is still not clear to me that
  we have to be... do we have a clear statement someplace of why our
  definition would not apply to CFI, for example? This seems like a critical
  discussion...)}. We define additional properties tailored for specific
contexts: a stronger property amenable to efficient testing and a weaker one
that is approximated by more efficient, lazy policies.

\bcp{... and now a sentence on testing!}

  \ifaftersubmission\bcp{should we mention stktokens here?}
  \leo{Not sure. We almost never cite stuff in the abstract, do we?}
  \bcp{Some people say not to, but I think their argument is weak: I've
    added Nick/Andre above.  But I'm not sure we want to get into stkTokens
    in the abstract.}
  \fi
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

%\ccsdesc[500]{Software and its engineering~General programming languages}
%\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{Stack Safety, Micro-Policies}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle

\section{Introduction}

\newcommand*{\MemoryLabel}[3]{\raisebox{#2}{\makebox(0,0){\hspace{#1}#3}}}
\newcommand{\memoryaddrs}[1]
  {
    \MemoryLabel{#1}{2em}{\SP}
  }

%\begin{figure}
%  \begin{minipage}{\textwidth}
%    \begin{center}
%\MemoryLabel{10em}{1.5em}{40}
%\MemoryLabel{16em}{1.5em}{60}
%\MemoryLabel{22em}{1.5em}{80}
%$\cdots$
%\memory{4}{gray}%
%\memory{5}{green}[{\makebox[0pt]{Caller}}]%
%\memory{5}{yellow}[{\makebox[0pt]{Args}}]%
%\memory{8}{red}[{\makebox[0pt]{Callee}}]%
%\memory{4}{\unusedc}%
%~$\cdots$\\
%  \end{center}
%  \end{minipage}\\
%~\\
%~\\
%  \begin{minipage}{0.45\textwidth}
%    \begin{center}
%\begin{verbatim}
%jal ...
%lw r1 r0 40
%\end{verbatim}
%    \end{center}
%  \end{minipage}~
%  \begin{minipage}{0.45\textwidth}
%    \begin{center}
%\begin{verbatim}
%jal ...
%sw r0 r0 40
%\end{verbatim}
%    \end{center}
%  \end{minipage}
%  \caption{Examples of Unsafe Programs\bcp{The numbers don't line up with
%      the boxes; the program fragments need comments explaining what each
%      line does.}}
%  \label{fig:stackunsafety}
%\end{figure}
%
%\leo{I hate the writing here, but I wanted to make a start.}

%\bcp{We don't actually say very clearly anywhere that we are interested in
%  low-level enforcement mechanisms for machine-code programs (because this
%  is where stack attacks occur, and because at higher levels of abstraction
%  the stack is a built-in concept, not something that can be attacked even
%  in principle).  This could cause confusion.\apt{not so worried}}
%\leo{I agree. I've clarified the writing in the top of the running example
%  a bit to make sure though}

The call stack is a perennial target for low-level attacks
based on memory-safety vulnerabilities, with consequences
ranging from leakage or corruption of private stack data to
control-flow hijacking. To detect or prevent such attacks, a menagerie of
software and hardware protections have been proposed,
%
including stack canaries~\citep{Cowan+98},
bounds checking~\citep{NagarakatteZMZ09,NagarakatteZMZ10,DeviettiBMZ08},
split stacks~\citep{Kuznetsov+14},
shadow stacks~\citep{Dang+15,Shanbhogue+19},
capabilities~\citep{Woodruff+14,Chisnall+15,Skorstengaard+19,Skorstengaard+19b,Tsampas+19,Georges+21},
and hardware tagging~\citep{DBLP:conf/sp/RoesslerD18}. \ifaftersubmission\apt{Mostly from
  nick; there could be more}
\fi
%
The implementation and configuration of these  stack-safety mechanisms can be complex,
and often involves compromises trading protection for efficiency.
Thus it would be useful to have a precise formal specification for stack
safety, both as a basis for comparing the security guarantees claimed by different enforcement
techniques and for verifying that these claims are met by particular implementations.

For example, \citet{DBLP:conf/sp/RoesslerD18} describe how to protect the stack
using tag-based \emph{micro-policies}~\citep{pump_oakland2015}.
In essence, memory locations within each stack frame are tagged with the identity of
the function that owns the frame, and access to those locations is then gated
according to the function that is currently executing. Many alternatives can
be implemented on top of this basic idea, by choosing how to identify functions
and deciding just which accesses are checked.  These alternatives vary in their
efficiency, but also in the spatial and temporal precision of their security guarantees.
For example, the \emph{Depth Isolation} policy strictly requires that a function owns any
frame into which it writes; this seems simple and intuitively correct, but can be expensive to implement.
On the other hand, the \emph{Lazy Tagging and Clearing} policy improves performance by
allowing illicit writes to the frame so long as the written locations are not
subsequently read. The intuition justifying laziness is that any genuinely dangerous writes will ultimately be
detected before they can affect program behavior.  But we clearly need a more formal foundation for
expressing, comparing and verifying these micro-policy security claims, as well as those of
other enforcement technologies.

So how should we formally define stack safety? Definitions in the literature are commonly informal
and proceed by enumerating the specific bad things that a given enforcement mechanism
prevents, such as corruption of return addresses, buffer overflows, or
use of uninitialized variables.
%\bcp{Does use of unitialized variables lead to stack safety attacks?\apt{sure, via buffer overflow}}
%with the notion usually being
%defined negatively: through examples of stack-based exploits
%\rb{usually though not always; as noted below there are other
%  approaches}\bcp{Yes, we have to be very careful about this}.
%\leo{Examples at a high level}: such as -- does not overwrite return addresses, or ...
The most abstract characterization of which we are aware is in
recent work by \citet{Skorstengaard+19}, which defines stack safety as the conjunction of two properties:
{\em local state encapsulation (LSE)} and {\em well-bracketed control flow (WBCF)}.
Informally, LSE says that the contents of
a caller's stack frame are not read and written while its callees (and their callees, etc.) are
executing, and WBCF says that callees always
return to the point just after their call (or don't return at all).
\citet{Skorstengaard+19} formalize these properties by defining an idealized machine
in which they visibly hold by construction; an enforcement mechanism (in their case, hardware capabilities)
can then be shown to provide stack safety by giving a fully abstract embedding function
from the idealized machine to an ordinary machine equipped with the enforcement.
%%
%% While this work represents an important step in the right direction, we believe
%% there is still room to craft an operational characterization
%% of stack safety. \apt{Perhaps our rhetoric should be more micro-policy-specific here?}
%% In other words, we seek formal criteria that can be applied
%% to a mechanism for protecting stack-based low-level programs to judge
%% whether or not it actually ``guarantees stack safety.''
%% \rb{The remainder of this paragraph may be diving too quickly, would a reader be
%% confused by the references to these types of properties, yet to be introduced?}
%% We begin with a strong
%% local state encapsulation property that closely models the behavior of eager
%% micro-policies from \citet{DBLP:conf/sp/RoesslerD18}. This ``eager'' property
%% is too strong to characterize stack safety; reasonable enforcement mechanisms
%% exist that violate it. One such mechanism is a ``lazy'' tag policy, similar to
%% the one in \citet{DBLP:conf/sp/RoesslerD18}. We weaken local state encapsulation
%% to support this technique and argue that the new property, defined relative to an
%% arbitrary notion of observation. This property is still strong enough to prevent
%% the bad behaviors we associate with unsafe stack behavior, but weak enough that
%% other reasonable enforcement mechanisms can enforce it.

In this work, we take the (informal) definitions of WBCF and LSE as a starting point,
but choose a different approach to formalizing them, based on the
technical framework of language-based security~\citep{??}.  We define
trace-based properties of \emph{stepwise stack integrity} and \emph{stack confidentiality}.
In applying these security concepts, we treat each caller-callee pair as mutually distrustful
and view the caller as needing protection from the callee. \apt{Need more explanation of
  ``nesting'' and our novelty factor here? Also about domains??}
{\em Integrity} says that a caller's local data is protected from modification by its callee.
This is vital for local reasoning about the behavior of the caller.
{\em Confidentiality} states that a caller's data is protected
from being leaked into its callee, and also that the callee cannot see uninitialized data.
As a reasoning principle, confidentiality guarantees that the callee's
behavior is determined only by a predictable interface that does not include the caller's
local state or the uninitialized portion of the stack.
Together, these properties (informally) capture LSE, as well as the ``temporal stack safety''
notion that data outside of live frames should be inaccessible (which LSE does not address).
We also give a simple trace-based formalization of WBCF, which is essential for keeping the
notions of ``caller' and ``callee'' sensible.

To illustrate the applicability of these properties, we perform randomized testing to
validate that \citeauthor{DBLP:conf/sp/RoesslerD18}'s Depth Isolation micro-policy enforces them.
\apt{more to say?}\bcp{Yes---partly to underscore our claim that this is a
  significant contribution, and partly to give the reader some context for
  why this is part of our story.  E.g., ``One nice thing we can do with our
  spec is to use it to evaluate the claims of existing or future proposals
  for enforcement mechanisms.  One relatively inexpensive way to carry out
  such an evaluation is to see whether property-based testing against this
  spec can distinguish correct implementations of the mechanism from buggy
  ones...''}

However, their Lazy Tagging and Clearing micro-policy does not enforce stepwise integrity.
To address this, we develop an \emph{observational stack integrity} property that
is intended to specify enforcement mechanisms that
detect integrity violations between the point when they occur and the point when they make
an observable impact. Intuitively this property can be phrased as ``no visible reads after dangerous
writes.''  We prove (in Coq) that stepwise integrity implies observational integrity.

Unfortunately the existing Lazy Tagging and Clearing
micro-policy does not quite enforce this property either, as we show in
testing~\apt{really??}, but formalizing the ideal property allows us to see how it falls short.
We propose  a variant of Lazy Tagging and Clearing that we believe \emph{does} enforce observational integrity,
at some performance cost.\apt{but we're not testing that, right?}
\rb{Not sure about the property part right now, the machine side of this is
pretty much done}

Observational integrity is attractive in being significantly more \emph{extensional} than
stepwise integrity.  We argue that it may therefore be a better minimal specification for
stack safety enforcement mechanisms in general.

Our initial description of the properties assumes single simple stack with no sharing between callers and
callees.  We expand it to allow passing of scalar stack data, granting a callee
access to the caller's data. We do not model more sophisticated sharing, because to do
so realistically would require a separate protection scheme, such as a capability
model. While this is quite feasible in a tag-based enforcement mechanism, it is properly
a separate policy outside the scope of stack safety. (A tag-based approach can easily
compose multiple such policies.)
\rb{So no pointer passing? Pass by reference is now discussed at the end of
Section 3, however it looks like things get tricky rather quickly.}

Finally, we define integrity and confidentiality properties for a coroutine system featuring finitely-bounded stacks.
Though simplistic, this extension is a step toward concurrency and demonstrates that
our approach is flexible enough to handle more sophisticated styles of control-flow.

While our property definitions were developed with micro-policy enforcement in mind, they
agnostic about many implementation details and should be applicable to other
systems with minor adjustment.\apt{move this up?}

%% %\begin{figure}[b]
%% %    \begin{minipage}{.4\textwidth}
%% %\begin{verbatim}
%% %int main() {
%% %  int x = 42;
%% %  f();
%% %  print(x);
%% %}
%% %\end{verbatim}
%% %    \end{minipage}
%% %    \begin{minipage}{.4\textwidth}
%% %\begin{verbatim}
%% %int main() {
%% %  int x = 1;
%% %  f();
%% %  if(x)
%% %    g();
%% %  else
%% %    h();
%% %}
%% %\end{verbatim}
%% %    \end{minipage}
%% %  \caption{Simple Examples}
%% %  \label{fig:examples1}
%% %\end{figure}

%% %\begin{figure}[b]
%% %    \begin{minipage}{.4\textwidth}
%% %\begin{verbatim}
%% %int main(int arg) {
%% %  if(arg)
%% %    g();
%% %  else
%% %    h();
%% %  f();
%% %}
%% %\end{verbatim}
%% %    \end{minipage}
%% %    \begin{minipage}{.4\textwidth}
%% %\begin{verbatim}
%% %int main() {
%% %  int x = 1;
%% %  f();
%% %  x = -x;
%% %  f();
%% %  print(x);
%% %}
%% %\end{verbatim}
%% %  \end{minipage}
%% %
%% %  \caption{More Examples}
%% %  \label{fig:examples2}
%% %\end{figure}

%% \paragraph*{The Use of Stack Safety}

%% How does the programmer rely on stack safety? The stack makes implicit promises about
%% the behavior of function calls. First, that when a call is finished, the caller can continue
%% from where it left off with its local state unchanged. And second, that the call's
%% behavior is determined only by a predictable interface that does not include the caller's
%% local state.

%% %The most natural example is the first
%% %in figure \ref{fig:examples1}, in which {\tt main} calls stores 42 to a local variable {\tt x},
%% %calls {\tt f}, and then prints {\tt x}. Here the programmer should be able to expect that
%% %{\tt main} will print 42, but in C such an assumption may be wrong, if {\tt f} should
%% %overwrite {\tt main}'s stack data.

%% Both expectations imply predictable control flow, termed {\em well-bracketed
%% control flow} (WBCF) by \citeauthor{Skorstengaard+19}. Calls always jump to valid entry
%% points and return to their call sites. Then the data portion of the first expectation is for
%% {\em integrity}: that a caller's local data is protected from modification by its callee.
%% This is vital for local reasoning about the behavior of the caller. The security implications
%% are significant: without integrity, a caller that branches on a local variable could be
%% manipulated by its callee into unexpected behavior.

%% Alongside integrity, we have {\em confidentiality}, in which a caller's data is protected
%% from leakage by its callee. As a reasoning principle, confidentiality tells us that to
%% predict the behavior of a call we need only consider a subset of the machine state.
%% And in security terms, we can guarantee the secrecy of local data that we haven't passed
%% even when calling a badly behaved function. Together, integrity and confidentiality
%% comprise \citeauthor{Skorstengaard+19}'s {\em local state encapsulation} (LSE) property.

%% It must be reiterated that LSE is of little use without WBCF: without WBCF, a callee
%% could subvert LSE by ``returning'' to arbitrary code; after the return LSE should permit
%% accessing the caller's state, but the execution would remain under the callee's control.

%% To phrase LSE and WBCF as security properties requires an explicit attacker model:
%% a clear understanding of who is being protected and from what. Most existing
%% stack protection mechanisms envisage scenarios where a callee is
%% influenced by external inputs to corrupt the stack in some way. To
%% avoid getting into the details of exactly how this happens when
%% formalizing our properties, we use an even stronger model: when we
%% state the protections that a function can rely on when it makes a
%% call, we assume that the callee can execute arbitrary instructions
%% until it returns. Thus we have no need to differentiate safe code
%% from adversarial --- all functions, and indeed all activations of the same function,
%% are mutually distrusting.
%% %
%% For this model to make sense, we need to know when a call or a return is taking
%% place. We assume that calls and returns are annotated in the code, so that
%% we can track the point at which control transfers from the protected caller
%% to the attacking callee --- and when that callee in turn becomes a protected
%% caller in its own right.

%% This ``nesting'' of properties is a key challenge in adapting the concepts
%% of language-based security to the stack setting. We address this issue by
%% turning any set of annotations into a dynamic model of {\em domains},
%% in which some stack locations are sealed by the caller that needs to preserve
%% them while the rest are unsealed and available for use by the active, attacking
%% callee.

%% First we introduce this model for a single simple stack with no sharing between callers and
%% callees. Then we expand it to allow passing of stack data, granting a callee one-time
%% access to the caller's data. We do not model more sophisticated sharing, because to do
%% so realistically would require a separate protection scheme, such as a capability
%% model. While this is quite feasible in a tag-based enforcement mechanism, it is properly
%% a separate policy outside the scope of stack safety. (A tag-based approach can easily
%% compose multiple such policies.)

%% Finally, we extend the model to a coroutine system featuring finitely-bounded stacks.
%% Though simplistic, this extension is a step toward concurrency and demonstrates that
%% the model is flexible enough to handle more sophisticated styles of control-flow.

%% After formally defining stack safety, we show that the definition is
%% enforceable using an existing tag-based enforcement
%% mechanism~\citep{DBLP:conf/sp/RoesslerD18}. We first consider
%% \citeauthor{DBLP:conf/sp/RoesslerD18}'s conservative Depth Isolation policy.
%% This policy testably enforces our {\em eager} properties, in which integrity is respected
%% at every step of a program, and each call respects integrity at each of its internal steps.
%% These properties are very strong, and our policies enforce them; this also makes them very
%% intensional, in that they do not distinguish internal behavior from external.
%% They also have the advantage that they are suitable for efficient testing, as they
%% fail immediately when a violation occurs.

%% The downside of the Depth Isolation policy is that it is rather expensive to implement.
%% In the same work, \citeauthor{DBLP:conf/sp/RoesslerD18} propose a different mechanism
%% that is more efficient by virtue of being lazy in its enforcement.
%% Instead of enforcing the integrity and confidentiality of the stack at the
%% exact moment when a callee returns, this policy signals violations when the
%% caller actually accesses data that the callee has written into its stack
%% frame. \ifaftersubmission\bcp{We could also hint at the lazy confidentiality
%%   property in the same way...}\fi
%% The lazy policy does not enforce our eager integrity property, and to our
%% knowledge there is no formal statement of the properties that it ought to enforce.
%% We propose {\em lazy} stack-safety properties, which capture the desired behavior of a
%% policy that detects integrity violations between when they occur and when they make
%% a visible impact. Intuitively this can be phrased as ``no visible reads after dangerous
%% writes.'' Unfortunately \citeauthor{DBLP:conf/sp/RoesslerD18}'s existing
%% ``Lazy Tagging and Clearing'' policy does not quite enforce this property, as we show in
%% testing, but formalizing the ideal property allows us to see how it falls short.
%% We provide a variant of ``Lazy Tagging and Clearing'' that does enforce lazy integrity
%% at some performance cost.
%% %We prove (in Coq) that the eager property implies the lazy one.\apt{do we?
%% %  and is it so interesting anyhow?}
%% We argue that any system purporting to enforce stack safety ought to obey at least lazy
%% stack safety, making it a plausible extensional characterization.

%% %\bcp{I worry that we are being a little
%% %  dishonest here: the stated advantage of the lazy policy is that it is more
%% %efficient, but our variant is {\em not} efficient!}

In summary, we offer the following contributions: \bcp{Section numbers are
  wonky---why just 7.1, where is 8, etc.}
\begin{itemize}
\item
  We adapt formulations of integrity and confidentiality from
  language-based security
  to formalize a parameterized {\em stepwise stack-safety property} and argue that
  it captures the intuitive concepts of local stack encapsulation, well-bracketed control
  flow, and temporal stack safety~(\cref{sec:lse-and-wbcf}).
\item
  We use a property-based randomized testing framework to support the claim
  that this stepwise property is actually enforced by the Depth Isolation policy
  of~\citet{DBLP:conf/sp/RoesslerD18}~(\cref{sec:conservative}).
\item
  We formalize a weaker but more extensional \emph{observational} variant of the
  stack safety property and argue that it accurately characterizes
  the Lazy Tagging and Clearing policy of~\citet{DBLP:conf/sp/RoesslerD18}
  enhanced with unique tags on each activation~(\cref{sec:lazy}).
\item
  We show how our models can be instantiated and extended to handle passing
  arguments on the stack~(\cref{sec:??}) and simple
  coroutines~(\cref{sec:??}).\bcp{``instantiated and extended'' is awkward.
  Maybe ``We show how our models can be instantiated to handle passing
  arguments on the stack~(\cref{sec:??}) and extended with simple
  coroutines~(\cref{sec:??}).''  If that's what we mean.}
%  This policy does not enforce our strict
%  local state encapsulation property,
%  but a modified version of it using unique per-activation tags enforces
%  an (also provably) weaker lazy variant of the property, which still prohibits
%  the same intuitively stack unsafe behaviors.
%
% \apt{Awkward, but the truth I fear.} \bcp{I agree the phrasing is awkward,
%   but the point we can make is still strong: To make the property easier
%   to enforce, we can slightly weaken its guarantees; the weaker version is
%   a bit more complicated to state but still retains the pragmatic
%%   protections of the original.}\apt{To clarify: Nick's LT\&C policy uses
%   depth-based tags, which do not enforce even our lazy policy. So we need to
%   introduce a version of LT\&C that uses unique activation tags.}

  %\item As evidence that the property is not too strong\apt{But we add the lazy version because the strict version \emph{is} too strong, in some sense.}, we show that it is
%  realizible by
%  a previously proposed micro-policy enforcement
%  mechanism~(\Cref{sec:micro-policy}). \leo{And it says yes on a
%    substantial body of code}. We also present a variation of the
%  property + policy that is lazy, easier/more efficient to implement,
%  and provably weaker (\Cref{sec:lazy}).
%\item A provably stronger (inductive? step-by-step/stepwise) variant
%  of this property tuned for testing/verification.  We test the whole
%  thing with a cool framework. \bcp{Which section?}
\end{itemize}
\Cref{sec:relwork} discusses related work, and 
\cref{sec:future} sketches directions for future work.

We begin with a description of our threat model
(\cref{sec:threat}), a running example that we refer to throughout the paper
(\cref{sec:running-example}),  and some technical preliminaries
(\cref{sec:prelim}).\apt{this para needs updating}

\section{Threat Model and Assumptions}
\label{sec:threat}

Stack safety is meant to protect a caller from a confused or malicious callee's
attempts to access private data or hijack its control flow. And in fact the caller,
while presumably not malicious, may itself be confused or erroneous. Our attacker
model reflects this intention by assuming that both the attacker and the defender
may run arbitrary machine code. Hardware and timing attacks are out of scope.

%\leo{Anything else to add here?}
%\leo{Maybe a good place to discuss calling conventions and that they
%  are essentially enforced by the policy or sth?}
%\rb{Intuition that the callee can do anything in principle, but the machine will
%halt if it does not follow the calling convention, etc.?}

\newcommand*{\rsp}{\textsc{sp}}

In this very strong attacker model, there is little obvious distinction between
the attacker and the defender. They are separated only by the control transfer
inherent in their calls and returns. Our stack-safety concepts therefore depend
critically on the notions of ``call'' and ``return,'' which are not fully explicit
in machine code (the same instruction opcode may be used in some places as part of
a call or return sequence and in other places for other purposes).
%
We assume we are given a machine language program annotated with the locations of
instructions representing calls, returns, and function entry points. Although this
information would typically come from a compiler together with the machine code,
we do not assume anything about their provenance. This means that attackers (or
foolish defenders) are under no obligation to have reasonable annotations.

\paragraph*{Well-formedness and Bad Annotations}

Since we rely so heavily on code annotations, we must ask: what if the annotations
are wrong? First, suppose a call or return occurs that is not annotated.
In this case, the answer is simple: as far as the properties are concerned, there has
been no call or return, only an instruction sequence resembling such an operation, and
therefore no associated change in permissions. This is particularly interesting in the
case of a false return, where control may appear to have returned to the caller, but
the enforcement mechanism must still protect the caller's memory.

Alternatively, an annotated instruction might not really be a call or return.
In the context of \citeauthor{DBLP:conf/sp/RoesslerD18}'s micro-policies, well-behaved calls
and returns are identified with ``blessed sequences'' of instructions, which the tag policy
will force to be executed from start to finish. The annotated instruction should always
be the final instruction of the sequence. What if an annotated instruction exists outside
such a sequence? We will statically rule out some scenarios with a well-formedness criterion
on initial states: annotated instructions will initially be tagged apropriately, as the
final instruction in the call or return sequence. This is all we need for the policy to
protect itself: it can then enforce that the call or return only occurs if that instruction
is a jump that follows the blessed sequence. So, even in our strong attacker model, we need
very little in the way of extra guarantees.

In general, regardless of enforcement mechanism, we take this approach:
a given enforcement mechanism may place a static well-formedness criterion
on the code it applies to. Weaker enforcement mechanisms might also need dynamic
reasonableness conditions to prevent the defender from sabotaging enforcement, as
discussed in section \ref{sec:cheri}.

\paragraph*{System Assumptions}

Our properties are defined in terms of an abstract machine model that makes very few
assumptions about the machine that instantiates it.  We assume a register
machine with a program counter \(\PCname\) and stack pointer \(\rsp\); other
details of the machine's ISA are unimportant. We assume that we know ahead of time
the layout of memory, including code (with annotations) and which addresses are within
the bounds of the stack.
%
We further parameterize over the structure of the stack, particularly the means by
which a caller identifies the data that should be protected during a call. We term
this the {\em sealing convention} and model it as a relation between machine states
and addresses. Then from any call, those the sealing convention relates to the
state will be protected until the return.

\paragraph*{Policies}

We model the base machine as a step function over machine states, and a machine enhanced with
an enforcement mechanism (in the language of tag-based enforcement, a {\em policy}) as
a partial step function over machine states extended with some auxiliary state, refining
the base machine. When the policy-enhanced machine fails to step, this is a {\em failstop},
halting the program before it can perform an action that would violate the security property;
our properties are therefore naturally \emph{termination insensitive.}
This model will be made more precise in \cref{sec:prelim}.

%We model ordinary program state and policy-enforcement state separately to make it
%easier to use well-established concepts from language-based security.
%In particular, confidentiality has an elegant extensional characterization in terms of
%\emph{noninterference}: a program preserves the confidentiality of certain data if
%\emph{varying} that data would not change the program's observable
%behavior~\citep{6234468}.
%In applying this idea to a system that incorporates policy enforcement, only
%ordinary program data should be varied, while any associated policy
%state should be left alone; our model makes this distinction obvious.
%That is, we compare the actual execution of the program with a \emph{hypothetical}
%execution where data has been varied.
%We will also introduce a notion of {\em observational integrity} that relies on the idea of
%\emph{rolling back} program state; again, our model makes it easy to describe
%rollback of ordinary program data while leaving policy state alone.

%% \leo{Policies of interest: null, micro, cherri, software-only}
%% \leo{TODO: Write down carefully}

This enforcement model directly describes micro-policies~\citep{pump_oakland2015}.
We believe that it is flexible enough to capture code-altering approaches, such as code
rewriting or capability-based techniques, perhaps with less elegance.

\newcommand{\TAGS}{\mathcal{T}}
\newcommand{\tagname}{t}
\newcommand{\uP}{{\mu P}}

\paragraph*{Micro-Policies}
%
Micro-Policies are a flexible tag-based, hardware-accelerated
reference monitoring mechanism; they been applied to stack safety
enforcement by \citet{DBLP:conf/sp/RoesslerD18}.
Here the policy state consists of metadata tags (e.g., identifying stack frames)
attached to each value in memory or registers, and the policy step
function checks that each machine operation obeys a set of rules on tags
(e.g., that the current $\PCname$ tag matches the tag on the stack
location being accessed), and halts the machine if not.  Distinguishing
ordinary machine state and policy state is very natural in this setting.
%% Here the machine state is extended with tags paired with all components of the
%% system, and a monitor checks for each operation that the tags on the operands
%% obey a set of rules. One could treat tags directly as part of the machine
%% state, and build the rules into the step function. But local state
%% encapsulation properties as we frame them are not intuitive in their
%% interaction with tags or other additional enforcement state, so it is helpful
%% to separate tags for clarity. This also distinguishes a hardware fail-stop from
%% a software fail state.
%% \rb{TODO Similarly to above, should we consider tags as part of the machine or part of the
%%   policy? }


\ifaftersubmission\apt{This still seems wrong to me. The policy part needs somehow to describe everything that isn't varied or rolled back.}\fi
%These approaches are represented as policies that
%accept\bcp{what does it mean for a policy to accept a state??} only initial machine states that are well-formed for the policy (due to
%being produced by the compiler.) The set of policy states is again the unit type
%and the step function constant. Property enforcement comes from the fact that
%only ill-formed states can induce a trace that violates the property.


%% \Paragraph*{Null policy}
%% %
%% The simplest possible policy is the neutral policy that carries no information
%% and does nothing. Its set of policy states is the unit type, and its policy step
%% function is the constant unit function, thereby allowing all calls to the machine
%% step function. Its initialization function is defined on all states. This
%% corresponds to the lifting of our machine model to the policy setting. Machines
%% that fulfill a property by construction will also fulfill that property with the
%% null policy.

%
%\leo{TODO: Add citations of mitigation techniques}
%\rb{And for those techniques, explain how they reflect their motivating
%examples. What does each technique do for those examples? How are they
%connected to this work?}
%
%\paragraph*{Bad Write}
%
%\[
%44 : [\rsp - 5] \leftarrow r_4
%%44 : ~ \sw ~ \rsp ~ r_4 ~ \negate 5
%\]
%
%Instruction 44 currently is
%$ [\rsp - 2] \leftarrow r_4$,
%%$\sw ~ \rsp ~ r_4 ~ \negate 2$,
%with the
%effect of storing the contents of $r_4$ as the result of {\tt g}. If
%instead it wrote to location $\rsp - 5$ then it would be overwriting
%private data (the return address) belonging to {\tt f}.
%
%\paragraph*{Bad Read}
%
%\[
%30 : r_5  \leftarrow [\rsp - 6]
%%30 : ~ \lw ~ r_5 ~ \rsp ~ \negate 6
%\]
%
%Instruction 30 currently is
%$ r_5 \leftarrow [\rsp - 4]$,
%%$\lw ~ r_5 ~ \rsp ~ \negate 4$,
%with the
%effect of loading the argument of {\tt f} to $r_5$. If instead it
%loaded the contents of location $\rsp - 6$, it would obtain access
%to the value of {\tt main}'s local variable {\tt x}. Worse, since the next instruction
%outputs the contents of $r_5$, that value would immediately be made public.
%
%\paragraph*{Bad Control Flow}
%
%\[
%5  : \rra \leftarrow \PCname + 1 ; ~ \PCname \leftarrow r_4
%%5 : \jalr ~ r_4 ~ \rra ~ 0
%\]
%If a {\jalr} instruction was added to {\tt main} with its target being
%the middle of the code block of {\tt g}.\apt{describe result of this?}

\section{Stack Safety, Intuitively}
\label{sec:running-example}
\colorlet{lgray}{gray!40}
\colorlet{lred}{red!40}
\colorlet{lblue}{blue!20}

\newcommand{\mainsealc}{cyan}
\newcommand{\fsealc}{green}
\newcommand{\unsealc}{lgray}
\newcommand{\emptyoutc}{white} % or gray for consistency with unusedc ?
\newcommand{\fulloutc}{white}
\newcommand{\badc}{lred}
\newcommand{\goodc}{lblue}
\newcommand{\retptrc}{black}

\begin{figure}

\begin{subfigure}{.4\textwidth}
\begin{verbatim}
int main() {
0:     int x, y;
1:     y = 42;
2[C]:  x = f();
3[R]:  return x+y;
}

int f() {
10:     $SP-2 = 0;
11[R]:  return 5;
}
\end{verbatim}
\end{subfigure}
\begin{subfigure}{.4\textwidth}
\begin{center}
\begin{tabular}{l l}
{\tt 1} &
\memoryaddrs{12em}
\memory{4}{\unsealc}[Unseal]
~$\cdots$
\vspace{.5em}
\\
{\tt 2} &
\memoryaddrs{12em}
\memory{4}{\unsealc}
~$\cdots$
    \MemoryLabel{-15em}{0.75em}{42}
    \vspace{.5em}
\\
{\tt 10} &
\memoryaddrs{16em}
\memory{2}{\mainsealc}[Seal(0)]%
\memory{1}{\retptrc}[RetPtr]%
\memory{1}{\unsealc}[Unseal]
~$\cdots$
\MemoryLabel{-15em}{0.75em}{42}
\vspace{.5em}
\\
{\tt 11} &
\memoryaddrs{16em}
\memory{2}{\mainsealc}
\memory{1}{\retptrc}
\memory{1}{\unsealc}
~$\cdots$
\MemoryLabel{-15em}{0.75em}{\bf 0}
\vspace{.5em}
\end{tabular}
\end{center}

\vspace{\abovedisplayskip}
\end{subfigure}
\label{fig:int1}
\caption{Integrity Violation}
\end{figure}

This section will elucidate the intuition behind the properties through a series
of examples. Existing stack safety enforcement work mostly refers directly to examples
of bad behavior that it prevents; we will insert our model between the enforcement and
the examples, arguing that:

\begin{itemize}
\item Examples that appear to violate stack safety, violate at least one of our properties
\item We do not find desireable behaviors that appear stack-safe yet violate our properties
\item Our properties are testably implemented by stack safety micro-policies from the
  literature, and the micro-policies are not significantly more conservative
  (i.e., policy failstops usually correspond to imminent property violations)
\end{itemize}

Our examples are written in C-like syntax, labeled with line numbers for reference,
but to reflect the increased power of the underlying assembly code, we allow code
to access the stack pointer directly, written {\tt \$SP}. The layout of the stack
is given to the right, with a box representing a word of memory, and variables stored
in declaration order from left to right. The stack grows upward in memory and to the
right on paper.

%The boundary between the frames of a caller and callee is indicated by the $\rsp$ register
%at the time of the call. The stack can be used to pass arguments or return results
%in the usual way, by placing them at the top of the caller's frame. This will be modeled
%as a form of sharing in section \sna{cite the sharing section}.
%Callers and callees can also communicate via registers if they wish; indeed, our
%definitions say nothing at all about registers (except $\PCname$ and $\rsp$).

%\ifaftersubmission\bcp{This implicitly amounts to choosing a caller-saves register
%  convention, though, doesn't it?\apt{I think it is neutral; we could add separate caller register protection if we wanted.}}\fi

Example \ref{fig:int1} shows an integrity violation. It consists of a {\tt main}
function that performs a call to a function {\tt f}, which overwrites {\tt main}'s
variable {\tt x}. In the initial memory layout the entire stack is {\em unsealed},
meaning that it is eligible to be written to. At the call from {\tt main} to
{\tt f}, {\tt main}'s locals are {\em sealed} and labeled with the depth of their owner,
in this case 0. Sealing is the fundamental integrity contract between the caller and callee:
{\tt main} expects that {\tt x} and {\tt y} will be unchanged when it gets control back.
This gives us our simple, intuitive statement of {\em stepwise integrity}:
if a component is sealed before a step, it will be unchanged afterward. Stepwise
refers to the fact that the property must hold on every step. %Later we will
%loosen this condition in the interest of extensionality -- the caller only
%cares about changes it can see after the return, not changes during execution.
In this case, after line 10, {\tt f} has violated integrity by writing to {\tt x}.

The black rectangle represents the return address. In a typical calling convention
this will have been passed in a register during the call itself, and {\tt f} stores
it in the stack during its preamble, so it is not itself sealed.

\begin{figure}
\begin{subfigure}{.33\textwidth}
\begin{verbatim}
int main() {
0:     int x;
1:     x = 42;
2[C]:  x = f();
3[R]:  return x+y;
}

int f() {
10:     int y,z;
11:     y = 5;
12:     z = $SP-2;
13[R]:  return y+z;
}
\end{verbatim}
\end{subfigure}
\begin{subfigure}{.59\textwidth}
\centering
\begin{tabular}{l l | l}
  {\tt 1} &
  \multicolumn{2}{c}{
    \memoryaddrs{8em}
    \memory{4}{\unsealc}
    ~$\cdots$
    \MemoryLabel{-18em}{0.75em}{0}
    \MemoryLabel{-14em}{0.75em}{0}
    \MemoryLabel{-10em}{0.75em}{0}
    \MemoryLabel{-7em}{0.75em}{0}
    \vspace{.5em}
  } \\
  {\tt 10} &
  \memoryaddrs{20em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{2}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{42}
  \MemoryLabel{-10em}{0.75em}{0}
  \MemoryLabel{-6em}{0.75em}{0}
  &
  \memoryaddrs{20em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{2}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{\(v_0\)}
  \MemoryLabel{-10em}{0.75em}{\(v_1\)}
  \MemoryLabel{-6em}{0.75em}{\(v_2\)}
  \\
  {\tt 11} &
  \memoryaddrs{20em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{1}{\goodc}
  \memory{1}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{42}
  \MemoryLabel{-10em}{0.75em}{5}
  \MemoryLabel{-6em}{0.75em}{0}
  &
  \memoryaddrs{20em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{1}{\goodc}
  \memory{1}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{\(v_0\)}
  \MemoryLabel{-10em}{0.75em}{5}
  \MemoryLabel{-6em}{0.75em}{\(v_2\)}
  \\
  {\tt 12} &
  \memoryaddrs{20em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{1}{\unsealc}
  \memory{1}{\badc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{42}
  \MemoryLabel{-10em}{0.75em}{5}
  \MemoryLabel{-6em}{0.75em}{42}
  \vspace{.5em}
  &
  \memoryaddrs{20em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{1}{\unsealc}
  \memory{1}{\badc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{\(v_0\)}
  \MemoryLabel{-10em}{0.75em}{5}
  \MemoryLabel{-6em}{0.75em}{\(v_0\)}
  \vspace{.5em}
\end{tabular}

\vspace{\abovedisplayskip}

\label{fig:conf1}
\end{subfigure}
\caption{A Confidentiality Violation}
\end{figure}

Meanwhile confidentiality intuitively states that a callee is unaware
of the context of its call (except for its arguments and any global variables).
We state this in a style similar to {\em noninterference}, a well-known concept in theoretical
security. Noninterference describes knowledge: it states that for some system components
containing secret values, we can substitute any values whatsoever without changing the results
of our computation. If there exists some value for which we see different results, then
in fact we can learn something about the initial state by observing those results, violating
confidentiality.

In example \ref{fig:conf1} we see that the callee, {\tt f}, violates confidentiality
by reading {\tt main}'s variable {\tt x}. The trace
shows how noninterference applies to this scenario. At line 11, we create a {\em variant}
state with arbitrary values \(v_0\), etc., in place of the values that should
be hidden from {\tt f}. At line 12, we note that the callee's first variable, {\tt y}, has
changed to 5 in both the primary trace and the variant. The primary and variant agree on
the location that changed, so confidentiality was not violated. But after line 13,
the other variable, {\tt z}, has changed. In the primary trace it changed to 42, but
in the variant trace it might have any value \(v_2\). Say \(v_2 = 43\) -- then the variant
has behaved differently than the primary, violating confidentiality.

This form of confidentiality we will term {\em lockstep confidentiality}, defined informally
as: for any state at the entry to a function, for any variant over the values in the stack,
the primary state and its variant will step in lockstep until they return. Any system components
whose values update will be identical between the primary and variant traces. This includes not
just memory, but registers as well, so that returning a secret is a violation. And when we
extend the model with a notion of external output, it will include any values that are
outputted.

We alternatively could imagine a stepwise form of confidentiality in which we explicitly
track which addresses may be read (i.e., which have been initialized), and at each step
guarantee that the step alters state identically to a step from a variant state. Then we
wouldn't need to divide up the execution by calls. But we can see in this example that taking
a single variant at the start of the call effectively captures initialization: when {\tt y} is
initialized, the primary and the variant agree on its value, so it can now be safely read.
This treatment is roughly in line with more standard uses of noninterference, which typically
vary states at the starts of whole program runs.

Together, stepwise integrity and lockstep confidentiality form the stepwise version of
local state encapsulation.

\begin{figure}

\begin{subfigure}{.3\textwidth}
\begin{verbatim}
int main() {
0:     int x;
1[C]:  x = f();
2[CR]: return g();
}

int f() {
10:     int y;
11:     y = 5;
12[R]:  return y;
}

int g() {
20:     int z;
21:     if(z == 5)
22[R]:    return 100;
23:     else
24[R]:    return 10;
}
\end{verbatim}
\end{subfigure}
\begin{subfigure}{.65\textwidth}
\begin{center}
\begin{tabular}{l r | l}
  \circled{\tt 01} &
  \multicolumn{2}{c}{
    \memoryaddrs{8em}
    \memory{4}{\unsealc}
    ~$\cdots$
    \MemoryLabel{-18em}{0.75em}{0}
    \MemoryLabel{-14em}{0.75em}{0}
    \MemoryLabel{-10em}{0.75em}{0}
    \MemoryLabel{-7em}{0.75em}{0}
    \vspace{.5em}
  } \\
  \circled{\tt 11} &
  \memoryaddrs{16em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{2}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{0}
  \MemoryLabel{-10em}{0.75em}{0}
  \MemoryLabel{-6em}{0.75em}{0}
  &
  \memoryaddrs{16em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{2}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{\(v_0\)}
  \MemoryLabel{-10em}{0.75em}{\(v_1\)}
  \MemoryLabel{-6em}{0.75em}{\(v_2\)}
  \\
  \circled{\tt 12} &
  \memoryaddrs{16em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{2}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{0}
  \MemoryLabel{-10em}{0.75em}{5}
  \MemoryLabel{-6em}{0.75em}{0}
  &
  \memoryaddrs{16em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{2}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{\(v_0\)}
  \MemoryLabel{-10em}{0.75em}{\(5\)}
  \MemoryLabel{-6em}{0.75em}{\(v_2\)}
  \\
  \circled{\tt 02} &
  \multicolumn{2}{c}{
    \memoryaddrs{8em}
    \memory{4}{\unsealc}
    ~$\cdots$
    \MemoryLabel{-18em}{0.75em}{5}
    \MemoryLabel{-14em}{0.75em}{2}
    \MemoryLabel{-10em}{0.75em}{5}
    \MemoryLabel{-7em}{0.75em}{0}
    \vspace{.5em}
  }
  \\
  \circled{\tt 21} &
  \memoryaddrs{16em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{2}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{5}
  \MemoryLabel{-10em}{0.75em}{5}
  \MemoryLabel{-6em}{0.75em}{0}
  &
  \memoryaddrs{16em}
  \memory{1}{\unsealc}
  \memory{1}{\retptrc}
  \memory{2}{\unsealc}
  ~$\cdots$
  \MemoryLabel{-19em}{0.75em}{\(v_3\)}
  \MemoryLabel{-10em}{0.75em}{\(v_4\)}
  \MemoryLabel{-6em}{0.75em}{\(v_5\)}
  \\ &
  \circled{\bf 22}
  ~$\cdots$
  &
  \circled{\bf 24}
  ~$\cdots$
  \\
\end{tabular}
\end{center}

\vspace{\abovedisplayskip}

\end{subfigure}

\caption{Another Confidentiality Violation}
\label{fig:conf2}
\end{figure}

It may be unclear in figure \ref{fig:conf1} why we also vary the initial values of {\tt y}
and {\tt z}. In figure \ref{fig:conf2}, we watch as {\tt f} leaves behind a value in {\tt y}
that corresponds to {\tt main}'s variable, {\tt x}. Then {\tt g} is able to access
{\tt y} without even directly using its ability to read the stack pointer. In
this case, {\tt g} branches on that value, which is a different way of
violating lockstep confidentiality. Technically, this violation
occurs because the program counter is also a component of the state that must change
identically between the primary and variant traces. In a case where \(v_1 \not = 5\),
the program counters will differ, breaking the lockstep.

\sna{Not terribly happy with the circles yet; the idea is that instead of a meta-level
  label for the line numbers we could treat the PC as a part of the system we're showing.}

%\paragraph*{Control Flow and Awkward Examples}

%\begin{figure}
%  \centering
%  \begin{minipage}{.4\textwidth}
%\begin{verbatim}
%int stash = 0;

%int main() {
%0:     int x = 1;
%1[C]:  f();
%2:     x = -x;
%3[C]:  f();
%4[R]:  return x;
%}
%\end{verbatim}
%  \end{minipage}
%  \begin{minipage}{.4\textwidth}
%\begin{verbatim}
%void f() {
%10:     if(!stash) {
%        // store return pointer
%11:       stash = $SP-1;
%12:     } else {
%13:       $SP-1 = stash;
%14:       stash = 0;
%15[R]:    return;
%        }
%}
%\end{verbatim}
%  \end{minipage}
%\label{fig:WBCF1}
%  \begin{subfigure}[t]{.4\textwidth}
%\begin{verbatim}
%int counter = 0;
%int stash = 0;
%
%int main() {
%0:     int x = 1;
%1[C]:  f();
%2:     x = -x;
%3[C]:  f();
%4[R]:  return x;
%}
%
%void f() {
%10:     switch(counter++) {
%11:       case 0:
%12[R]:      return;
%13:       case 1:
%14:         stash = $SP-1;
%15[C]:      main();
%16:         return;
%17:       default:
%18:         $SP-1 = stash;
%19[R]:      return;
%        }
%}
%\end{verbatim}
%\label{fig:WBCF2}
%  \end{subfigure}
%  \caption{Control Flow Attacks}
%  \label{fig:WBCF}
%\end{figure}

%Example \ref{fig:WBCF} shows how control flow attacks can compromise integrity in principle
%even when the attacker obeys LSE. In this case, the attacker hides a return pointer in a
%global variable and retrieves it the second time it is called, returning to the first call
%point rather than the second. A return was performed, so {\tt x} is no longer sealed, but
%line {\tt 2} is executed twice, so that {\tt main} returns 1 instead of -1.

%To prevent this, we define Well-bracketed Control Flow. Throughout the above examples, we
%have implicitly tracked the depth of the current call in order to seal memory at the correct
%depth. We also have a clear sense of where the entry points to functions are -- the top line,
%in our examples, and another instruction annotation in practice. Well-bracketed Control Flow
%holds if:

%\begin{itemize}
%\item All calls jump to entry points ({\em entry integrity})
%\item The depth counter decreases by exactly one on a return ({\em non-skipping})
%\item From a call at depth \(d\), the next state to have depth \(d\) matches the stack pointer,
%  and the program counter is at the instruction following the jal ({\em return integrity})
%\end{itemize}

%Non-skipping may seem strange -- surely the depth counter ought to decrease by one at a time
%as part of the property definition, and the problem comes when the return is to a shallower
%call's return pointer, violating return integrity? Yet some weaker enforcement mechanisms,
%such as those in which returns are implemented as invocation of a Cheri object capability,
%do allow skipping when adversarial code acts unreasonably by leaking its own return capability,
%reasoning that this is acceptable as long as only the unreasonable function is skipped
%(e.g. \citep{Skorstengaard+19b}).\apt{???} A variant of our model supports this weaker form of WBCF,
%but with tag-based policies we can enforce the non-skipping version entirely.\apt{??}
%\sna{Cite future work for details when the relevant section is finished.}

\paragraph*{Pass by Reference and Stack Arguments}

\newcommand{\mainpassc}{magenta}

\begin{figure}
  \centering
  \begin{subfigure}{.45\textwidth}
\begin{verbatim}
int stash = 0;

int main() {
0:         int x, y;
1[P(x)C]:  f(x);
2:         x = 0;
3[P(y)C]:  f(y);
4[R]:      return x;
}

void f(int &a) {
10:    if(!stash)
11:      stash = &a;
12:    *stash = 5;
13[R]:   return;
}
\end{verbatim}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \begin{tabular}{l l}
      {\tt 10} &
      \memoryaddrs{17em}
      \memory{1}{\mainpassc}[{\makebox[0pt]{Pass(0)}}]%
      \memory{1}{\mainsealc}[{\makebox[0pt]{Seal(0)}}]%
      \memory{1}{\retptrc}
      \memory{1}{\unsealc}
      ~$\cdots$ \\
      {\tt 11} &
      \memoryaddrs{17em}
      \memory{1}{\mainpassc}
      \memory{1}{\mainsealc}
      \memory{1}{\retptrc}
      \memory{1}{\unsealc}
      ~$\cdots$
      \MemoryLabel{-18em}{0.75em}{5}
      \\
      {\tt 3} &
      \memoryaddrs{12em}
      \memory{4}{\unsealc}
      ~$\cdots$
      \MemoryLabel{-18em}{0.75em}{0}
      \\
      {\tt 10} &
      \memoryaddrs{17em}
      \memory{1}{\mainsealc}[{\makebox[0pt]{Seal(0)}}]%
      \memory{1}{\mainpassc}[{\makebox[0pt]{Pass(0)}}]%
      \memory{1}{\retptrc}
      \memory{1}{\unsealc}
      ~$\cdots$
      \MemoryLabel{-18em}{0.75em}{0}
      \\
      {\tt 11} &
      \memoryaddrs{17em}
      \memory{1}{\mainsealc}
      \memory{1}{\mainpassc}
      \memory{1}{\retptrc}
      \memory{1}{\unsealc}
      ~$\cdots$
      \MemoryLabel{-18em}{0.75em}{\bf 5}
\end{tabular}
\end{subfigure}

\label{fig:passing}
\end{figure}

Up til now we have ignored function arguments, since our simplest model places them in
registers. This is an oversimplication, which we will now correct, starting with the more
interesting case: call-by-reference. There are important stack safety concerns
when passing arguments by reference, as seen in example \ref{fig:passing}. In this example,
{\tt main} calls {\tt f} with a reference to {\tt x}, but then clears {\tt x} and instead
passes a reference to {\tt y}. We would expect that the first call to {\tt f} might modify
{\tt x}, but not the second. Unfortunately {\tt f} has hidden away our reference and modifies
{\tt x} in the second call.

At each call, we add a new annotation \(P(x)\) or \(P(y)\) identifying the memory to be
passed, and our stack model marks it in magenta. Then the caller's frame that are not
passed are sealed, as usual. Just as we marked sealed memory with the depth of its owner,
we do the same to passed memory. We keep track of the current depth of the active function
next to the stack pointer in the diagram. A passed address may only be accessed when the
current depth is one higher than the one it is sealed with.

So at the initial call, {\tt x} is passed, and {\tt f} is free to write to it. Then upon
return, {\tt main}'s frame is unsealed. At the second call it is {\tt y} that is passed,
and when {\tt f} instead tries to write to {\tt x}, it violates integrity. For that matter
a future call with no arguments passed by reference would also violate integrity if it wrote
to either variable.

Passed addresses are also excluded from confidentiality: at the start of each call, we vary
only stack addresses that are not passed. This is important, as we use the passing label
for those arguments that are passed by value on the stack, as well. We might instead
treat them as unsealed, but then we would treat them as confidential like the rest of the stack.

We do not model permanent sharing, in which a variable's address is taken and it might be
passed far afield before use, because this situation is uninteresting and hard to motivate
as an instance of stack safety. We would expect to treat such sharing as completely
unprotected, and perhaps attach a separate memory safety policy to add some protection
to such variables.

\section{Machines, Traces, and Observations}
\label{sec:prelim}

We next describe our machine model. To make our definition of stack safety
as generic as possible, this section proposes an abstract interface to a
machine model plus some kind of (software- or hardware-enforced) policy monitor.
In~\cref{sec:enforcement}, we will instantiate this model with a concrete dynamic
policy enforcement mechanism based on \citet{DBLP:conf/sp/RoesslerD18}.

\subsection{Values, States, and Observations}

The basic building blocks of the machine are {\em values} and {\em addresses}.
Both are drawn from some set of {\em words} \(\WORDS\), ranged over by \(\word\).
%
Our machine states are composed of {\em components} \(\component\), addressed either by
words (memory addresses) or by register names (\(\reg\)) drawn from some set
\(\REGS\).
%
    \[\component \in \COMPONENTS = \WORDS + \REGS \]
%
The register names are assumed to include two special purpose registers: the
program counter {\PCname} and the stack pointer \(\SP\).

A {\em machine state} is a map from components to values:
%
\[\mach \in \MACHS = \COMPONENTS \rightarrow \WORDS\]

The step function of the machine takes a machine state and produces a
pair of a machine state and an {\em observation}, which may be either
silent (written \(\tau\)) or a word of data that the machine
communicates to the outside world:
\ifaftersubmission\bcp{This is a rather concrete
  choice!  Why not leave the set of observations abstract?}\fi
%
\[\obs \in \OBSS = \{\tau\} \uplus \WORDS\]
\[\mach \stepstoobs[\obs] \mach' \in \MACHS \rightarrow \MACHS \times \OBSS \]
%
This step function is total, so the machine's execution traces
(defined in \cref{sec:traces}) are infinite.

We choose to model observations explicitly, rather than simply treating
a subset of registers or memory as being observable.  This choice gives
us the flexibility to treat programs as observationally equivalent even when
their internal states may differ.
%The choice of modelling observations is an interesting one. A first approach
%would be to consider any change to the registers or memory as observable.
%However, as some parts of the stack will be considered ``private''
%this choice won't work. A relaxation of this to consider all changes
%to ``non-private'' data as observable could work in principle, but
%does not allow for an interesting weaker-but-more-efficient enforcement
%mechanism. We will come back to this point in \Cref{sec:lazy}.

In the running example of \Cref{sec:running-example},
$\WORDS$ is the
set of 64-bit integers and $\REGS$ is the set $\{r_0, \cdots, r_{31}\}
\cup \{\PCname,\SP\}$. Observations are writes to the
special $\rout$ location\ifaftersubmission\bcp{Which is 100! (do we need a name for it? seems
  not...)}\fi, which can be thought of as a memory-mapped
  output port: if during an execution step a word $w$ is
written to $\rout$ then $\obs = w$, otherwise $\obs = \tau$.

\subsection{Policies}

We now present our model of enforcement, which we introduced informally in \Cref{sec:threat}.
A policy is a general model of an enforcement mechanism, consisting of a set of policy
states \(\pol \in \POLS\) and a policy step function \((\mach, \pol) \polstep \pol' \in
\MACHS \times \POLS \rightharpoonup \POLS\). This step function is partial;
it is undefined on input configurations that correspond to a policy fault. The policy
can see the machine state, but cannot modify it. We assume that the
compiler provides an initial policy state \(\pol_0\) to accompany the initial machine state.
%
A concrete policy based on \citet{DBLP:conf/sp/RoesslerD18} will be described in
\Cref{sec:enforcement}.

We write $\MPS$ for the set of all pairs of machine states and policy
states.
%
\[\mpstatename \in \MPS = \MACHS \times \POLS\]
%
We lift the policy step function to operate on such pairs
by combining it with the regular step function for machine states.

\judgmenttwo{\(\mach \stepstoobs[\obs] \mach'\)}{\((\mach, \pol)
               \polstep \pol'\)}
            {\(\mpstate{\mach}{\pol} \stepstopol[\obs]
               \mpstate{\mach'}{\pol'}\)}

This combined step function is also partial, and traces of the machine-with-policy
will be finite iff a policy fault occurs.

\subsection{Contexts}

Separate from the machine state, our formal properties will keep track of additional context
information representing the history of the run. The following definitions are parameterized
over an arbitrary set of context states, \(\CONTEXTS\), annotations \(\ANNS\),
an initial context state \(\context_0 \in \CONTEXTS\), and a context step function
\(\mach, \context, \ann \in \powerset \ANNS \constep \context' \in \MACHS \times \CONTEXTS
\times \ANNS \rightarrow \CONTEXTS\). We also assume for any set of annotations that we have
code maps, with \(\FUNIDS\) a set of function identifiers:
\[\codemap \in \WORDS \rightharpoonup \FUNIDS \times \powerset(\ANNS)\]

We lift contexts into an MP trace in much the same way as we lifted policy states to get
a state type \(\mpcstatename \in \MPCS = \MACHS \times \POLS \times \CONTEXTS\).

\judgmentthree{\(\mpstate{\mach}{\pol} \stepstopol[\obs] \mpstate{\mach'}{\pol'}\)}
              {\(\codemap ~ (\mach ~ \PCname) = \funid, \ann\)}
              {\(\mach, \context, \ann \constep \context'\)}
              {\(\mpcstate{\mach}{\pol}{\context} \stepstocon[\obs]
                \mpcstate{\mach'}{\pol'}{\context'}\)}

\subsection{Traces}
\label{sec:traces}

A trace over a type \(\TRACEELEMS\) is a nonempty, finite or infinite sequence of
\(\TRACEELEMS\)s; we write \(\TRACE{\TRACEELEMS}\) for the set of traces over
\(\TRACEELEMS\). We will mostly be concerned with sequences of MPC-states,
called \emph{machine traces} and denoted \(\MPCT \in \TRACE{\MPCS}\), and of observations.

\paragraph*{Trace-Of}

The ``trace-of'' operator, written \(\mpcstatename \hookrightarrow \MPCT\),
coinductively relates an initial state with the trace of states
produced by repeated application of step, and potentially cut short by
the policy component. The {\sc Default} rule applies only if the other does not.
(We use juxtaposition of an element and a trace to represent ``cons.'')

\begin{minipage}{.4\textwidth}
\judgmenttwo{\(\mpcstatename \stepstocon \mpcstatename'\)}
            {\(\mpcstatename' \hookrightarrow \MPCT\)}
            {\(\mpcstatename \hookrightarrow \mpcstatename' \MPCT\)}%
\end{minipage}
\begin{minipage}{.4\textwidth}
\judgment[Default]
         {}
         {\(\mpcstatename \hookrightarrow \mpcstatename\)}
\end{minipage}
%

\paragraph*{Projections}

We project out the machine state of an MPC-state with \(\pi_\mach\), the
policy state with \(\pi_\pol\), and the context with \(\pi_\context\). We overload
these operators to project out of an machine-trace a trace of machine states,
policy states, and contexts, respectively.

\paragraph*{Observations-Of}

The ``observations-of'' operator, written \(\pi_\obs\), takes a machine trace and
creates a trace of observations corresponding to the observations of each step in
the trace. It uses a coinductive helper \(\mathit{obsforward}\) that relates an
initial observation, a machine-trace, and an observation trace:

\judgmentthree{\(\mpcstatename \stepstocon[\obs'] \mpcstatename'\)}
              {\(\mpcstatename' \hookrightarrow \MPCT\)}
              {\(\mathit{obsforward} ~ \obs' ~ \MPCT = \obsT\)}
              {\(\mathit{obsforward} ~ \obs ~ \mpcstatename \MPCT = \obs\obsT\)}%
\judgment{}
         {\(\mathit{obsforward} ~ \obs ~ \mpcstatename = \obs\)}

Then to take the observations from any particular trace, we start with a \(\tau\) observation:

\judgment{\(\mathit{obsforward} ~ \tau ~ \MPCT = \obsT\)}
         {\(\pi_\obs(\MPCT) = \obsT\)}
\noindent

In a similar fashion, we define \(\mpcstatename \hookrightarrow \MPCT\)
to relate a machine-policy-context state triple to the MPCO-trace
induced by the step function.

\paragraph*{Head and Last}

We take the first element of a trace with \(\head(\Trace)\), which is a total
function since traces are non-empty, and the final element (if one exists) with
\(\last(\Trace)\), which is partial.

\paragraph*{Until}
The operation \(\PUT ~ f ~ \Trace\) takes a trace
\(\Trace \in \TRACE{\TRACEELEMS \times \OBSS}\) and a predicate
on elements \(f \subset \TRACEELEMS\) and gives the prefix of
\(\Trace\) ending with the first element on which \(f\) holds.

\begin{center}
  \begin{minipage}{.3\textwidth}
    \judgment{\(f ~ \traceelem\)}
             {\(\PUT ~ f ~ \traceelem\Trace = \traceelem\)}
  \end{minipage}
%
  \begin{minipage}{.3\textwidth}
    \judgment{}
             {\(\PUT ~ f ~ \traceelem = \traceelem\)}
%
  \end{minipage}
  \begin{minipage}{.3\textwidth}
    \judgmenttwo{\(\neg f ~ \traceelem\)}{\(\PUT ~ f ~ \Trace = \Trace'\)}
                {\(\PUT ~ f ~ \traceelem\Trace = \traceelem\Trace'\)}
  \end{minipage}
\end{center}
%
If the resulting trace is a strict prefix of the parameter, then the
predicate must hold on its final element. If the predicate always holds, then
\(\PUT\) is the identity function.

% The {\it join} operation, written \(T_1 \cdot T_2\) is the reverse of
%$\mathit{split}$: it takes two traces and links them by replacing the
%last element of the first (if it has one) with the first element of
%the second

%\judgment{} {\(t \cdot T = T\)}
%\judgment{\(T_1 \cdot T_2 = T\)}{\(tT_1 \cdot T_2 = tT\)}
%\leo{Lemma/theorem: split/join = id?}
%\leo{TODO: FORMATTING: Remove force line break/par at end of judgments}

We will frequently take a prefix of the trace from an initial state up
through the first state where some condition holds on the machine state.
This can easily be implemented using \(\PUT\), and we provide special notation
for convenience.
%
If \(f\) is a predicate on machine states, we define \(\mpcstatename
\hookrightarrow \MPCT | f\) (read ``\(\MPCT\) is the prefix of
running \(\mpcstatename\) up to \(f\)'') and similarly for traces with policies:

\begin{center}
\judgmenttwo{\(\mpcstatename \hookrightarrow \MPCT'\)}
            {\(\PUT ~ f ~ \mpcstatename ~ \MPCT' = \MPCT\)}
            {\(\mpcstatename \hookrightarrow \MPCT | f\)}
\end{center}

\paragraph*{Context Segments}

In particular, we wish to formulate properties around consecutive subtraces
in which the context obeys a given predicate. This will commonly be used to extract
individual calls from a trace by taking subtraces in which the stack is
at or below a given depth. If \(f \subseteq \MACHS \times \CONTEXTS\), then
we have \(\mpcstatename \hookrightarrow \MPCT \langle f \rangle\), pronounced
``\(\mpcstatename\) segmented by \(f\) yields a subtrace \(\MPCT\).''

\begin{center}
  \judgmenttwo[Now]
              {\(f ~ \mach ~ \context\)}
              {\(\mach,\pol,\context \hookrightarrow \MPCT | \neg f\)}
              {\(\mach,\pol,\context \hookrightarrow \MPCT \langle f \rangle\)}
%
  \judgmentthree[Skip]
                {\(f ~ \mach ~ \context\)}
                {\(\mach,\pol,\context \hookrightarrow \MPCT | \neg f\)}
                {\(\last(\MPCT) \hookrightarrow \MPCT' \langle f \rangle\)}
                {\(\mach,\pol,\context \hookrightarrow \MPCT' \langle f \rangle\)}
%
  \judgmentthree[Search]
                {\(\neg f ~ \mach ~ \context\)}
                {\(\mach,\pol,\context \hookrightarrow \MPCT | f\)}
                {\(\last(\MPCT) \hookrightarrow \MPCT' \langle f \rangle\)}
                {\(\mach,\pol,\context \hookrightarrow \MPCT' \langle f \rangle\)}
\end{center}

\paragraph*{Reachability}

We often want to quantify over all traces from any initial state so that our
properties apply to the system as a whole. We say that a state \(\mpcstatename'\)
is {\em reachable} if for some initial state \(\mpcstatename\), \(\mpcstatename \hookrightarrow \MPCT\)
and \(\MPCT\) contains \(\mpcstatename'\). A {\it reachable segment} \(\MPCT \langle f \rangle\) is a subtrace
such that from some initial state \(\mpcstatename\), \(\mpcstatename \hookrightarrow \MPCT \langle f \rangle\).

\paragraph*{Observational Similarity}

We say that a trace of observations $\obsT_1$ is a prefix of $\obsT_2$
as far as an external observer is concerned, written \(\obsT_1 \lesssim
\obsT_2\), if the sequence of
non-silent observations of $\obsT_1$ is a prefix of those of
$\obsT_2$; that is, we operate up to deletion of \(\tau\) observations,
coinductively:

\begin{minipage}{.3\textwidth}
  \judgment{}{\(\obsT \lesssim \obsT\)}
\end{minipage}
\begin{minipage}{.3\textwidth}
\judgment{}{\(\tau \lesssim \obsT\)}
\end{minipage}
\begin{minipage}{.3\textwidth}
\judgment{}{\(w \lesssim w\obsT\)}
\end{minipage}

\begin{minipage}{.3\textwidth}
\judgment{\(\obsT_1 \lesssim \obsT_2\)}
         {\(\tau \obsT_1 \lesssim \obsT_2\)}
\end{minipage}
\begin{minipage}{.3\textwidth}
\judgment{\(\obsT_1 \lesssim \obsT_2\)}
         {\(\obsT_1 \lesssim \tau \obsT_2\)}
\end{minipage}
\begin{minipage}{.3\textwidth}
\judgment{\(\obsT_1 \lesssim \obsT_2\)}
         {\(w\obsT_1 \lesssim w\obsT_2\)}
\end{minipage}

\smallskip
We then define similarity of observation traces as traces prefixing each other:
\[\obsT_1 \simeq \obsT_2 \triangleq \obsT_1 \lesssim \obsT_2 \land \obsT_2 \lesssim \obsT_1\]
%
%\leo{I still don't know how to do this transition. Maybe contrast
%with the ``perhaps surprisingly'' later?} Note that
%similarity of observation traces could alternatively be defined coinductively,
%just like the prefix relation, but by dropping the asymmetric rules that allow
%for a finite trace ($\tau \lesssim \obsT$ or $w \lesssim w\obsT$).\apt{Is this immportant?}\leo{I agree. Probably not}

Note that an infinite silent trace is a
prefix of (and similar to) any other trace. While this might seem
surprising at first, it makes sense in a timing-insensitive context:
an external observer looking at two machine runs cannot (computably)
distinguish between a machine that steps forever and a machine that
steps for a long time before producing some output.

\section{Local State Encapsulation, Formally}
\label{sec:lse}

We are finally ready for our definition of stack safety. We begin
by describing how we model knowledge about the program structure,
then dive into the formal definitions of local state encapsulation and
well-bracketed control flow, which together constitute stack safety.
Initially we will focus on a simple stack model, with a single stack
that doesn't share between caller and callee. All arguments are passed
in registers.

We begin with our only annotations being {\em call} or {\em return}, and
define our code map accordingly.

\[\ann \in \ANNS ::= \callmap + \retmap\]
\[\codemap \in \CODEMAPS ::= \WORDS \rightharpoonup \FUNIDS \times \powerset(\ANNS)\]

\paragraph*{Call Depth and Domains}

At any given step of a program, some number of callers have pushed their frames
onto the stack, ``sealing'' that memory for later use; this number is the current
{\em call depth}. The call depth, as well as the association between stack addresses
and sealed frames, are dynamic and cannot be deduced from the state alone, so we will
track them over the course of an execution as additional context that informs the property.

Each component will be assigned to a {\em domain}, which may be {\em Outside} of the
stack entirely, {\em Unsealed} and therefore accessible, or {\em Sealed} by a particular
depth.

\[\stackDom \in \STKDOMS ::= \outside + \unsealed + \sealed{\depth}\]
\[\context \in \CONTEXTS ::= (\domMap \in \COMPONENTS \rightarrow \STKDOMS)
  \times (\depth \in \mathbb{N}) \]

An initial context \(\context_0 = \domMap_0, 0\) where \(\domMap_0\) maps
the stack to \(\unsealed\) and all other addresses and all registers to \(\outside\).
Then at each step we update the context according to the context-step function,
\(\stepsto_C\). We will define this in terms of the operations
\(\push{\domMap}{n}{\components}\), in which \(\components \subseteq \COMPONENTS\),
and \(\pop{\domMap}{n}\), pronounced ``push'' and ``pop'' and defined as follows.

\[
(\push{\domMap}{\depth}{\components}) ~ \component =
\begin{cases}
  \sealed{\depth} & \text{if } \domMap ~ \component = \unsealed
                    \text{ and } \component \in \components \\
  \domMap ~ \component & \text{else} \\
\end{cases}
\]
\[
(\pop{\domMap}{\depth}) ~ \component =
\begin{cases}
  \unsealed & \text{if } \domMap ~ \component = \sealed{\depth-1} \\
  \domMap ~ \component & \text{else} \\
\end{cases}
\]

Then we can define the context update relation. Let \(\sealcon\) be the sealing convention
that maps a machine state to a set of components, typically the set of addresses below
the stack pointer. The {\sc Default} rule applies only if neither of the others do.

\begin{center}
\begin{minipage}{.4\textwidth}
  \judgment{}
           {\(\mach,(\domMap,\depth),\{\callmap\} \stepstocon
             \push{\domMap}{\depth}{\sealcon ~ \mach},\depth+1\)}
\end{minipage}
\hspace*{0.1\textwidth}
\begin{minipage}{.4\textwidth}
  \judgment{}
           {\(\mach,(\domMap,\depth),\{\retmap\} \stepstocon
             \pop{\domMap}{\depth},\depth-1\)}
\end{minipage}
\end{center}

\begin{center}
  \judgment[Default]
           {}
           {\(\_,\context,\_ \stepstocon \context\)}
\end{center}

\subsection{Ultra-eager Integrity}

Our first property will be {\em ultra eager}, meaning that it will be quantified over each step
of execution from any initial state. The criterion for integrity is simple: sealed components do not
change.

\definition A system enjoys {\em ultra-eager integrity} if for any reachable state
\(\mpcstatename\), \(\mpcstatename'\) such that \(\mpcstatename \stepstocon \mpcstatename'\),
and component \(\component\), if \(\pi_\context(\mpcstatename) = \domMap,\_\) and
\(\domMap ~ \component = \sealed{\_}\), then \(\pi_\mach(\mpcstatename) ~ \component =
\pi_\mach(\mpcstatename') ~ \component\).


\subsection{Eager Integrity}

\definition Let \(\depth\) be any depth and \(f_\depth ~ \mach ~ (\_, \depth')\) hold when
\(\depth' \geq \depth\). A system enjoys {\em eager integrity} if each reachable segment
\(\MPCT \langle f_\depth \rangle\) satisfies \(\pi_m(\head(\MPCT)) ~ \component =
\pi_m(\last(\MPCT)) ~ \component\) for all \(\component\) where \(\domMap ~ \component =
\sealed{\_}\).

\subsection{Eager Confidentiality}

\definition Machine states \(\mach\) and \(\nach\) are {\em \(\components\)-variants} if, for
all \(\component \not \in \components\), \(\mach ~ \component = \nach ~ \component\). We write
this \(\mach \approx_\components \nach\).

\definition Machine state pairs \(\mach,\mach'\) and \(\nach,\nach'\) have the {\em same differences}
if for all components \(\component\) such that \(\mach ~ \component \not = \mach' ~ \component\) or
\(\nach ~ \component \not = \nach' ~ \component\), \(\mach' ~ \component = \nach' ~ \component\).
This is written \(\mach,\mach' \diamond \nach,\nach'\).

\definition Let \(\depth\) be any depth and \(f_\depth ~ \mach ~ (\_, \depth')\) hold for
\(\depth' \geq \depth\). For any reachable segment \(\MPCT \langle f_\depth \rangle\),
let \(\head(\MPCT) = \mach,\pol,(\domMap,\depth)\) and
\(\components\) be the set of components \(\component\) such that \(\domMap ~ \component \not = \outside\).
Then take any \(\nach\) such that \(\mach \approx_\components \nach\). \(\MPCT\) corresponds to the
trace of a subroutine from its call to its return.

If we take the induced trace from this variant until a return,
\(\nach,\pol,\context \hookrightarrow \NPCT | \neg f_\depth\), we get a subtrace corresponding
to the same subrouting called with the same (register) arguments but with a potentially different
state at the call-site. A system enjoys {\em eager confidentiality} if for all such pairs of traces,
they correspond according to one of four cases:

\begin{enumerate}
\item \(\MPCT\) may be finite and \(\pi_\context(\last(\MPCT)) = (\_,\depth')\) with \(\depth' < \depth\),
  and \(\pi_\context(\last(\NPCT)) = (\_,\depth')\) with \(\depth' < \depth\). That is, both traces end
  having left the call of depth \(\depth\). Further:
  \begin{itemize}
  \item They have the same observable behavior, \(\pi_\obs(\MPCT) \simeq \pi_\obs(\NPCT)\)
  \item Their final states have the same differences:
    \[\pi_\mach(\head(\MPCT)), \pi_\mach(\last(\MPCT)) \diamond
    \pi_\mach(\head(\MPCT)), \pi_\mach(\last(\NPCT))\]
  \end{itemize}
\item \(\MPCT\) and \(\NPCT\) may both be infinite, with \(\pi_\obs(\MPCT) \simeq \pi_\obs(\NPCT)\)
\item \(\MPCT\) may be finite but end without returning -- which can only have happened if it fail-stopped --
  and its observable behavior prefixes that of \(\NPCT\), \(\pi_\obs(\MPCT) \lesssim \pi_\obs(\NPCT)\)
\item \(\NPCT\) may end with a fail-stop as in case 3, with \(\pi_\obs(\NPCT) \lesssim \pi_\obs(\MPCT)\)
\end{enumerate}

\subsection{Trace Properties}

Having seen instances of eager confidentiality and integrity, defined in terms of reachable trace
segments, we can generalize them to properties of the trace segments themselves. Then as we expand
our model, our further properties will be defined in terms of the trace properties.

\definition Let \(\MPCT\) be a reachable segment and \(\components\) be any set of components.
\(\MPCT\) enjoys {\em trace integrity} when \(\last(\MPCT) ~ \component = \head(\MPCT) ~ \component\)
for all \(\component \in \components\), and when \(\last(\MPCT)\) is undefined.

\definition Let \(\MPCT \langle f \rangle\) be a reachable segment delimited by the predicate \(f\)
and the \(\components\) be any set of components. Let \(\head(\MPCT) = \mpcstate{\mach}{\pol}{\context}\)
and \(\nach\) be a machine state such that \(\mach \approx_\components \nach\). Suppose we take the
induced trace from \(\npcstatename = \mpcstate{\nach}{\pol}{\context}\) that corresponds to \(\MPCT\),
\(\npcstatename \hookrightarrow \NPCT | \neg f\). The trace \(\MPCT\) enjoys {\em trace confidentiality}
with respect to \(f\) and \(P\) if, for all such \(\NPCT\), they correspond according to the four cases
above:

\begin{enumerate}
\item \(\MPCT\) may be finite and \(\pi_\context(\last(\MPCT)) = (\_,\depth')\) with \(\depth' < \depth\),
  and \(\pi_\context(\last(\NPCT)) = (\_,\depth')\) with \(\depth' < \depth\). That is, both traces end
  having left the call of depth \(\depth\). Further:
  \begin{itemize}
  \item They have the same observable behavior, \(\pi_\obs(\MPCT) \simeq \pi_\obs(\NPCT)\)
  \item Their final states have the same differences:
    \[\pi_\mach(\head(\MPCT)), \pi_\mach(\last(\MPCT)) \diamond
    \pi_\mach(\head(\MPCT)), \pi_\mach(\last(\NPCT))\]
  \end{itemize}
\item \(\MPCT\) and \(\NPCT\) may both be infinite, with \(\pi_\obs(\MPCT) \simeq \pi_\obs(\NPCT)\)
\item \(\MPCT\) may be finite but end without returning -- which can only have happened if it fail-stopped --
  and its observable behavior prefixes that of \(\NPCT\), \(\pi_\obs(\MPCT) \lesssim \pi_\obs(\NPCT)\)
\item \(\NPCT\) may end with a fail-stop as in case 3, with \(\pi_\obs(\NPCT) \lesssim \pi_\obs(\MPCT)\)
\end{enumerate}

\subsection{Stack Safety with Sharing}

Next we introduce sharing. We will need to extend the set of annotations to include
sharing and passing, and now the code map may map a single instruction to multiple
annotations, because in some calling conventions a single instruction might carry
meaning as both a call and a pass. But we will never have a single instruction annotated
as both a call and a return.

\[\ann \in \ANNS ::= \callmap + \retmap + \sharemap(P \subseteq \MACHS \times \COMPONENTS) +
\passmap(P \subseteq \MACHS \times \COMPONENTS)\]

And we extend the set of domains to include shared and passed components, the latter of
which are labeled with the depth of the caller that has passed them.

\[\stackDom \in \STKDOMS ::= \outside + \unsealed + \shared{\depth} + \sealed{\depth} + \passed{\depth}\]
\[\context \in \CONTEXTS ::= (\domMap \in \COMPONENTS \rightarrow \STKDOMS)
\times (\depth \in \mathbb{N}) \]

What it means to push the stack is similar to above, but two details are worth noting.
First, shared and passed components are not sealed by the sealing convention, and second,
anything not touched by the sealing convention must be unsealed. Sharing and passing cannot
apply to memory the callee could otherwise use.

\[
(\push{\domMap}{\depth}{\components}) ~ \component =
\begin{cases}
  \sealed{\depth} & \text{if } \domMap ~ \component = \unsealed
                    \text{ and } \component \in \components \\
  \domMap ~ \component & \text{else if } \component \in \components \\
  \unsealed & \text{else} \\
\end{cases}
\]

Popping is simpler, but note that while passed components are unsealed on return to their
passer, shared ones are unsealed on return {\em from} their sharer - on deallocation,
in other words.

\[
(\pop{\domMap}{\depth}) ~ \component =
\begin{cases}
  \unsealed & \text{if } \domMap ~ \component = \sealed{\depth-1}
              \text{ or } \domMap ~ \component = \passed{\depth-1}
              \text{ or } \domMap ~ \component = \shared{\depth} \\
  \domMap ~ \component & \text{else} \\
\end{cases}
\]

Finally we introduce a more direct update. If \(P\) is a relation between machine states and
components, we write \(\domMap[P ~ \mach \mapsto \stackDom]\) to say \(\domMap\) updated
so that each component \(\component\) where \(P ~ \mach ~ \component\) holds is mapped to
\(\stackDom\).

Now we can inductively define the context update function; as these could be ambiguous assume that
they apply in the order presented.

\begin{center}
  \judgment{\(\mach,\context,\ann \stepstocon (\domMap,\depth)\)}
           {\(\mach,\context,(\{\callmap\} \cup \ann) \stepstocon
             (\push{\domMap}{\depth}{\sealcon ~ \mach},\depth+1)\)}
  \judgment{\(\mach,\context,\ann \stepstocon (\domMap,\depth)\)}
           {\(\mach,\context,(\{\retmap\} \cup \ann) \stepstocon
             \mach',(\pop{\domMap}{\depth},\depth-1)\)}
  \judgment{\(\mach,\context,\ann \stepstocon (\domMap,\depth)\)}
           {\(\mach,\context,(\{\passmap ~ P\} \cup \ann) \stepstocon
             \domMap[P ~ \mach \mapsto \passed{\depth}], \depth)\)}
  \judgment{\(\mach,\context,\ann \stepstocon (\domMap,\depth)\)}
           {\(\mach,\context,(\{\sharemap ~ P\} \cup \ann) \stepstocon
             \domMap[P ~ \mach \mapsto \shared{\depth}], \depth)\)}
  \judgment{}
           {\(\mach,\context,\_ \stepstocon \context\)}
\end{center}

\definition Let \(\MPCT\) be a reachable trace segment of the form
\(\mpcstatename ~ \mpcstatename'\), where \(\pi_\context(\mpcstatename) = (\domMap,\depth)\).
Let \(\components\) be the set of components \(\component\) such that either
\(\domMap ~ \component = \sealed{\_}\) or \(\domMap ~ \component = \passed{\depth'}\) for
some depth \(\depth' < \depth-1\). If any such \(\MPCT\) obeys trace integrity with respect
to \(\components\), then the system enjoys {\em ultra-eager confidentiality} in the sharing
model.

\definition Let \(\MPCT \langle f \rangle\) be a reachable trace segment where \(f\) holds
on contexts whose depth is less than or equal to some \(\depth\). Let \(\head(\MPCT) =
\mpcstate{\mach}{\pol}{(\domMap,\depth')}\) and \(\components\) be the set of components
\(\component\) such that either \(\domMap ~ \component = \sealed{\_}\) or
\(\domMap ~ \component = \passed{\depth''}\) for some depth \(\depth'' < \depth'-1\).
Then the system enjoys {\em eager integrity} in the sharing model if \(\MPCT\) obeys
trace integrity with respect to \(\components\).

\definition Let \(\MPCT \langle f \rangle\) be a reachable trace segment where \(f\) holds
on contexts whose depth is less than or equal to some \(\depth\). Let \(\head(\MPCT) =
\mpcstate{\mach}{\pol}{(\domMap,\depth')}\) and \(\components\) be the set of components
\(\component\) such that either \(\domMap ~ \component = \outside\) or
\(\domMap ~ \component = \shared{\_}\). Then the system enjoys {\em eager confidentiality}
in the sharing model if \(\MPCT\) obeys trace confidentiality with respect to \(\components\)
and \(f\).

\subsection{Coroutine Safety}

Our final extension to the model is to add coroutines. We assume a very simple model with
a fixed number of coroutines, each with its own stack.
Stack identifiers are drawn from a set \(\STACKS\), and there is a static partial map
\(\stackof\) from addresses to stack ids. The ``active stack'' -- the stack belonging to the
currently executing coroutine -- in any given machine state is \(\stackof(\mach ~ \SP)\).
We extend the set of possible annotations with a yield.

\[\ann \in \ANNS ::= \callmap + \retmap + \yieldmap +
\sharemap(P \subseteq \MACHS \times \COMPONENTS) +
\passmap(P \subseteq \MACHS \times \COMPONENTS)\]

We use the same domain model as above, but instead of keeping track of a single depth
we use a map from stacks to their depths.

\[\context \in \CONTEXTS ::= (\domMap \in \COMPONENTS \rightarrow \STKDOMS) \times
(\depthMap \in \STACKS \rightarrow \mathbb{N})\]

\subsection{Local State Encapsulation}

\definition A system enjoys {\em local state encapsulation} if it enjoys eager integrity and
eager confidentiality.

\definition A system enjoys {\em lazy local state encapsulation} if it enjoys lazy integrity
and eager confidentiality.

We do not give a confidentiality equivalent to lazy integrity, because the family of lazy
enforcement mechanisms that we consider under the HOPE system in fact enforce eager confidentiality.
A policy based on taint-tracking might require such a lazy confidentiality, and it would be constructed
by replacing the ``same-difference'' condition with one that requires any differences to produce
observationally equivalent states. But for our purposes it is irrelevant.

\section{Well-bracketed Control Flow}
\label{sec:wbcf}

Both the integrity and confidentiality components of local state
encapsulation concern themselves only with the accessibility of data,
and may still hold on a wide range of programs that violate expected
control flow. The archetypal example of this is the {\em awkward example},
in which an adversarial callee stores a return address and returns to it
on a subsequent call, thus violating the expected control flow of the
caller without actually violating the caller's integrity. But it also
encompasses simpler violations, such as a function bypassing a password
check by jumping (or returning) in the middle of a block of code.
%
Stack safety enforcement mechanisms like \citet{Skorstengaard+19}
usually aim to prevent such unwanted flows: every jump between
functions should either be a call or a return, and each return should
be to the instruction after its corresponding call.

To formally capture this aspect of stack safety, we introduce some more
security properties. {\em Control separation} restricts transitions between
functions to follow the interfaces set out by call, return, and yield, forming
a foundation for all of the other properties that rely on those interfaces for
meaning. {\em Entry integrity} ensures the validity of call targets,
{\em return integrity} ensures that of return targets, and
{\em yield-back integrity} ensures that of yield targets.

Recall that our properties use a compiler-derived code-map determine
which addresses are instructions and which functions they belong to, as well
as annotating them as calls, returns, and yields, as well as sharing information.
We now extend it with an annotation for valid entry points:
\[\ann \in \ANNS ::= \powerset(\{\callmap, \retmap, \yieldmap, \entmap\} \cup
\{\sharemap ~ P | P \subseteq \MACHS \times \COMPONENTS \} \cup
\{\passmap ~ P | P \subseteq \MACHS \times \COMPONENTS \})\]

\definition%{Control Separation}

Let \(\mpcstatename\) be a reachable state whose program counter points to code in
function \(\funid\), and \(\mpcstatename \stepstopol \mpcstatename'\) where
\(\mpcstatename'\) is state whose program counter points to a different function
\(\funid'\). A system enjoys {\em control separation} when all such \(\mpcstatename\)
are annotated \(\callmap\), \(\retmap\), or \(\yieldmap\).

\definition%{Entry Integrity}
A system enjoys {\em entry integrity} when, for any reachable \(\mpcstatename\) and
\(\mpcstatename'\) such that \(\mpcstatename\) is annotated \(\callmap\) and
\(\mpcstatename \stepstopol \mpcstatename'\), \(\mpcstatename'\) is annotated \(\entmap\).

\definition%{Return Integrity}
A system enjoys {\em return integrity}... [this one needs to talk about context.]

\definition%{Yield-back Integrity}
A system enjoys {\em yield-back integrity}... [same.]

\definition% {Well-Bracketed Control Flow}
The above properties are somewhat orthogonal, but together they
capture a standard notion of control flow for a stack: functions keep
their code separate outside of calls and returns, calls respect
explicit entry points, and returns respect implicit entry points
corresponding to their call.  A system with all three
properties (with respect to a function map, call map, return map,
and entry map) is said to enjoy
{\em well-bracketed control flow} (with respect to those maps).

\section{Enforcement}
\label{sec:enforcement}

In this section we examine how an existing enforcement mechanism,
Depth Isolation from \citet{DBLP:conf/sp/RoesslerD18}, can implement
the formal stack-safety property described in the previous section: it in
fact enforces a stronger, inductive version of that property which strengthens the
local state encapsulation component by checking for integrity and
confidentiality violations at every step of the callee's execution trace.

%\paragraph*{Micro-Policies}
%
The enforcement policy of \citeauthor{DBLP:conf/sp/RoesslerD18} relies
on a programmable, tag-based reference monitor that runs alongside the
program.  To control this monitor, they use a programming model that
allows fine-grained manipulation of metadata tags to encode so-called
\emph{micro-policies}~\citep{pump_oakland2015}.
In such a system, all values in
memory addresses and registers (including the $\PCname$)
are enriched with an abstract metadata tag, which can represent
arbitrary information about the value. A micro-policy is defined as a set of tags and a
collection of software-defined rules, indexed by machine opcode.
At each step of the machine, the relevant rule is applied to the tags on
the instruction's inputs (PC, registers, memory) and on the instruction itself, and produces
one of two outcomes: either the instruction is allowed to execute (and generates
tags for the result of the operation and the new PC), or the machine
fail-stops with a policy violation.
\citeauthor{pump_oakland2015} have shown that a wide range of micro-policies can
be defined using this scheme.

Efficient execution of these micro-policies relies on hardware implementations,
such as the PUMP architecture~\citep{pump:asplos2015}.  Tags are represented
as word-size bit vectors, stored separately from the regular memory and registers.
The hardware incorporates a rule cache
to allow quick retrieval of rule outputs for mapped inputs. If the cache misses,
the hardware traps to a software handler (running in a privileged context or
on a co-processor) to compute the rule result. To obtain adequate performance,
it is important to design micro-policies so that they hit in the cache as
much as possible. Thus, practical policies maintain a small working set of
distinct tags.
%% %
%% \rb{Maybe go into more detail, examples later\ldots or try to segue into stack
%% policies}

\subsection{A Conservative Tag Policy}
\label{sec:conservative}
%
The first micro-policy we present to enforce the definition of stack
safety developed in \cref{sec:lse-and-wbcf} is a small variation
on the Depth Isolation policy presented by
\citet{DBLP:conf/sp/RoesslerD18}. Their policy tags the stack memory
with ownership information associated to each stack frame (and to each
separate object inside that frame, a more finely grained access
control that we do not need to consider here), and tags registers containing
stack pointers with access permission information. The policy also
uses tags on instructions to identify the code
sequences that have permission to manage the stack, say during calls and returns.
%% \rb{Conceivably on alloc
%%   operations, etc., which we have not yet mentioned.}

Our micro-policy works as follows. (We assume for ease of exposition that no
arguments are passed on the stack.)
%% \rb{Come back to this later?}.
The micro-policy maintains tags on values in memory of the form $\tagStackDepth{n}$,
indicating locations that belong to the stack frame at activation depth $n$,
or $\tagNoDepth$; it tags the $\PCname$ with tags of the form $\tagPCDepth{n}$.
During normal execution, the micro-policy rules only permit load and store operations
when the target memory is tagged with the same depth as the current {\PCname} tag.
Initially, the entire stack is tagged $\tagNoDepth$,
%\rb{actually, the ``initial frame'' should be tagged with its proper depth}
and the {\PCname} has tag $\tagPCDepth{0}$. These tags are altered
at exactly those points in
the program where the contour of the stack-safety property changes:

\begin{itemize}

\item From caller to callee, when the machine executes an instruction marked as a call in
  the call map.
  At this point, the current
  $\tagPCDepth{n}$ tag is incremented to $\tagPCDepth{(n + 1)}$, and the function
  entry sequence initializes  all locations of the new function frame with
  tag $\tagStackDepth{(n + 1)}$.

\item From callee back to caller, when execution is about to reach the return point
  $\ret{c}$
  corresponding to the call at $c$.  The exit sequence retags all locations in the function
  frame with $\tagNoDepth$. At the actual return instruction, the
  $\PCname$ tag is decremented.

\end{itemize}

This discipline suffices to enforce local stack encapsulation.
To implement it,  we define ``blessed'' instruction sequences
intended to appear at the entry and exit of each function,
which manipulate tags as just described in addition to performing the
usual calling convention tasks of saving/restoring the return address to/from
the stack and adjusting the stack pointer. With the aid of an additional tag on
the $\rsp$ register, these sequences also serve to enforce
well-bracketed control flow.
The micro-policy guarantees atomic execution of these sequences
using a combination of tags on the instructions
and an additional tag on the $\PCname$; we omit the details here.

There remains the question of how to ensure that the sequences are
invoked at the right places. For a program to enjoy the stack-safety property
(with respect to a particular call map), all we need require is that
the entry sequence be initiated at any instruction marked as a call
in the call map. This is achieved by giving these instructions (another) special tag,
and it is easy to check statically that this has been done correctly.
If the code fails to initiate an exit
sequence at a point where the stack-safety property expects a return,
the micro-policy will incorrectly behave as if execution
is continuing in the callee, but since the callee never has stronger access
rights than the caller, this is harmless.

%% APT: removed because the tag part is too mysterious and the instruction part
%% is standard.
%% This is the entry
%% sequence:
%% %
%% \setcounter{pcctr}{1}
%% \[
%%   \begin{array}{l|l|l|l}
%%      & \mathit{Instruction} & \mathit{Tags} & \\
%%     \hline
%%     \row{\sw ~ \rsp ~ \tta ~ 1}{[\rsp+1] \leftarrow \rra}{\tagHa, \tagInstr}
%%         {Store return address in stack}
%%     \row{\addi ~ \rsp ~ \rsp ~ 2}{\rsp \leftarrow \rsp + 2}{\tagHb, \tagInstr}
%%         {Increment stack pointer by frame size}
%%   \end{array}
%% \]
%% %
%% And the exit sequence:
%% %
%% \setcounter{pcctr}{1}
%% \[
%%   \begin{array}{l|l|l|l}
%%      & \mathit{Instruction} & \mathit{Tags} & \\
%%     \hline
%%     \row{\lw ~ \rra ~ \rsp ~ \negate 1}{\rra \leftarrow [\rsp - 1]}{\tagRa, \tagInstr}
%%         {Load return address}
%%     \row{\addi ~ \rsp ~ \rsp ~ \negate 2}{\rsp \leftarrow \rsp - 2}{\tagRb, \tagInstr}
%%         {Decrement stack pointer by frame size}
%%     \row{\jalr ~ \rra ~ \rra ~ 0\apt{wrong}}{\PCname \leftarrow \rra}{\tagRc, \tagInstr}
%%         {Return to caller ($\jalr$)}
%%   \end{array}
%% \]

%% \rb{Positioning of the above sequences, relation to running example. The tagging
%%   of stack frames would be considered part of these sequences.}

%% These sequences also enforce the $\SP$ discipline. When stack frames
%% are of fixed size, it suffices to increment $\SP$ and tag it with a
%% dedicated tag in the header sequence, which is later checked after
%% decrementing $\SP$ during the exit sequence to authorize the
%% return. The well-formedness of the blessed sequences can be checked
%% statically.

%% Only programs whose sequences are well-formed are
%% protected by the micro-policy, although programs are still protected
%% (by fail-stopping) even if the intended sequences are missing. By
%% tracking the execution of the blessed sequences in the tag state of
%% the {\PCname} register, the micro-policy ensures that the entry
%% sequences of calls declared in the call map are executed correctly.

Note that in this policy both the entry and exit sequences
must write to each element of
the frame, which can be quite expensive, especially for programs that
allocate large but sparsely populated frames; we return to this point
in \cref{sec:lazy}.
%
The running example in \cref{fig:running-trace-a} manages the stack in this way.
For example, {\tt f}'s ``blessed'' sequences allocate space to save its return
address (and private variables) in instructions 20--21, and later restores that
return address and deallocates its private frame space before returning in
34--36. Additionally, unlike in the simplified presentation of the policy, the
caller passes information to the callee through the stack, and for this creates
and initializes (22 and 24--25) and destructs (29) the argument part of the
callee's stack frame.\rb{The policy would also need to control this, I think it
  is clear}
%
%\rb{Note that we have also considered and implemented more flexible schemes
%  based on allowing operations above the current activation depth. How do we put
%  these together?}
%\rb{Do we want to discuss variations involving frame pointers or more implicit
%  return addresses, argument passing on the stack, alloc/dealloc, etc.?}
%\rb{Where to talk about the role of the compiler?}

%% Because the access rights of callers subsume those of callees, it is essential
%% that the micro-policy change tags when the property thinks that a call occurs,
%% but it is safe for the micro-policy to fail to reset to the caller's tag when the
%% property thinks a return occurs. (Dually, it is safe for the micro-policy to
%% change tags even without executing a call point, but not for it to reset tags to
%% the caller when the property thinks execution is still in the caller).

%% Because the property does not identify return sites, we have no way to require
%% that ``intended returns'' are tagged correspondingly. Some unintended results
%% might also be so tagged, but as long as the jump has the effect of a legal
%% return, it is OK.

\section{Randomized Testing}

\section{Stack Safety for Lazy Enforcement}
\label{sec:lazy}

The conservative policy described in the previous section, while
testable and enforceable, is rather slow to be of practical use. What
we want, instead, is a somewhat more permissive policy that can also
be efficiently implemented. Most of the performance overhead incurred
stems from the need to set stack activation tags as a frame is created
and reset them as it is destructed, as
\citet{DBLP:conf/sp/RoesslerD18} confirm in their evaluation. To
mitigate those costs, they propose optimizations called Lazy Tagging and
Lazy Clearing that relax and defer some of the checks of the Depth Isolation
policy. Under these optimizations, the policy does not initialize stack
frames on entry or clear them on exit (thus speeding up execution),
and it permits all writes to the stack, even when the $\PCname$ tag
does not match the memory tag.
However, reads from the stack do still require $\PCname$ tag and memory tag
to match. Thus, even if a callee illicitly writes to a private location
in its caller, the caller will eventually detect this if it ever tries to read from
that location.
%
These \emph{lazy policies} admit more efficient implementations, but
they deliberately allow violations of stack integrity temporarily,
with the checks deferred until the point a violation truly becomes
harmful. The natural question then is, how do we characterize the
protections provided by these policies once stack safety is broken?
How does a harmful violation that must be caught later differ from a
harmless one? The properties in this section rely on our notion of
observations to draw this distinction.

\subsection{Observable Integrity}

\paragraph{Rolling Back to Idealized State}

Observable integrity weakens eager integrity by allowing a returned
state to differ from its call state at high integrity components,
provided that the differences do not change the observable behavior of
the rest of the program. The challenge here is to define what constitutes
a change in behavior. We construct an idealized return state that
contains only the changes that the caller was allowed to make, and use
the trace from that state as a baseline against which the real trace
will be compared.

      \definition

      Given a contour, a call state, and a returned state, we define a
      {\em rollback} function \(\mathit{roll} \in \CONTOURS \times \MACHS \times
      \MACHS \rightarrow \MACHS\). The rollback returns a state that matches
      the call state on those components that are high integrity in its
      contour, and matches the return state on low integrity components:
      \[\mathit{roll}(\contour,\mach_e,\mach_r)(\component) =
      \begin{cases}
        \mach_e(\component) & \text{if } \contour(\component) = (\HIGHINT,\_) \\
        \mach_r(\component) & \text{if } \contour(\component) = (\LOWINT,\_) \\
      \end{cases}\]

      If \(\mach_e\) is an entry to a call with contour \(\contour\)
      and \(\mach_r\) its return, the result of
      \(\mathit{roll}(\contour,\mach_e,\mach_r)\) agrees with the
      entry \(\mach_e\) on every component the callee should not
      change, and with the actual return \(\mach_r\) on those
      components that it is permitted to change.  So the observable
      behavior that follows is the idealized behavior to which we
      compare the trace from \(\mach_r\).

      Now we can define a version of trace integrity that compares the trace
      from a returned state to that from its idealized counterpart.

    \definition%{Observable Trace Integrity}

      A trace enjoys {\em observable trace integrity} with respect to
      some contour \(\contour\) if the trace induced by its final
      state (if any) has observable behavior that prefixes that of its
      idealized version (to account for premature halts due to policy
      violations):

      \[\begin{split}
        \propdef{ObsTraceInt} ~ \contour ~ \MPT \ \ \triangleq \ \
        & \head(\MPT) = \mpostate{\mach_e}{\_}{\_} ~ \land ~
          \last(\MPT) = \mpostate{\mach_r}{\pol_r}{\_} ~ \land \\
        & \mpstate{\mach_r}{\pol_r} \hookrightarrow \MPT' ~ \land ~
          \mathit{roll}(\contour,\mach_e,\mach_r) \hookrightarrow \machT'
          \Rightarrow \\
        & \pi_o(\MPT') \lesssim_O \pi_o(\machT')
      \end{split}\]

%    \begin{figure}
%      \integritylazyexample
%      \caption{Rolling back from Figure \ref{fig:intex}.}
        %\apt{But at 27, $[\rsp]$ contains 0, not 18, because of the change at 44. Could just fix diagram, but this complicates story.}}
%      \label{fig:intlex}
%    \end{figure}

    For our example, we return to the violation of trace integrity in
    \cref{fig:intex}, in which {\tt g} has written to {\tt z} and returned.
    Whether observable integrity holds depends on the continuing trace, shown
    in \cref{fig:intlex}. Above, we have the final state of \(\MPT\),
    \(\mach_r\), and the rolled-back state, \(\mach_r' =
    \mathit{roll}(\contour,\mach_e,\mach_r)\), in which high integrity
    components are restored to their values as of \(\mach_e\). Specifically,
    address 103 is restored to 0. When instruction 31 prints {\tt z} in the
    trace from \(\mach_r\), it prints 18, but in the trace from \(\mach_r'\)
    it prints 0. Only then is observable trace integrity violated.

    However, consider instead if we modify the original code of \cref{fig:running-trace-b} to insert after instruction 44 a new
    instruction \(44.5: [\rsp-4] \leftarrow 50\), overwriting {\tt w} in
    {\tt f}'s frame. So \(\mach_r'\) maps address 105 to 50, while the
    rolled-back state \(\mach_r'\) maps it to 0. But in the trace from either
    state, instructions 27 and 28 load the return value (18) and store it to
    105, eliminating the only difference. The remainder of the observation
    traces must be identical, so this trace does obey observable trace
    integrity.

    \definition%{Observable Stack Integrity}

      A system enjoys {\em observable stack integrity}, like stack integrity,
      if, for any initial state and each \(n\)-transition from
      \(\mpstate{\mach_c}{\pol_c}\) to \(\mpstate{\mach_e}{\pol_e}\) in its
      induced trace, we take the prefix of the call
      \(\mpstate{\mach_e}{\pol_e} \hookrightarrow \MPT_{\mathit{call}} ~ | ~ \ret{\mach_c}\)
      and observable trace integrity holds on \(\MPT_{\mathit{call}}\) with
      contour \(\mathit{Cof}(\mach_c,n)\).

  \subsection{Observable Confidentiality}

      Two of the cases of trace confidentiality (\cref{def:trace-conf}), \(\mathit{InfCase}\)
      and \(\mathit{FailCase}\), are defined solely in terms of
      observation traces. If
      the traces are infinite, they should be similar; if one
      fail-stops, it should be a prefix. Only the
      \(\mathit{ReturnCase}\) checks machine states, because in that
      case the traces could continue, but have been cut off by a
      return. The key to making confidentiality lazy is not to perform that
      check but rather to consider the traces after that return,
      checking only whether they are observationally similar.

      This requires restoring varied components to their original
      values in the trace from the variant state, since after return
      they will once again be legitimately accessible. To return to
      the example in \cref{fig:confex}, the varied final state
      \(\mach_r'\) still has $-5$ as a return address for {\tt f}, so
      the trace after that return will behave oddly in a way that
      does not reflect a confidentiality violation.  That means we
      cannot immediately compare traces from \(\mach_r\) and
      \(\mach_r'\), but rather that we must restore the varied values
      while preserving differences that have occurred during
      execution.

      \definition

      The \(\mathit{restore}\) function takes four states: the
      original entry state, a variant entry state, and corresponding return
      states. It returns another state. The resulting state restores those
      components that varied between entries to match the original entry,
      unless they were changed during execution:
      \[\mathit{restore}(\mach_e,\mach_e',\mach_r,\mach_r')
        (\component) =
        \begin{cases}
          \mach_e(\component) & \text{if } \mach_e(\component) \not =
                                \mach_e'(\component) \text { and } \\
          & \mach_e(\component) = \mach_r(\component) \text{ and }
            \mach_e'(\component) = \mach_r'(\component) \\
          \mach_r'(\component) & \text{else} \\
        \end{cases}\]

      Now we can describe a trace property akin to trace confidentiality, but
      continuing execution after the end of the callee trace.

    \definition{%Observable Trace Confidentiality}

      {\em Observable trace confidentiality} is the lazy analog to
      trace confidentiality, and reuses the cases in which the traces
      do not return.  For a return predicate \(R\) and a pair of
      traces \(\MPT\) and \(\machT\), we consider the return case, in
      which $\MPT$ has a last element
      \(\mpostate{\mach_r}{\_}{\_}\) and that machine state is
      a return, i.e. \(R ~ \mach_r\) holds.
%
      Observable trace confidentiality then requires that $\machT$
      also has a final state \((\mach_r',\_)\) that is a return, i.e.
      \(R ~ \mach_r'\) holds.
%
      In addition, just like in the eager case,
      observations of \(\MPT\) are similar to those of \(\machT\).

      The only thing that changes is the requirement that \(\mach_r\)
      and \(\mach_r'\) agree on components. Instead, let \(\mach_e\)
      be the machine state of \(\head(\MPT)\), and \(\mach_e'\) be
      that of \(\head(\machT)\). We restore \(\mach_r'\) based on
      \(\mach_e\) and \(\mach_e'\) to get \(\mach_r'' =
      \mathit{restore}(\mach_e,\mach_e',\mach_r,\mach_r')\).  Then
      observable trace confidentiality requires that the
      induced trace from \(\mpstate{\mach_r}{\pol_r}\) should be an
      observational prefix of the induced trace from \(\mach_r'\),
      reflecting the possibility of a fail-stop after return.

      \[\begin{split}
        \mathit{ObsReturnCase} ~ R ~
        (\mpostate{\mach_e}{\_}{\_}\MPT) ~
        ((\mach_e',\_)\machT) \ \ \triangleq \ \
        & \last(\MPT) = \mpostate{\mach_r}{\pol_r}{\_} ~ \land ~
         \last(\machT) = (\mach_r',\_) ~ \land \\
        & \mach_r'' = \mathit{restore}(\contour,\mach_e,\mach_e',\mach_r,\mach_r') ~ \land \\
        & \mpstate{\mach_r}{\pol_r} \hookrightarrow \MPT' ~ \land ~
        \mach_r'' \hookrightarrow \machT' \Rightarrow \\
        & \pi_o(\MPT) \eqsim \pi_o(\machT) ~ \land ~
        \pi_o(\MPT') \lesssim \pi_o(\machT') \\
      \end{split}\]

      We combine this case with the other two cases from ordinary trace
      confidentiality:
\[
        \propdef{ObsTraceConf} ~  R ~ \MPT ~ \machT \ \ \triangleq \ \
         \mathit{ObsReturnCase} ~ R ~ \MPT ~ \machT ~ \land
         \mathit{InfCase} ~ \MPT ~ \machT \land \mathit{FailCase} ~ R ~
          \MPT ~ \machT
\]

%      \begin{figure}
%        \confidentialitylazyexample
%        \caption{Continuing from \cref{fig:confex}.}
%        \label{fig:conflex}
%      \end{figure}

      In our example, \cref{fig:conflex} shows how execution continues from
      \(\mach_r'\) and the restored
      \[\mach_r'' = \mathit{restore}(%\contour,
      \mach_e,\mach_e',\mach_r,\mach_r').
      \]
      Addresses 106 and 107
      are not restored; every address that was varied and not changed in one
      of the executions is. So 107 contains 43 in the trace from \(\mach_r\),
      and after returning from {\tt f}, {\tt main} prints 85. But in the trace
      from \(\mach_r''\), 107 contains $-4$, and ultimately 38 is printed.
      So observable trace confidentiality does not hold. This example
      illustrates the weakness of observable confidentiality as a testing
      property: it takes until the very end of the program to determine whether
      a tainted value actually influences observable behavior.

      On the other hand, suppose that we inserted an instruction at 43:
      \(r_6 \leftarrow [\rsp-8]\). \(r_6\) would hold 42 in \(\mach_r\)
      and $-5$ in both \(\mach_r'\) and \(\mach_r''\). But since \(r_6\) is
      never used otherwise, the traces of {\tt f} will enjoy observable trace
      confidentiality.

    \definition%{Observable Stack Confidentiality}

      {\em Observable stack confidentiality} is structured like stack
      confidentiality. For each initial
      state \((\mach_0, \pol_0)\) and \(n\)-transition from \(\mpstate{\mach_c}{\pol_c}\)
      to \(\mpstate{\mach_e}{\pol_e}\) in the induced trace
      \(\mpstate{\mach_0}{\pol_0} \hookrightarrow \MPT\), take any
      variation \(\mach_e' \approx_{\mathit{Cof}(\mach_c,n)} \mach_e\). Let
      \(\mpstate{\mach_e}{\pol_e} \hookrightarrow \MPT_{\mathit{call}} ~ | ~
      \ret{\mach_c}\) and \(\mach_e' \hookrightarrow \machT_{\mathit{call}} ~ | ~
      \ret{\mach_c}\). We must have \(\propdef{ObsTraceConf} ~
      \ret{\mach_c} ~ \MPT_{\mathit{call}} ~ \machT_{\mathit{call}}\).

      Additionally, for any variation \(\mach_0' \approx_{\mathit{Cof}(\mach_0),0}
      \mach_0\), taking the induced trace  \(\mach_0' \hookrightarrow
      \machT\), we must have \(\propdef{TraceConf} ~ \emptyset ~ \MPT ~ \machT\).

      A system enjoys observable stack confidentiality if observable
      confidentiality holds in all such cases.

    \definition%{Observable Stack Safety}
    A system enjoys {\em observable stack safety}
    % with respect to a call map       and a function map
    if it enjoys both observable stack integrity and
      observable stack confidentiality with respect to them.

      \medskip
      \textsc{Theorem.}
      Stack safety implies observable stack safety.
      \smallskip

      This is more straightforward than the implication in the previous
      section (1,5K LoC in Coq) as the recursive
      structure of the two properties is similar.

    \paragraph{Connection to Policies}

    Observable properties allow us to defer enforcement until a property
    violation would become visible, as in lazy policies. But by formalizing
    the connection to the eager policy, we can identify when lazy policies
    miss violations that can become visible. The optimized lazy tag policy from
    \citet{DBLP:conf/sp/RoesslerD18} tags each stack slot with the call depth
    at which it was written, and enforces that it must be read from the
    same depth. However, this allows violations of observable stack safety!
    Say we have functions {\tt foo}, {\tt bar} and {\tt baz}, where {\tt foo}
    calls {\tt bar}, and then after {\tt bar} returns {\tt foo} calls
    {\tt baz}. Since {\tt bar} and {\tt baz} are at the same stack depth, if
    {\tt bar} writes to {\tt foo}'s frame, {\tt baz} can then read the tainted
    write and use it to create an observable difference that violates both
    observable integrity and confidentiality without violating the policy.

    The lazy policy can be repaired if, instead of tagging each function
    activation with its depth in the stack, we generate a fresh activation
    identifier on each call, which prevents a program from exploiting stale
    tags from previous activations. A related mechanism was explored in the
    Static Authorities policy of \citet{DBLP:conf/sp/RoesslerD18}, which
    associates a unique activation identifier to each function, which is
    however shared by all activations of the same function in order to obtain
    good cacheability of tag rules. Static Authorities comes closer to
    observable stack safety, though the example above would still exist
    if {\tt bar} and {\tt baz} were merged into a single function that tracked
    whether it had been called previously and changed its behavior accordingly.
    \ifaftersubmission\apt{More blatently, static authorities doesn't work for recursive calls.}\fi

\section{Extension to Cheri}
\label{sec:cheri}

%\section{Extension to Software-based Enforcement}

\section{Related Work}
\label{sec:relwork}

%% CHERI \rb{cite}

\paragraph{Formal Stack Safety on Capability Machines}
%
To our knowledge, the only line of work to date that has attempted
a positive and formal characterization of
stack safety is \citet{Skorstengaard+19b}, who introduce a calling
convention that uses local capabilities to preserve local state
encapsulation and well-bracketed control flow, using a logical
relation to reason about the stack safety of concrete
programs. Although the required hardware support is readily available
in capability machines like CHERI\bcp{including linear capabilities?? or
  don't they use them in this paper?}, this technique incurs significant
costs\bcp{This is the pot calling the kettle black}, because it requires the entire unused part of the stack to be
cleared whenever a security boundary is crossed. Their logical
relation captures capability safety without ``externally observable
side-effects (like console output or memory access traces).'' \bcp{Not very clear:}In the
discussion, ``while [the authors] claim that [their] calling
convention enforces control-flow correctness, [they] do not prove a
general theorem that shows this, because it is not clear what such a
theorem should look like,'' noting that the correctness property
enjoyed by their technique ``is not made very explicit.''

StkTokens \citep{Skorstengaard+19} continues this line of work. Like
the earlier paper, it aims to protect the stack by enforcing local state
encapsulation and well-bracketed control flow. It does so by defining a new
calling convention that makes use of linear capabilities for stack and return
pointers. The convention operates on a single shared stack and requires that
protected components avoid compromising their own security by following certain
simple rules---like not leaking their private capability seals. More
precisely but still informally, local state encapsulation is defined as
restricting accesses to the range of memory allocated to the current stack
frame, and well-bracketed control flow as only allowing returns from the topmost
frame to the immediately adjacent frame below. Formally, it improves on
\citet{Skorstengaard+19b} by building those properties into the semantics of a
capability machine with a built-in call stack and call and return instructions,
which is proven fully abstract with respect to a more concrete capability
machine that replaces those pseudo-instructions with their calling convention.
Their proof of full abstraction uses a standard notion of components, which
import and export functions through their interfaces; their model of
observations is limited to cotermination.

We believe that the linear capability machines introduced by
\citep{Skorstengaard+19}, in combination with the StkTokens calling convention,
can be modeled in our framework and satisfy our definition of stack safety in
\cref{sec:lse-and-wbcf}. Further, we expect them to satisfy the even stronger
property developed in \cref{sec:enforcement}. Proving these conjectures would
involve materializing their definitions of local state encapsulation and
well-bracketed control flow, which exist only in enmeshed form as part of the
semantics of the capability machines. The main practical limitation of StkTokens
is its reliance on linear capabilities, as it is unclear now they could be added
to practical capability machines, especially in terms of efficiency---previous
work on micro-policies \citep{yannis-report} has shown how to use that framework
to implement linear return capabilities.\bcp{But IIUC the Dover realization
  of micro-policies can't do this...?}

\paragraph{Protecting the Stack with Micro-Policies}
%
\citet{DBLP:conf/sp/RoesslerD18} consider a standard attacker model where all
attacks against stack data are in scope (but not side channels or hardware
attacks), and study the protection of stack data through three families of
micro-policies that tag stack objects with a pair of frame and object identifiers
used to validate accesses to the stack: Return Address Protection (which
prevents an adversary from overwriting designated return addresses), Static
Authorities (which only allows the code of a function to access the stack frames
of its own dynamic instances), and Depth Isolation (described in
\cref{sec:enforcement}). All these policies exploit various kinds of spatial and
temporal locality of stack memory and local call graphs, as well as information
generated by the compilation toolchain, to strike various balances between
precision and cacheability, evaluated through benchmarks that demonstrate
limited performance overhead. In addition to the baseline ``eager'' policies,
they propose a number of lazy optimizations, notably Lazy Tagging and Lazy
Clearing, discussed in \cref{sec:lazy}, with associated improvements in
performance.

\paragraph{Heap Safety as a Security Property}

Heap safety, like stack safety, may be framed as a security property
in the form of noninterference~\citep{DBLP:conf/post/AmorimHP18}. Just
like~\citeauthor{DBLP:conf/post/AmorimHP18} give a rigorous
characterization of the meaning of (heap) memory safety, in this paper
we aim to do the same for the stack.\bcp{Let's describe their work in a bit
  more detail.}
%
Their model describing safety of shared stack-allocated
objects extends naturally to heap safety, complicated by the fact that
these objects are not deallocated by returns, and therefore a
function's privilege may increase or decrease after its entry
point. For instance, if a callee allocates a heap object and returns
the pointer to its caller, that object's addresses become accessible
in contradiction to a contour computed at the caller's entry. So such
an extension must enforce trace confidentiality and integrity
properties separately on continuous segments of each call, with
contours computed at each crossing between caller and callee.

\section{Future Work}
  \label{sec:future}

  We have presented a pure notion of stack safety that omits many complicating
  factors common to real systems. In particular, our model as presented exploits
  the simplifying assumption that privilege never increases during a function
  call, and that therefore a contour computed once on the entry to a function
  describes its privilege throughout. Our future plans are to extend the model
  with common language features, some of which violate this assumption. Here we
  describe the basic principles by which our model can be extended to some of
  these cases. We also plan to develop a testing framework for quickly
  checking the validity of different enforcement mechanisms.

  \paragraph*{Capability machines}
  %
  %Other policies are implemented in hardware or built on hardware mechanisms.
  Capability machines such as CHERI \citep{Woodruff+14} extend conventional
  architectures to support efficient and fine-grained control over memory
  accesses, which can be used to implement security policies, including
  the stack protection policies of
  \citet{Skorstengaard+19b} and (with an extension to support linear capabilities) \citet{Skorstengaard+19}.
  %\bcp{I thought these policies used
  %  linear capabilities?}\leo{Only the second one AFAIK. What would you
  %  change to characterize it more accurately?}\rb{StkPointers uses linear
  %  capabilities, its predecessor users local capabilities (and unlike
  %  StkPointers could be implemented in CHERI)}.
  Capabilities package up pointers
  (ordinary data) with base and bounds information (policy data),
  and they require rewriting source code (e.g., to use capabilities in place
  of ordinary pointers), so separating ordinary machine state from policy
  state is delicate; we conjecture it can be done with some effort.

  %% Mapping the underlying protection
  %% mechanisms to policy states and step function would be a delicate operation, as
  %% those are tightly coupled to the hardware design. However, separating the
  %% enforcement aspects of a concrete policy (such as StkTokens, where policy state
  %% would encompass details like the abstract call stack, the capability sealing
  %% scheme, etc. \rb{is this digging too deep? or on the contrary, say more?}) from
  %% the machine would be relatively straightforward.  \bcp{Not sure that any of
  %%   this is comprehensible... :-(}

  %\paragraph*{Software-only policies}
  %
  %Some policies are implemented in software, by modifying code in the
  %compiler, for example to perform bounds checking~\citep{NagarakatteZMZ09} or
  %to insert stack canaries~\citep{Cowan+98}. Such approaches can, in principle,
  %be evaluated in terms of our formal notion of stack safety with a trivial null
  %policy. But any state maintained by the enforcement mechanism must be in normal
  %memory and subject to the same confidentiality and integrity concerns (to be
  %varied, rolled back, etc.), a heavy constraint. Alternately it can be stripped
  %out as policy state provided that it can be proven to protect itself.

%  \paragraph{Stack-derived Pointers}

%    Common programming idioms involve a caller passing a pointer to
%    its local data into a callee. Using standard notions of pointer
%    provenance~\citep{provenance}, we can extend our model to
%    distinguish safe use of this idiom from true stack safety
%    violations. We require additional annotation to identify which
%    addresses within a function's stack frame correspond to distinct
%    objects. Then a valid pointer to an object is one derived through
%    legal arithmetic from its base address, and at each call point, an
%    object's addresses are marked low confidentiality and integrity if
%    a valid pointer exists in a register or in the transitive closure
%    of the callee's accessible memory. All pointers to an object cease
%    to be valid when it is deallocated along with its stack frame.

%  \paragraph{Non-stack control flow}

%    Control structures beyond well-bracketed calls and returns require
%    modifications to the model. Tail calls, for instance, reuse the caller's
%    stack frame for its callee, and every nested tail call returns
%    simultaneously to the top non-tail-calling function. Under normal stack
%    integrity, the first function in a chain of tail calls returns and violates
%    integrity, while subsequent tail calls never actually return at all.
%    Instead we need a variant that treats the entry of a tail call as the
%    return of its caller.

%    More complex is the addition of a coroutine model, in which multiple stacks
%    respect stack safety internally, but might also yield to one another.
%    Here we must distinguish three levels of integrity: accessible, in the same
%    stack, and in another stack. Stack integrity requires that same-stack data
%    be unchanged when a function returns, but other-stack data may change. A
%    similar {\em yield integrity} property requires that, from a yield out of
%    a coroutine to the next yield back into it, all accessible and same-stack
%    data are unchanged. Confidentiality properties are split similarly.

%    \paragraph{Random Testing}

%    Part of the motivation for the strong lockstep stack-safety property
%    is that it uncovers errors quickly, and therefore should be quicker
%    to test, just like the smarter testing properties
%    of~\citet{TestingNI:ICFP}. We have already developed a testing
%    framework for debugging our policy enforcement for this property, and
%    it would be interesting to explore whether a testing-amenable variant
%    of the lazy property also exists.

% \rb{Notes from meeting:
%   \begin{itemize}
%   \item Explain how the theory applies to production compilers like GCC and
%     LLVM, which aspects are directly applicable, what are the effects of
%     alternative calling conventions. Other less charted waters: tail calls,
%     coroutines, jumps into supervisor mode.
%   \item CHERI and its operation, documentation and practical configurations and
%     operation.
%   \item Compare and contrast with StkTokens.
%   \item Under what circumstances can stack safety exist without an enforcement
%     mechanism?
%   \item Zeroing out of allocated space inside blessed sequences, eager vs lazy.
%     Possibility of blessed alloc and dealloc sequences, which also manipulate SP
%     and would not be legal without blessing. In such an extended setting, the
%     return sequence would check for matching addresses (it would be nice to have
%     testing for this!).
%   \end{itemize}
% }

%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}


%% Bibliography
\bibliography{bcp.bib,local.bib}


%% Appendix
%\appendix
%\section{Appendix}
%Text of appendix \ldots

\end{document}
