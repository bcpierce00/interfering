\documentclass{article}

\usepackage{geometry}

\begin{document}

\section{A Stack-safe Calling Convention for Capabilities}

This is Skorstengaard's calling convention from the worse of their papers, with a
difference in that we use a single piece of code to handle the ``landing pad'' of a return.
We must combine this with descriptions of how to adjust out model to support them. At the
high level, we need to accept that their threat model is slightly incompatible with ours:
adversarial code, defined as code whose dynamic behavior does not obey simple reasonableness
properties, is exempt from the LSE and WBCF protections in its own calls. For instance, such
a function might make a call in such a way that the return skips over it and returns straight
to its caller, violating WBCF. Or it might create capabilities on its own data and pass them
along, violating LSE. This latter piece of unreasonableness is easy to handle: we add a clause
to integrity to assume that the caller is reasonable. Equivalent adjustments to confidentiality
might be trickier, in that we should acknowledge the possibility of unreasonableness in either
the callee or the caller. But the violations of WBCF are much more problematic, because our
model of counting depths assumes that callers are never ``skipped'' by a multi-level return.

\subsection{Updating Stack Safety to Support Unreasonable Functions}

The solution given in StkTokens is, essentially, that unreasonable functions don't get to
make ``real'' (protected) calls. Rather, they perform all the operations involved in a call,
then jump into another function at its entry point. That function may then jump back -- which
is not a ``real'' return -- or do an actual return to the original caller. The equivalent to
this is in our model is for us to, on a call-annotated instruction, check that the current
function is reasonable. If it is not, we do not update the depth, and given the callee an
unsealed piece of stack to work with. Similarly, on return, we only update the context
if we are returning to a reasonable function. That requires examining the program counter
that we're returning to. The problem with this whole approach is that it doesn't play nice
with the initialization side of confidentiality. The adversary gets to initialize its
callee's frame before fake-calling it. A reasonable callee will not read those initialized
variables, anyway, but the property will not capture that the protection is needed. We
could potentially treat this as a special kind of call, obeying confidentiality but not
integrity or WBCF, which gets us closer to the next approach.

The more complex approach is to accept that unreasonable callers might get skipped or
accessed, but otherwise model their calls and returns normally. That means extending
our model with support for dynamic returns, which are then forbidden by WBCF in the case
of reasonable callers. Specifically, the {\sc Ret} annotation is now parameterized by
a function that takes a machine state and a context and returns an integer depth: the
return target. Instead of only popping the top depth, we pop the depth
we're returning to and every one above it, by the same mechanisms as in a single return.
In the context of PIPE, where we always guarantee single returns, the function can simply
subtract 1 from the current depth, as before. We will discuss how to implement this
function for Cheri once we cover the calling convention.

Now most of our properties are unaffected by this, even Return Integrity, which after
all talks about what happens {\it when} we return to a caller. Then we simply need a second
property, which I propose to call Return Guarantee, that if a caller is reasonable it
will see a return {\it before} any caller with a lower depth does.

\subsection{Calling Convention Details}

First let's discuss our assumptions about the capability system, starting with basic
capabilities.

A {\em capability} is composed of a {\em base}, {\em offset}, and {\em bound}
Each word of memory carries a 2-bit tag, the first bit indicated whether it is a
{\em valid} capability and the second whether that capability is {\em local}
A local capability can never be stored in memory outside of the stack. A load or store
failstops if the capability used as its pointer does not have its offset within its
bounds. A load or store using a non-capability pointer uses the bounds of one of the
following special capabilities.

There are three special capability registers. The {\em default data capability} ({\tt \$ddc}),
is applied to reads and writes through non-capability pointers. The
{\em program counter capability} ({\tt \$pcc}) is applied to instruction fetches.
The {\em stack capability} ({\tt \$stc}) takes the place of the stack pointer and is used by
compiler generated code. Basic capabilities are supported by the following instructions:

\begin{tabular}{l l l}
  {\tt st}      & Data Store       & Stores data, clearing any capability bits \\
  {\tt ld}      & Data Load        & Loads data, clearing any capability bits \\
  {\tt stc}     & Capability Store & Stores a capability, but failstops rather than store
                                     a local one outside the stack \\
  {\tt ldc}     & Capability Load  & Loads a capability from memory \\
%  {\tt incbase} & Increment Base   & Increases the base of a capability by one and
%                                     decrements the offset \\
%  {\tt decbnd}  & Decrement Bound  & Decreases the bound of a capability by one \\
\end{tabular}

We will need to use {\em object capabilities}. Object capabilities come in twos:
a code pointer and a memory capability. They are sealed with a matching identifier and
can only be unsealed by invoking them together, simultaneously jumping to the code pointer
and unsealing the memory capability. This way we ensure that the memory capability is
only used by trusted code. These are supported by the following instructions:

\begin{tabular}{l l l}
  {\tt seal}   & Seal Object    & Takes a pair of capabilities and a seal identifier and
                                  seals them with the identifier \\
  {\tt invoke} & Invoke Object  & Takes matching sealed capabilities \(c\) and
                                  \(p\), jumps to \(p\), and unseals \(c\) \\
\end{tabular}

Finally, we will use {\em uninitialized capabilities}. These have a special permission structure
where their offset only moves when written to. The portion of the capability below the offset,
which is initialized, can be promoted into a normal capability. They support the following
instructions:

\begin{tabular}{l l l}
  {\tt stu}  & Data Store (Uninitialized) & Stores data and increments offset of Ucap \\
  {\tt stuc} & Capability Store           & As {\tt stU} but stores a capability \\
  {\tt prom} & Promote                    & Promote initialized part of an uninitialized capability
                                            into a normal capability \\
  {\tt morp} & Co-promote                 & Remove the initialized part of an uninitialized capability \\
\end{tabular}

\section{A Capability Machine}

Here I'll describe a simple machine implementing the operations above. Eventually. For now
I mocked it up in CheriModel.v. It's pretty close to what's described above. The most interesting
thing is the semantics of invoking a sealed object. A sealed object is a pair of a code capability
and a data capability (though these are distinguished only by convention.) There is a special register,
{\tt \$invdst}, in which the unsealed data capability is stored (and of course, the unsealed code capability
is copied to the {\tt \$pcc}. The idea is that on entry to an invoked capability, we always need to know
which register has the data capability, so it can't be dynamic.

\subsection{A call}

Here we will give an example of a call to a function at address {\tt 20} that takes an
argument in register {\tt ra}. Let our call start at address {\tt 0}. Suppose that we have a piece
of code at {\tt 50} that executes a ``landing pad'' for a return. Assume that all capabilities referenced
here are local, and the stack capability is an uninitialized capability that can write locally. There
could be nonlocal capabilities as well, for use in the heap, but to match our model we can also just assume
that such capabilities are derived from a permissive {\tt $ddc}.

\begin{verbatim}
<0: [clear all registers]
0: mov 42 ra
1: mov $stc r1
2: morp $stc
3: stuc r1 $stc               [PASS $stc]
4: stuc [$pcc + 7] $stc       [PASS $stc]
5: mov $stc r1
6: prom r1
7: morp $stc
8: mov [$pcc + 42] r2
9: seal s r1 r2
10: jmp +10                   [CALL]
11: ...
\end{verbatim}

We can walk through what this does step by step. First, to make sure that we don't have any extra
capabilities laying around, we clear all of our registers. Then we set the argument in {\tt ra}.
Now things get interesting: instructions 1-2 save the current stack capability and cut off
the readable portion, leaving only the uninitialized portion. Instructions 3-4 construct an activation
record containing the previous stack capability and the appropriate return address. Note that each {\tt stuc}
instruction increments the stack pointer, so we are implicitly allocating these two variables.
They will need to be accessible to the callee, so we annotate them as passed. Next, we create a capability
that can only access them, in instructions 5-6: the promoted stack capability is a capability on
only the written portion. Finally instruction 7 ensures that the stack capability cannot access
them. Instructions 8 and 9 now seal together the data capability and a code capability on address
50, where our ``landing pad'' code lives, and address 10 jumps us ahead to our function entry.

Now, what happens when we return? Let's look at the landing pad. Recall that we have a special
register {\tt \$invdst} that is where the unsealed data capability lives.

\begin{verbatim}
50: ldc $invdst $stk
51: jalr [$invdst+1]          [RETURN]
\end{verbatim}

Pretty simple! Notice something interesting: if we invoke a (properly constructed) return capability,
we {\it always} hit a {\tt jalr} labeled as a return two steps later. In other words, an adversary can't do
something that looks like a return without it being labeled a return.

Now it's pretty easy here to argue some of our properties. We should have that no unsealed capabilities are
present unless they were passed as arguments, except the stack capability, which is completely uninitialized.
So we should not be able to read anything without writing it or access a caller's frame at all.

When we've returned, we've restored the old stack capability, so the callee's frame is again uninitialized.
But! We may have a capability on it that the callee returned. That's a potential problem, because the capability
could then be passed to a future call in a way that allows it to persist. There's a solution to that, unpleasant
though it is: clear every register on return except for the value being returned, and manually check that it
isn't a capability that overlaps the uninitialize portion of the stack. Or indeed, any stack capability at all.

\subsection{Friction points}

In this model we don't really have control separation. And if we did, the landing pad wouldn't
actually belong to any one function, so it'd be a bit odd. We could get separation back by making
the call an invocation of an object, with its own launchpad that handles all of the rest of the
construction.

We still have the problem with returns, in that a callee could potentially pass along its
return object as an argument to a further callee, which could then bypass the first and return
straight to the original caller. We can model what happens in this case by checking, at the return point,
where we are in the stack - that is the depth we are about to return to, and it may be more than one depth
down (or zero, as the callee could always manufacture its own return object to ``return'' to itself).

\section{The Question of Policy States}

This half of the document deals with questions of the division between machine state and
policy state. In existing models of our security properties, the state is divided into a
machine state, which is a mapping from addresses and registers to data, and a policy state,
which may be any arbitrary data. A machine state can step on its own, or in conjunction
with a policy state; when states step together the machine state component always steps
as it would on its own. A policy state may only cause the step to fail outright, never
change the result.

We are faced with the task of deciding 1) which parts of an architecture should be treated
as machine, and which parts as policy; 2) what is to be done with the policy state in the
context of confidentiality properties; and 3) whether there are parts of the machine state
that ought to be treated differently in such properties.

We will discuss these ideas in terms of three enforcement mechanisms:

\begin{itemize}
\item HOPE
\item Cheri
\item A software-based mechanism such as SFI
\end{itemize}

\section{What if we eliminate Policy States?}

Let's start by considering what a system would look like that doesn't use policy state. The
simplest such model simply makes the machine transition function partial, with failure to step
indicating a fail-stop. Here the enforcement is built directly into the machine. This applies to
our three enforcement mechanisms as follows.

\paragraph*{HOPE} The machine contains both ordinary registers and memory, and tag registers
and memory. The transition function is parameterized by a tag rule table, and we must be
careful to construct the model such that tags only be read and updated as described by the
table. Fail-stops should occur only on tag rule failures.

\paragraph*{Cheri} The machine contains ordinary registers and memory, with some effort needed
to model the extra space that capabilities take up. (Either words need to be larger to fit
capabilities, or we need to model them being spread across multiple words.) It also contains
the map of 1-bit tags marking capabilities as valid. The ISA is extended with the Cheri
operations and the transition function handles the valid-capability tags appropriately.
Here a fail-stop occurs whenever the Cheri architecture would trap to a failstate, mostly
when reading or writing through an invalid capability or outside of a valid capability's
authority.

\paragraph*{Software} Here the machine gets to be completely unchanged, and never failstops.
Instead we restrict the set of initial states to those whose code comes from the compiler
with the appropriate modifications, and it will simply never violate the policy - any errors
will take it to some error state where it silently diverges, which is perfectly allowed.

\paragraph*{Implications for confidentiality} If everything lives in the machine state, then
we need to think about what it means to vary it to create a confidentiality property. Take
the HOPE example: does a variant include states with different tags, such that either the
primary trace failstops and the variant does not, or the variant does and the primary does
not? Similarly, in Cheri, do we vary the valid-capability bit? What about bounds information?

We can pretty firmly say that we do not need to vary tags, including the capability bit. That's
because these can never be read, and therefore it is meaningless to talk about their
confidentiality. But the bounds information in Cheri presents a bigger problem. We will
revisit it as we start moving policy data out of the machine.

\section{Separate Policy States}

We have identified some kinds of data that can never be read and serve exclusively to inform
the enforcement mechanism. We can think of data as having two meanings, its ``application
content'' and its ``security content.'' In HOPE, the distinction seems clear: tags have only
security content, never application. So we can justify separating out security-only data
into a separate part of the state, from which it cannot escape and therefore does not need
to be varied to support confidentiality. Let's discuss how that works in HOPE and Cheri
(in software methods, of course, everything has application content.)

\paragraph*{HOPE} In HOPE, we put all tags in the policy state, along with any policy-specific
machinery such as counters that give us the fresh tags. The machine step function can now be
total, with failure only coming from the policy step function. But it turns out that some
machine data still has security content. Consider a load instruction: its tag rule must
take into account the tag at the target address, which means that the policy step function
must be aware of the machine state.

This gives us the same question of what we do in confidentiality with variant states that
fail-stop when the original didn't.

\paragraph*{Cheri} In Cheri, the valid-capability bit has no application content, so these can
live in the policy state. But the application content of a capability is its bitwise value.
We might argue that actually the base and offset are application content, while permissions
and bounds information are security-only, but it is not feasible to separate the latter
from the former. Even if we did, the base and bounds would give us the same issues as HOPE
pointers.

\paragraph*{Implications} The primary implication is that we no longer need to worry about
what happens if we vary tags, whether that's the full HOPE tag or the 1-bit Cheri tag.
But that doesn't save us from sometimes varying security content, because pointer values
still have security content. In both systems, if we read a valid pointer that is supposed to
be secret and then dereference it, there is a variant run in which the pointer is invalid and
the variant fail-stops. This will always be the case no matter how we separate machine and
policy.

The advantages that we do get by separating them as much as possible are: it's easier to model
the separate state than if we built it into the machine architecture, which matters more for
HOPE but is a small concern for Cheri. And depending on how we handle these variant fail-stops,
it may be more efficient to test if fewer of them occur.

\section{Handling Variations in Security Content}

Lets consider a very simple machine with three addresses and an external feed of instructions
(instructions are not in memory.) An address can be marked either secret or public. Let's start with
the following configuration (top rows are addresses marked S or P, bottom are contents):

\begin{tabular}{| c | c | c |}
  \hline
  0(S) & 1(P) & 2(P) \\
  \hline
  0 & 0 & 0 \\
  \hline
\end{tabular}

We have a register \(r\), initialized to 0, and an output {\sc Out} that can be loaded to like a register.
We want to apply our confidentiality model by varying secret locations and determining if programs
differ in output between the original state and a variant. I'm using the simplified format from our
examples in the paper. Assume that loads use the address modulo three so that the step operation is
total. Consider the program:

\vspace{\abovedisplayskip}

\begin{tabular}{l l l}
  Line & Instr & Description \\
  \hline
  1 & \(r \leftarrow [0]\) & Load from address 0 \\
  2 & {\sc Out} \(\leftarrow [r]\) & Load from the resulting address and output it \\
  3 & \(r \leftarrow 0\) & Reset \(r\) to 0 \\
\end{tabular}

\vspace{\belowdisplayskip}

At the end of this program, 0 has been printed. In any variant when address 0 mods to a 1 or a 2, it
will also print 0. And since the final instruction clears \(r\), the final state is identical as well.
So we would not say that this program violates confidentiality of address 0, even though it was read.

Now suppose that we also make address 1 secret. The variation might vary both 0 and 1, so that the
program reads address 1 and outputs a non-zero result. Now, confidentiality is violated, because this
variation produces a different output. Suppose we consider some possible security properties to
prevent this from occurring. The property we come up with attaches to the load instruction and,
if the source register contains an address that is marked secret, fail-stops the machine. But we forget
to implement a similar rule for loading through immediate values, oops! So the load on line 1 never failstops.
Let's look at that second start state and the same program:

\begin{tabular}{| c | c | c |}
  \hline
  0(S) & 1(S) & 2(P) \\
  \hline
  0 & 0 & 0 \\
  \hline
\end{tabular}

Now the primary execution will load from address 0, then attempt to do so again and fail-stop. The variant
might instead try to read from a different location, but we are termination insensitive, so even if it reads
from 2 (successfully) the property holds. But what about this start state?

\begin{tabular}{| c | c | c |}
  \hline
  0(S) & 1(S) & 2(P) \\
  \hline
  2 & 0 & 0 \\
  \hline
\end{tabular}

Here the primary execution loads from 2 and prints 0. But a variant in which address 0 holds 0 or 1 will
load from those addresses, which would violate the policy and cause a failstop. So the question is, how do
we handle those variants? We can:

\begin{enumerate}
\item Recognize that the variant should fail-stop, and since the primary execution does not,
  treat that as a violation.
\item Recognize that the variant should fail-stop, and since we are termination insensitive,
  treat that as acceptable and look for another variant that actually produces different output.
\item Do not check whether the variant fail-stops. So when we vary address 0 to 1 and address 1 to 1,
  the program prints 1. Thus the property is violated.
\end{enumerate}

Options 1 and 2, in testing, will let us stop short and either move on to a different test or
declare a failure. Option 1 seems at odds with the notion of termination-insensitivity; so does option 3
in its own way, with the added downside that it's hard to predict whether a given case might fail. Option
2 is similar to only doing variants that maintain the legality of pointers. I've been leaning toward option
2, and I tend not to like option 3 because our variant run can show behavior that is infeasible in the
target machine.

This small example illustrates a scenario that can come up in HOPE and Cheri, when pointers and capabilities
being varied introduces fail-stop behavior in the variant trace when none was present in the primary. This
is an edge case, of course, because in any such scenario the program could instead print the pointer and thus
violate the property immediately. But we do need to decide how to approach it. We could imagine a policy that
prevents printing the pointer directly (via taint-tracking, say) but does not prevent dereferencing it.

\section{Cheri Implementation}

In order to implement Cheri we'll need to make a couple of changes. First of all, we need to extend the model
with capability instructions and preferably add some capability-sized registers. It will be tedious aligning
capabilities properly in memory; could we instead take a shortcut by just making words capability-sized, and
only reading half of them for non-capabilities? We need at the very least cap-load, cap-store, arithmetic,
and the instructions to derive capabilities from the ambient capabilities (esp. stack capability). Currently
looking into just how much we can strip out and still have it work.

The policy state will be the mapping from addresses/registers to their capability bits. Capability instructions
will maintain the mapping as appropriate. Certain operations will fail-stop based on violating that mapping.

\end{document}
