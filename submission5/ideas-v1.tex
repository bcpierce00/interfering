The paper is structured as follows. In \cref{sec:example}, we will walk through a function
call in a simple example machine and discuss how each of our properties applies to it,
informally. Then in \cref{sec:machine} we will introduce the machine model that we test, and give
its {\em security semantics}, described in brief below. We formalize the definitions of
stack safety that derive from the security semantics in \cref{sec:props}.
\Cref{sec:testing} describes the testing framework, and
\cref{sec:relwork,sec:future} discuss related and future work.
The remainder of this section introduces ideas that will be important throughout.

\paragraph*{Security Semantics and Property Structure}

A {\it security semantics} extends a machine
with additional context about the identity of current and pending
functions (which act as security principals) and about the registers and memory they require
to be secure. This added context is purely notional;
it does not affect the real machine. The security context
evolves dynamically through the execution of {\it security-relevant operations},
which include events like calls, returns, and frame manipulations.
Our security properties are phrased in terms of this context, often as predicates
on future states, e.g. of the form ``when control returns to the current function...'',
or relations on traces of future execution (hyper-properties).

The security-relevant operations typically correspond to underling machine instructions,
but the correspondence may not be obvious just by inspecting the machine code.
For example, in the tagged RISC-V machine we use in our examples and tests,
calls and returns are conventionally performed by {\tt jal} (``jump-and-link'')
and {\tt jalr} (``jump-and-link-register'') instructions, respectively, but these
instructions might also be used for other things. An instantiation of an operation
might require some external mechanism for distinguishing which instructions are
security-relevant; the most obvious technique is to add optional labels to instructions,
but the theory does not depend on this choice.

The set of security-relevant operations (\(\Psi\)) covered in this paper is given in
\cref{tab:psi}. Those that we explore in detail in \cref{sec:example}
are shown in white: calls, returns, and the allocation and deallocation of local memory.
In \cref{sec:machine}), we cover additional operations that add parameters to the examples',
to support various forms of memory sharing \ifexceptions , exceptions, \fi
and tail-call elimination. These are shown in light gray.
We give a proof-of-concept formalization of a more sophisticated, heap-like sharing model
based on pointer provenance in \cref{app:ptr}, represented by the dark grey
operations, but we do not test this.

\newcommand{\example}{\rowcolor{black!0}}
\newcommand{\testing}{\rowcolor{black!10}}
\newcommand{\theory}{\rowcolor{black!25}}

\begin{table}
  \begin{tabular}{| l | l |}
    \hline
    \(\psi \in \Psi\) & Parameters \\
    \hline
    \example \(\mathbf{call}\) & Target address, argument registers \\
    \testing & Stack argument offset \& size \\
    \example \(\mathbf{return}\) & \\
    \example \(\mathbf{alloc}\) & Offset \& size \\
    \testing & Public flag \\
    \example \(\mathbf{dealloc}\) & Offset \& size \\
    \testing \(\mathbf{tailcall}\) & As \(\mathbf{call}\) \\
    \hline
    \multicolumn{2}{|c|}{{\it Pointer provenance operations}} \\
    \hline
    \theory \(\mathbf{promote}\) & Register, offset \& size \\
    \theory \(\mathbf{propagate}\) & Source register/address \\
    \theory & Destination register/address \\
    \theory \(\mathbf{clear}\) & Target register/address \\
    \hline
  \end{tabular}
  \caption{Security-relevant Operations and their Parameters}
  \label{tab:psi}
\end{table}

Our security semantics has similarities to the overlay semantics that have been proposed
to characterize stack safety in the past~\cite{SkorstengaardSTK}, but it does not restrict
the behavior of the underlying machine in any way. Rather, it tracks additional context
about the history of security-relevant operations, which inform the criterion a machine
must satisfy in order to correctly obey the properties.

The security principals in our account of stack safety are function
activations: at any given time, pending activations
have security requirements that are recorded in a call stack, and the current
activation tracks context information that will inform its requirements when
it makes a call. Each activation has a {\it view}
of the system that maps each state element to a {\it security class}:
it may be private to an inactive principal (\(\sealed\)),
in an allocated object that is accessible to the current principal (\(\object\)),
available to be allocated (\(\unsealed\)), or
always accessible (\(\public\)).

\paragraph*{Events}

It is common to specify the behavior of a system, including security properties, in
terms of traces of observable {\em events} rather than details of the machine state.
Lazy enforcement works because the deferred check catches errors before they appear
in the trace. The nature of events is a parameter of the machine, and can be specialized
to any notion of observable behavior.

\paragraph*{Variants and Non-interference}

We model confidentiality by drawing on 

\paragraph*{Threat Model}

We must trust that our method of distinguishing security-relevant operations is accurate; if it
involves labels placed on code by a compiler, that means trusting that the compiler placed
those labels correctly. If operations occur that are not recognized, those operations
might not be guaranteed to protect their principals---for instance, an unlabeled call
might not protect the caller's data from the callee. On the flip side, applying an incorrect
label most likely means that the property becomes too strong to be enforced.

Otherwise, we do not assume that the code adheres to any particular
calling convention or implements a source language construct.
In particular, while we are agnostic as to the source
language, we certainly aim to support C, and so any source function might contain undefined
behavior resulting in its compilation to arbitrary machine code. A given enforcement
mechanism may place additional constraints, particularly on the behavior of
call and return sequences. For instance, extant implementations tend to assume
implicitly that callee-saved registers are treated appropriately by whichever compiler
generated their code. Our properties explicitly require them to be treated as belonging
to the caller, which could be enforced by a micro-policy or by a well-behaved compiler.

In general, it is impossible to distinguish buggy machine code from an attacker; in
our examples we will identify one function or another as an attacker, but we do not
require any static division between trusted and untrusted code, and we aim to protect
even buggy code.

This is a strong threat model, but it does omit some important aspects of
security enforcement: in particular, hardware and timing attacks are out of scope.

\paragraph*{Tagged Architectures and Laziness}

In \cref{sec:testing}, we apply our properties to test the existing stack-safety
mechanism of Roessler and DeHon \cite{}, who present a collection of tag-based enforcement
mechanisms using PIPE \cite{???}, a tagged hardware reference monitor.
In monitors such as PIPE and STAR \cite{???}, \apt{double-check this} every value in memory
and registers is paired with a {\em metadata tag}.
A set of software-defined rules, termed a micro-policy, determines the tags for operation
results based on the tags of their inputs---or, if the input tags are in a dangerous
combination, it may halt the machine (a ``failstop.'')

We are particularly interested in their {\em Lazy Tagging and Clearing} micro-policy.
In their conventional micro-policies, they tag each word
in a stack frame with the (depth of) its owning function activation.
Roessler and DeHon find that it is inefficient, because tag setting
and clearing must be performed at every allocation, even on elements that are never accessed,
or that are initialized again and accessed long afterward. This is quite common, e.g. in code
that allocates large local arrays.

Instead they offer Lazy Tagging and Clearing, in which tags are not initialized at allocation, but only
when a function writes to memory. This means that writes cannot be prevented, which in turn means
that the existing line of stack safety specifications from Georges et al. will deem it unsafe.
Instead the micro-policy enforces that memory be read by the same function activation that wrote it.
The owner of the data will never see the changes,
so it is secure in the sense that the attacker is unsuccessful at changing program behavior
(except, perhaps, by causing a fault). We elaborate this notion of observational safety, and test whether the
micro-policy fulfills it.

\paragraph*{Randomized Property-based Testing}

[TODO: Brief discussion of PBT and why we use it, explain the lack of proofs.]
