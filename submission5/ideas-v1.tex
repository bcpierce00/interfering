The paper is structured as follows. In \cref{sec:example}, we will walk through a function
call in a simple example machine and discuss how each of our properties applies to it,
informally. In the process we discuss some motivation for the properties, from a security
perspective: we consider a simple attacker, and discuss the harm that it can do to
its caller. This perspective is essential to understanding how lazy enforcement can
be safe: even if an attacker can overwrite some memory, if it cannot accomplish
its bigger-picture goals, the system is safe.

Then in \cref{sec:machine} we will introduce the machine model that we test, and give
its {\em security semantics}, described in brief below. We formalize the definitions of
stack safety that derive from the security semantics in \cref{sec:props}.
\Cref{sec:testing} describes the testing framework, and
\cref{sec:relwork,sec:future} discuss related and future work.
The remainder of this section introduces ideas that will be important throughout.

\paragraph*{Security Semantics and Property Structure}

A {\it security semantics} extends a machine
with additional context about the identity of current and pending
functions (which act as security principals) and about the registers and memory they require
to be secure. This added context is purely notional;
it does not affect the real machine. The security context
evolves dynamically through the execution of {\it security-relevant operations},
which include events like calls, returns, and frame manipulations.
Our security properties are phrased in terms of this context, often as predicates
on future states, e.g. of the form ``when control returns to the current function...'',
or relations on traces of future execution (hyper-properties).

The security-relevant operations typically correspond to underling machine instructions,
but the correspondence may not be obvious just by inspecting the machine code.
For example, in the tagged RISC-V machine we use in our examples and tests,
calls and returns are conventionally performed by {\tt jal} (``jump-and-link'')
and {\tt jalr} (``jump-and-link-register'') instructions, respectively, but these
instructions might also be used for other things. An instantiation of an operation
might require some external mechanism for distinguishing which instructions are
security-relevant; the most obvious technique is to add optional labels to instructions,
but the theory does not depend on this choice.

Whatever the mechanism for labeling transitions, we get a base
machine transition \(\mach \xrightarrow{\bar{\psi}, \obs} \mach'\), where \(\obs\) is
an event (see below) and \(\bar{\psi}\) is a list of security-relevant operations.
\apt{Example doesn't include the event annotation. Should remove here, or add a note there.}
We then lift this into a transition between pairs of machine states and contexts,
\((\mach,\context) \stepstounder{\overline{\psi},\obs} (\mach', \context')\).\apt{We don't need this notation for the example.}

The set of security-relevant operations (\(\Psi\)) covered in this paper is given in
\cref{tab:psi}. Those that we explore in detail in \cref{sec:example}
are shown in white: calls, returns, and the allocation and deallocation of local memory.
In \cref{sec:machine}), we cover additional operations that add parameters to the examples',
to support various forms of memory sharing \ifexceptions , exceptions, \fi
and tail-call elimination. These are shown in light gray.
We give a proof-of-concept formalization of a more sophisticated, heap-like sharing model
based on pointer provenance in \cref{app:ptr}, represented by the dark grey
operations, but we do not test this.

\newcommand{\example}{\rowcolor{black!0}}
\newcommand{\testing}{\rowcolor{black!10}}
\newcommand{\theory}{\rowcolor{black!25}}

\begin{table}
  \begin{tabular}{| l | l |}
    \hline
    \(\psi \in \Psi\) & Parameters \\
    \hline
    \example \(\mathbf{call}\) & Target address, argument registers \\
    \testing & Stack argument offset \& size \\
    \example \(\mathbf{return}\) & \\
    \example \(\mathbf{alloc}\) & Offset \& size \\
    \testing & Public flag \\
    \example \(\mathbf{dealloc}\) & Offset \& size \\
    \testing \(\mathbf{tailcall}\) & As \(\mathbf{call}\) \\
    \hline
    \multicolumn{2}{|c|}{{\it Pointer provenance operations}} \\
    \hline
    \theory \(\mathbf{promote}\) & Register, offset \& size \\
    \theory \(\mathbf{propagate}\) & Source register/address \\
    \theory & Destination register/address \\
    \theory \(\mathbf{clear}\) & Target register/address \\
    \hline
  \end{tabular}
  \caption{Security-relevant Operations and their Parameters}
  \label{tab:psi}
\end{table}

Our security semantics has similarities to the overlay semantics that have been proposed
to characterize stack safety in the past~\cite{SkorstengaardSTK}, but it does not restrict
the behavior of the underlying machine in any way. Rather, it tracks additional context
about the history of security-relevant operations, which inform the criterion a machine
must satisfy in order to correctly obey the properties.

The security principals in our account of stack safety are function
activations: the context tracks, for the active function and all pending
functions, their {\em view} of the security state of the system.

\paragraph*{Views}

The security context consists of a stack of views: functions that map
each state element to a {\it security class}. The security classes are
\(\public\), \(\unsealed\), \(\object\), and \(\sealed\).

State elements that are outside of the stack---general-purpose memory used for
globals and the heap, as well as the code region and globally-relevant
registers---are always labeled \(\public\). We place security requirements on some
\(\public\) elements for purposes of \(\wbcf\), and a given enforcement mechanism
might restrict their access (e.g., by rendering code immutable) but for integrity
and confidentiality purposes they are considered accessible at all times.

For a newly active function, every element that is available for use but uninitialized
is seen as \(\unsealed\). From the perspective of the caller, the callee has no obligations
regarding its use of \(\unsealed\) elements, but the callee itself 

Arguments are seen as \(\object\), meaning that their contents may be safely used.
When the active function allocates memory, that memory will also be \(\object\).
Then, on a call, \(\object\) elements that are not being used to communicate with
the new callee will become \(\sealed\)---reserved for an inactive principal,
and expected to be unchanged when it becomes active again.

\paragraph*{Events}

It is common to specify the behavior of a system, including security properties, in
terms of traces of observable {\em events} rather than details of the machine state.
Lazy enforcement works because the deferred check catches errors before they appear
in the trace. The nature of events is a parameter of the machine, and can be specialized
to any notion of observable behavior.

\paragraph*{Variants and Non-interference}

To characterize confidentiality, we borrow the idea of \emph{variant} states
from the theory of non-interference. Non-interference is a way of expressing
the flow of information through the system. If we begin with a state \(\mach\)
and modify it to \(\nach = \mach[\component \mapsto v]\) for arbitrary \(v\),
then execute both \(\mach\) and \(\nach\), any differences in the outputs they
produce must be the result of accessing \(\component\).

\paragraph*{Putting It Together}

Putting these ideas together, we get the basic structure of a property in our model.
First we define the security-relevant operations that we support, a security context,
and the function according to which the security context updates with each step.
The security context gives each function activation a view, mapping each element to a
security class.

Then, we define our security requirements in terms of the security classes of state
elements. These take the form of predicates on states and contexts that use variants,
assertions about future return states, and comparison of event traces to capture the
desired behavior without reference to implementation details.

Separately, in order to apply a property to a specific machine, we do need to consider
implementation details, by determining how the machine's execution translates into
security relevant operations. But this step is transparent to the properties themselves.
