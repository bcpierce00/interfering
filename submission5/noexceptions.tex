%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[10pt,conference]{ieeetran}%\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{amsmath,amsthm,amssymb}

\usepackage{xcolor,listings}

\usepackage{multirow}

\usepackage{stmaryrd}

\usepackage[noadjust]{cite}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\input{macros}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\begin{document}

%% Title information
\title{Formalizing Stack Safety as a Security Property}

\author{
  \IEEEauthorblockN{
    Sean Noble Anderson
  }
  \IEEEauthorblockA{
    Portland State University\\
    ander28@pdx.edu\\
  }
  \and
  \IEEEauthorblockN{
    Leonidas Lampropoulos
  }
  \IEEEauthorblockA{
    University of Maryland, College Park\\
    leonidas@umd.edu\\
  }
  \and
  \IEEEauthorblockN{
    Roberto Blanco
  }
  \IEEEauthorblockA{
    Max Planck Institute for Security and Privacy\\
    roberto.blanco@mpi-sp.org\\
  }
  \linebreakand
  \IEEEauthorblockN{
    Benjamin C. Pierce
  }
  \IEEEauthorblockA{
    University of Pennsylvania\\
    bcpierce@cis.upenn.edu\\
  }
  \and
  \IEEEauthorblockN{
    Andrew Tolmach
  }
  \IEEEauthorblockA{
    Portland State University\\
    tolmach@pdx.edu\\
  }
}



%% Keywords
%% comma separated list
\ifcameraready
\keywords{Stack Safety, Micro-Policies}  %% \keywords are mandatory in final camera-ready submission
\fi

\maketitle

\begin{abstract}

``Stack safety'' is a term associated with a
variety of compiler, run-time, and hardware mechanisms for protecting stack
memory. But unlike heap memory, the stack does not correspond to any single
high-level language concept. The result is that
these mechanisms typically lack precise specifications,
relying instead on informal descriptions and examples of the bad
behaviors that they prevent. Such a specification is especially desirable
for an unusual enforcement mechanism, like the ``lazy'' stack safety
micro-policies proposed by Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}.

We propose a formal characterization of ``observable'' stack safety based on
concepts from language-based security. Stack safety is a
combination of an integrity property (``the private
state in each caller's stack frame is held invariant by the callee'')
and confidentiality properties (``the caller's and callee's behavior are
insensitive to one another's private state,''), both compatible with lazy tagging, as well as
control-flow property.

The stack governs a large number of language features that are often excluded
from discussion of stack safety. Our stack safety property targets a system with
both caller- and callee-saved registers, coroutines, arguments passed on the stack,
exceptions, [tail call elimination, heaps, and concurrency.]

We use these properties to validate Roessler and DeHon's micro-policies
via property-based random testing. Our testing successfully detects violations
in several broken variants, including Roessler and DeHon's original lazy policy.

\end{abstract}

\newcommand{\paragraphx}[1]{\emph{#1.}}

\section{Introduction}

The call stack is a perennial target for low-level attacks, leading to a
range of dire consequences, from leakage or corruption of private stack data
to control-flow hijacking. To foil such attacks, a profusion of
software and hardware protections have been proposed,
%
including stack canaries~\cite{Cowan+98},
bounds checking~\cite{NagarakatteZMZ09,NagarakatteZMZ10,DeviettiBMZ08},
split stacks~\cite{Kuznetsov+14},
shadow stacks~\cite{Dang+15,Shanbhogue+19},
capabilities~\cite{Woodruff+14,Chisnall+15,SkorstengaardLocal,SkorstengaardSTKJFP,Georges+21},
and hardware tagging~\cite{DBLP:conf/sp/RoesslerD18}.
  \ifaftersubmission\apt{Mostly from
  nick; there could be more}\bcp{Yes, going back to MIT days---we should
  include several more of these, if only to give readers the impression that
this is a well-studied mechanism (so formalizing its protections is
useful).}
\fi

The protections offered by such mechanisms are commonly described in terms
of concrete examples of attacks that they can prevent---corruption of return
addresses, buffer overflows, use of uninitialized variables, etc.---leaving
a more abstract characterization to the reader's intuition.  But these
mechanisms can be quite intricate, and the behaviors they aim to prevent are
subtle and varied. It can be hard to guess, based only on informal
intuitions, whether a given mechanism actually blocks all potential
attacks---or, conversely, whether it is overly conservative and disallows
system behaviors that are actually safe.
To settle such questions with confidence,
we need a precise, generic, and formal specification for stack
safety, as a basis both for comparing the security claims of different
enforcement techniques and for validating that these claims
are met by particular implementations. With the notable exception of the model used
in Skorstengaard et al.'s StkTokens work~\cite{SkorstengaardSTKJFP}, which we
discuss in Section \ref{sec:relwork}, there is a dearth of such properties in
the literature.

We propose a novel characterization of stack safety using the tools of language-based
security~\cite{sabelfeld2003language}, decomposing stack safety into
the {\em integrity} and {\em confidentiality} of the caller’s local state
during the callee's execution, and the confidentiality of the calle's local state
afterward, as well as the control flow protection of {\em well-bracketed control
  flow (WBCF)}~\cite{SkorstengaardSTKJFP}. We extend WBCF to a setting with exceptions,
in which there may be multiple points at which a callee can ``return.''

A key technical novelty in these definitions,
compared to standard formulations of confidentiality and integrity from the
security literature, is that they
are ``nested'': {\em each} caller is guaranteed protection from its
immediate callees (which, in turn, need protection from their immediate
callees, etc.).
Caller confidentiality is especially interesting. It is based on a traditional
notion of noninterference, but whereas ordinary noninterference
is an end-to-end hyper-property on whole program runs, caller
confidentiality is a nested form of noninterference that applies to
part of a run, requiring that the callee’s behavior is
invariant under hypothetical scrambling of the caller’s stack
but allowing the caller to access its own data upon return.

Our threat model is very strong, allowing the attacker
to execute arbitrary code, including attempting to smash
the stack to disrupt the program’s control flow. Our stack safety
properties demand that, even in the presence of such attackers, confidentiality
and integrity still apply.
%

For confidentiality and integrity, our properties are {\em observational},
meaning that they allow a function activation to read from and write to other activations'
data, as long as such reads and writes do not affect the observable behavior of the
system. This permissiveness enables our properties to handle lazy enforcement mechanisms.

To demonstrate the utility of our formal characterization, we use these
properties to validate and improve an existing enforcement mechanism, the
{\em stack safety micro-policies} of Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}, re-implemented
in the Coq proof assistant on top of a RISC-V specification.  We
use QuickChick~\cite{Denes:VSL2014,Pierce:SF4}, a property-based testing
tool for Coq, to generate random programs and check
that Roessler and DeHon's micro-policies correctly abort the ones that
attempt to violate one of our properties. Furthermore, we
%
check that the testing framework is able to generate counterexamples
that violate our properties but are \emph{not} halted by incorrect
enforcement variants---both variants that we accidentally created
during our re-implementation of the micro-policy and ones that we
intentionally crafted to be broken in order to increase our confidence
in testing and the enforcement mechanism itself.

We find that Roessler and DeHon's {\em Depth Isolation} micro-policy, in
which memory within each stack frame is tagged with the depth of
the function activation that owns the frame and access is
permitted only when an activation at that depth is currently executing, validates our
stepwise properties. On the other hand, our testing reveals that \emph{Lazy Tagging and Clearing}
violates the temporal aspect of confidentiality in
cases where data can leak across repeated calls to the same callee,
and also violates integrity if the leak uses the caller's frame. We
propose a variant of {\em Lazy Tagging and Clearing} that testably enforces
confidentiality, albeit at some performance cost.
%

%For ease of exposition, we initially assume a single simple
%stack with no sharing between callers and callees: all parameters and return
%values are passed in registers.  Later, we will introduce enhanced versions
%supporting
%(1) passing of scalar stack data,
%(2) callee-saves registers calling conventions, and
%(3) a coroutine system with a static layout.

In sum, we offer the following contributions:

\begin{itemize}
\item We formalize permissive stack confidentiality and integrity and integrity properties.
  Both are parameterized over a notion of external
  observation, and are violated only if accessing secrets or overwriting
  data causes a visible change in the system behavior %(\cref{sec:lazy}).
\item We apply these definitions to a complex system with features such as argument passing
  on the stack, callee-saves registers, exceptions, and a simple coroutine system% (\cref{sec:ext}).
\item We use property-based random testing to validate the relationship between
  our properties and micro-policies %(\cref{sec:testing})
\end{itemize}
%\Cref{sec:relwork,sec:future} discuss related and future work.

Our artifact contains Coq formalizations of our properties for illustrative purposes
as well as the testing implementation.

\section{The Setting}

We state our properties in terms of a generic machine subject to a few constraints,
but our examples and tests are focused on a RISC-V-like machine enhanced with PIPE,
a tag-based reference monitor. Importantly, such a machine has no explicit ``call''
instruction, only {\tt jal} instructions which may be used for that purpose or potentially
other purposes. Similarly, there is no ``return'', only {\tt jalr}, and so on.
We therefore define our properties under the assumption that we can distinguish the
steps that represent these abstract operations from those that do not---for instance,
by labeling instructions as calls, returns, etc.
Such a labeling scheme appears in
our testing environment (Section [TBD]) and is already necessary
to correctly apply tag-based enforcement in practice (cite something about this.)

The set of special operations that are recognized in this way are termed
{\it security-relevant operations}. Our properties are phrased as a {\it security semantics},
which extends the underlying machine with additional context about its
security principals and which registers and memory they require to be secure.
This context evolves dynamically through the execution of security-relevant operations.
This notion of security may be phrased as
predicates on future states, e.g. assertions of the form
``When control returns to me...'', or relations on traces of future execution
(hyper-properties).

Unlike approaches that give a safe-by-construction semantics
for various security-relevant operations and then prove secure compilation to the underlying
enforcement mechanism, our machine is explicitly not safe unless the enforcement
is correctly instantiated; the security semantics gives us the means to judge whether
an enforcement scheme implements stack safety or not. 

We will first give a detailed example of a security semantics for a simple setting
in which our security-relevant operations are restricted to making calls and returns, and allocating
private memory within the current function activation. We then extend this model
to the full system that we test, which features:
\begin{itemize}
\item Function calls and returns, with caller- and callee-saved registers
\item Allocation of strictly private and strictly public regions on the stack
\item Arguments spilled onto the stack and/or passed by reference
\item Exceptions
\item Tail-call Elimination
\end{itemize}

Finally, we give a property definition for stack safety in which (some) stack-allocated
objects are governed by a provenance-based capability system, accessible only to those
functions which have recieved a valid pointer to them.

In section (TBD) we discuss the extension of this model into a simple concurrency setup.

\paragraph*{Threat Model}

We must trust that our method of distinguishing security-relevant operations is accurate; if it
involves labels placed on code by a compiler, that means trusting that the compiler placed
those labels correctly. If operations occur that are not recognized, those operations
might not be guaranteed to protect their principals---for instance, an unlabeled call
might not protect the caller's data from the callee. On the flip side, applying an incorrect
label most likely means that the property becomes too strong to be enforced.

Otherwise, we do not assume that the code adheres to any particular
calling convention or implements a source language construct.
In particular, while we are agnostic as to the source
language, we certainly aim to support C, and so any source function might contain undefined
behavior resulting in its compilation to arbitrary machine code. A given enforcement
mechanism may place additional constraints, of course, particularly on the behavior of
call and return sequences.

In general, it is impossible to distinguish buggy source code from an attacker; in
our examples we will identify one function or another as an attacker, but we do not
require any static division between trusted and untrusted code, and aim to protect
even buggy code.

This is a strong threat model, but hardware and timing attacks are out of scope,
and our properties are termination insensitive as a result of the enforcement mechanism.

\paragraph*{Limitations}

Our concurrency model is fairly
simplistic, assuming a fixed number of threads each with its own dedicated processor.
We model memory safe stack objects, but not a heap. Regions outside of
stacks can be used however the compiler likes, including as a heap, but no protection is
built in and our properties assume that if a pointer to a stack object is stored there,
it is permanently compromised.

\subsection{The Basic Machine}

The building blocks of the machine are {\em words} and {\em registers}.
Words are ranged over by \(\word\) and, when used as addresses, \(\addr\),
and are drawn from the set \(\WORDS\).
Registers in the set \(\REGS\) are ranged over by \(\reg\), with the stack pointer
given the special name \(\SP\).
They are divided into sets of caller-saved and callee-saved registers;
Figure \ref{fig:RISCVregs} gives an example division for a RISC-V machine as will
appear in our examples and testing, along with the register names that will appear in
example code, and their default security class (explained below).

In addition to memory and registers, a machine has a program counter \(\PCname\)
and may have additional architecture-specific state; particularly relevant is state
that is used for security enforcement. We treat this as a set \(\mathcal{P}\) with
\(\PCname \in \mathcal{P}\).

\begin{figure}
  \begin{tabular}{| l | l | l |}
    \hline
    Set / & Names & Purpose \\
    Class & & \\
    \hline
    \(\mathit{CLR}\) / & {\tt t0} -- {\tt t6} & Caller-saved temps \\
    \(\unsealed\) & {\tt a0} -- {\tt a1} & Caller-saved args / return vals \\
    & {\tt a2} -- {\tt a7} & Caller-saved args \\
    & {\tt ra} & Return Address \\
    \hline
    \(\mathit{CLE}\) / & {\tt s0} -- {\tt s11} & Callee-saved \\
    \(\sealed\) & {\tt sp} & Stack Pointer \\  
    \hline
    \(\mathit{PUBLIC}\) / & {\tt gp} & Global Pointer  \\
    \(\public\) & {\tt tp} & Thread Pointer \\
    \hline
  \end{tabular}
  \caption{RISC-V integer register set}
  \label{fig:RISCVregs}
\end{figure}

Collectively addresses and registers are {\em state elements} \(\component\)
in the set \(\COMPONENTS ::= \WORDS + \REGS + \mathcal{P}\). A {\em machine state}
\(\mach\) is a map from components to words, written \(\mach[\component]\).

\sna{It occurs to me, based on some of your comments further down, that the old machine
  states were unclear. My intention was not for \(\mach\) to be a map, but for
  it to be an arbitrary type on which \(\cdot[\cdot]\) was defined. (Akin to
  the coq idiom of a record with a field t:Type and proj:t\(\rightarrow\)k\(\rightarrow\)w.)
  Therefore, implicitly, there could be additional state for enforcement and other
  purposes. The new presentation with P makes that explicit but is less elegant.
}

{\em Events} are drawn from a set \(\OBSS\) and ranged over by \(\obs\), with a
special silent event \(\tau\).
The machine step function
\(\mach \xrightarrow{\overline{\psi},\obs} \mach' \in \MACHS \rightarrow
\MACHS \times \mathit{list} ~ \Psi \times \OBSS\)
is labeled by an ordered list of security-relevant operations, defined below,
and an event.

While some enforcement mechanisms may cause a machine to
failstop, we model this as the machine silently diverging by remaining in the
same state. Since failstops are not distinguished from normal divergence,
our properties will suffer the limitations typical of
{\it termination insensitive} systems \cite{}. TODO: more detailed explanation later.

\subsection{Security-Relevant Operations, Principals, and Security State}

Security-relevant operations consist of calls, returns, and allocations.
Calls identify the address of the call's target,
and the argument registers. Allocations and deallocations are phrased in terms of a pair
of an offset from the stack pointer and a size. Returns don't require any additional information.
%
\begin{align*}
  \psi \in \Psi ::= & \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} & \\
  | & \mathbf{return} & \\
  | & \mathbf{alloc} ~ \mathit{off ~ sz} & \mathit{off, sz} \in \mathbb{N} \\
\end{align*}
%
Security principals are function activations: at any given time, pending activations
have security requirements that are recorded in a call stack, and the current
activation tracks context information that will inform its requirements when
it makes a call. Each activation has a {\it view}
of the system that maps each state element to a {\it security class}:
it may be private to an inactive principal (\(\sealed\)),
in an allocated object that is accessible to the current principal (\(\object\)),
available to be allocated (\(\unsealed\)), or
always accessible (\(\public\)). The {\it initial view} \(V_0\) maps all stack locations to \(\unsealed\),
all other locations to \(\public\), and registers based on which set they
belong to (Figure \ref{fig:RISCVregs}): \(\sealed\) for callee-saved,
\(\unsealed\) for caller-saved, and \(\public\) otherwise. Elements of \(\mathcal{P}\),
such as the program counter, are always \(\public\). Even if they might deserve their
own protection, that protection is orthogonal to stack safety.

\sna{And the payoff of \(\mathcal{P}\) being explicit: we can just state this.}

A {\it context} is a pair of the current activation's view and 
a list of views representing the call stack (pending inactive
principals), ranged over by \(\sigma\).  
%\[\context \in \CONTEXTS ::= \mathit{VIEW \times list ~ VIEW}\]
The initial context is \(\context_0 = (V_0, \emplist)\).

As the machine takes a step that executes a particular operation, it updates the
corresponding context, as described informally in the following section, and formally
in Section \ref{sec:props}.

%  Every security-relevant operation manipulates the call stack via a function
%\(Op : \MACHS \rightarrow \CONTEXTS \rightarrow \psi \rightarrow \CONTEXTS\).

%From a machine state and a context, we create a {\it combined state}
%\(s = (\mach,\context)\) and a transition \(\stepstounder{}\) on combined states,
%labeled with the same operations. Note that \(Op\) takes as its state argument
%\(\mach\), the state before the step.

%\judgmenttwo{\(\mach \xrightarrow{\overline{\psi}} \mach', \obs \)}
%            {\(\mathit{foldl} ~ (Op ~ \mach) ~ \context ~ \overline{\psi} = \context'\)}
%            {\((\mach,\context) \stepstounder{\overline{\psi}} (\mach', \context', \obs)\)}

%\subsection{Policy State}

%\apt{As we discussed at length, it would certainly be nice to avoid separate policy state if possible.}
%In addition to its elements, a system may contain additional ``policy'' state that carries
%information relevant to a hardware enforcement mechanism. We assume that policy state
%cannot influence execution, except possibly to cause the system to failstop, which is modeled
%as a state stepping to itself. A failstop is therefore indistinguishable from silent
%divergence.

%Formally, we can treat our machine state as
%\(\mach = (\mach[\cdot],\langle \mach \rangle) \in \MACHS
%::= (\COMPONENTS \rightarrow \WORDS) \times \POLS\) for some \(\POLS\).
%\apt{This overloaded notation doesn't work. $\mach$ can be either a pair or a map, not both.
%Note that earlier text strongly implies that it is a map.}
%Then \(\langle \mach \rangle\) is the security state. We assume that if
%\(\mach[\component] = \nach[\component]\) for all \(\component\),
%%\(\mach \stepstounder{\bar{\psi_1}} \mach'\), and \(\nach \stepstounder{\bar{\psi_2}} \nach'\),
%then \(\bar{\psi_1} = \bar{\psi_2}\) and either
%\(\mach = \mach'\), \(\nach = \nach'\), or \(\mach'[\component] = \nach'[\component]\)
%for all \(\component\).

%\section{Calls, Returns, and Private Allocations}
\section{Examples}

In this section we consider a simple setting with calls, returns, and private allocations.
Figure \ref{fig:main} gives the C code and corresponding compiled RISC-V code
for a function {\tt main}. (Code is compiled using the standard RISC-V
calling conventions~\cite{??}; the stack grows towards lower addresses. We assume that
{\tt main} begins at address 0 and its callee {\tt f} at address 100.)

{\tt main} takes a secret as its argument and initializes a variable to contain
potentially sensitive data. We assume that observable events are stores
to a special address, called {\tt out}. 
{\tt main} calls another function {\tt f},
and afterward may decide to output its secret based on the contents of
its sensitive data. In this case, since {\tt sensitive} is initialized to 0,
it should not---instead it will output the results of {\tt f}.

\begin{figure}
  \begin{subfigure}{\columnwidth}
    {\tt
      volatile int out;
      
      void main(int secret) \{

      ~ ~ int sensitive = 0;

      ~ ~ int res = f();

      ~ ~ if (sensitive == 42) 

      ~ ~ ~ ~ out = secret;

      ~ ~ else 

      ~ ~ ~ ~ out = res;

      \}}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \begin{tabular}{r l | l}
      \labeledrow{0:}{addi sp,sp,-20}{\(\mathbf{alloc} ~ (-20,20)\)}
      \labeledrow{4:}{sd ra,12(sp)}{}
      \labeledrow{8:}{sw a1,8(sp)}{}
      \labeledrow{12:}{sw zero,4(sp)}{}
      \labeledrow{16:}{jal f,ra}{\(\mathbf{call} ~ \mathtt{f} ~ \emplist ~ \emplist\)}
      \labeledrow{20:}{sw a0,0(sp)}{}
      \labeledrow{24:}{lw a4,4(sp)}{}
      \labeledrow{28:}{li a5,42}{}
      \labeledrow{32:}{bne a4,a5,L1}{}
      \labeledrow{36:}{lw a0,8(sp)}{}
      \labeledrow{40:}{sw a0,out}{}
      \labeledrow{44:}{j L2:}{}
      \labeledrow{L1, 48:}{lw a0,0(sp)}{}
      \labeledrow{52:}{sw a0,out}{}
      \labeledrow{L2, 56:}{ld ra,12(sp)}{}
      \labeledrow{60:}{addi sp,sp,20}{\(\mathbf{dealloc} ~ (0,20)\)}
      \labeledrow{64:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \includegraphics[width=\columnwidth]{stacklayout.png}
  \end{subfigure}

\caption{Example: Main}
\label{fig:main}
\end{figure}

Suppose that {\tt f} is not a (valid) C function at all, but an attacker seeking
to leak {\tt secret}! It might do so in a number of ways, shown as snippets of
assembly code in Figure \ref{fig:f}.\footnote{We arrange that all the snippets
have the same length just to simplify some later explanations.}
%
In Figure \ref{subfig:direct}, {\tt f} takes an offset from the stack
pointer, accesses {\tt secret}, and directly outputs it. But more
subtly, even if somehow prevented from outputting {\tt secret} directly, {\tt f}
can instead return that value so that {\tt main} stores it to {\tt out},
as in Figure \ref{subfig:indirect}.
%
Beyond simply reading {\tt secret}, the attacker might overwrite {\tt sensitive}
with 42, guaranteeing that {\tt main} publishes its own secret unintentionally
(Figure \ref{subfig:integrity})!
Attacks of this kind do not violate {\tt main}'s confidentiality, but its
{\it integrity}.
And in Figure \ref{subfig:WBCF}, the attacker arranges to return to the
wrong instruction, thereby bypassing the check and publishing {\tt secret} regardless,
violating the program's {\it well-bracketed control flow} (WBCF.)

\begin{figure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{lw a4,8(sp)}{}
      \labeledrow{104:}{sw a4,out}{}
      \labeledrow{108:}{li a0,1}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \caption{Leaking {\tt secret} directly}
    \label{subfig:direct}
  \end{subfigure}  
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{lw a4,8(sp)}{}
      \labeledrow{104:}{mov a4,a0}{}
      \labeledrow{108:}{sw zero,-4(sp)}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \caption{Leaking {\tt secret} indirectly}
    \label{subfig:indirect}
  \end{subfigure}  
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{li a5,42}{}
      \labeledrow{104:}{sw a5,4(sp)}{}
      \labeledrow{108:}{li a0,1}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking {\tt sensitive}}
    \label{subfig:integrity}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{addi ra,ra,16}{}
      \labeledrow{104:}{nop}{}
      \labeledrow{108:}{nop}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking control flow}
    \label{subfig:WBCF}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{addi sp,sp,8}{}
      \labeledrow{104:}{nop}{}
      \labeledrow{108:}{nop}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Relative address attack}
    \label{subfig:WBCF2}
  \end{subfigure}

  \caption{Assembly code for {\tt f} as an attacker}
  \label{fig:f}
\end{figure}

The security semantics for this program is based
on the security-relevant events noted in the right columns of Figures~\ref{fig:main}
and~\ref{fig:f}, namely execution of instructions that allocate or deallocate space,
make a call, or make a return.
Figure \ref{fig:exec1} shows how the corresponding security context
develops over the first few steps of the program. We assume execution begins
at the start of {\tt main} (\(\PCname = 0\)) and that the stack pointer is initially at
address 1000.
\(\xrightarrow{\overline{\psi}}\) transitions are numbered and
written \(\downarrow \overline{\psi}\) in this diagram; events are omitted.

Step 1 allocates a word each for {\tt secret}, {\tt sensitive}, and {\tt res}, as well
as two words for the return address. This will have the
effect of marking those bytes \(\object\), assuming they were previously
\(\unsealed\). (We use \(V\llbracket\cdot\rrbracket\) to denote updates to \(V\).)
%

\begin{figure*}
  \begin{subfigure}[t]{.55\textwidth}
    \vskip 0pt
    \begin{tabular}{|r|r||l|}
      \hline
      \(\PCname\) & \(\SP\) & Context \\
      \hline
      0 & 1000 & \(V_0, \emplist\) \\
      \hline
      \multicolumn{3}{l}{\(1 \downarrow [\mathbf{alloc} ~ (-20,20)]\)} \\
      \hline
      4 & 980 & \(V_1 = V_0 \llbracket 980..999 \mapsto \object\rrbracket, \emplist\) \\
      \hline
      \multicolumn{3}{l}{\(2 \downarrow []\)} \\
      \multicolumn{3}{l}{\(\ \ \vdots\)} \\
      \multicolumn{3}{l}{\(4 \downarrow []\)} \\
      \hline
      16 & 980 & \(V_1, \emplist\) \\
      \hline
      \multicolumn{3}{l}{\(5 \downarrow [\mathbf{call} ~ 100 ~ \emplist ~ \emplist]\)} \\
      \hline
      100 & 980 & \(V_2 = V_1 \llbracket 980..999 \mapsto \sealed, \mathtt{a0} \mapsto \unsealed\rrbracket,[V_1]\) \\
      \hline
      \multicolumn{3}{l}{\(6 \downarrow []\)} \\
      \multicolumn{3}{l}{\(\ \ \vdots\)} \\
      \multicolumn{3}{l}{\(8 \downarrow []\)} \\
      \hline
      112 & 980 & \(V_2,[V_1]\) \\
      \hline
      \multicolumn{3}{l}{\(9 \downarrow [\mathbf{return}]\)} \\
      \hline
      20 & 980  & \(V_1, \emplist\) \\
      \hline
    \end{tabular}
  \end{subfigure}
  \begin{subfigure}[t]{.4\textwidth}
    \vskip 0px

    \(\underbrace{~ ~ ~}_\unsealed \uparrow \SP \)

    \includegraphics[width=\columnwidth]{stack2.png}
    \includegraphics[width=\columnwidth]{stack3.png}
    \includegraphics[width=\columnwidth]{stack4.png}
  \end{subfigure}

\caption{Execution up through the return from {\tt f}\apt{Label rows on right. Also show registers?}}
\label{fig:exec1}
\end{figure*}
%
At step 5, the formerly active principal's record is pushed onto the inactive list.
Its return target is the return address of the call, 
and the stack pointer target is the stack pointer at the moment of call.
The callee's view is updated from the caller's such that all \(\object\) locations
become \(\sealed\). (In more complex settings, objects that are intentionally passed
to the callee will not get sealed, but for now we assume no sharing of memory between activations.)
For registers, \(\overline{\reg_{args}}\) tells us which registers are used as arguments,
in this case just {\tt a0}. These are mapped to \(\public\), while any non-argument, caller-saved
registers remain mapped to \(\unsealed\). All callee-save registers remain \(\sealed\) for all calls.
At step 9, {\tt f} returns, and the topmost inactive view, that of {\tt main}, is restored.

We now show how this security semantics can be used define notions of confidentiality,
integrity, and correct control flow in such a way that many classes of
bad behavior, including the attacks in Figure~\ref{fig:f}, are 
are detected as security violations. 

\paragraph*{Well-bracketed Control Flow}

To begin with, what if {\tt f} returns to an unexpected place (i.e. \(\PCname \neq 20\) or
\(\SP \neq 980\)). We consider this to
violate well-bracketed control flow (WBCF). WBCF is a relationship between
call steps and their corresponding return steps: just after the return, the program
counter should be at the next instruction following the call,
and the stack pointer should be the same as it was before the call.

Consider the attack in Figure \ref{subfig:WBCF}: the attacker adds
16 to the return address and then returns, thus bypassing the {\tt if}-test in the code and outputting
{\tt secret}. Before the call, the program counter is 16 and the stack pointer is 980,
so we define a predicate \(\ret\)
on states that should hold just after the return, namely, on states \(\mach\)
such that \(\mach[\PCname] = 20\) and \(\mach[\SP] = 980\).

We can identify the point just after the return (if a return occurs)
as the first state in which the pending call stack is smaller than it was
just after the call.
WBCF requires that if \(\mach\) is the state at that point, then \(\ret ~ \mach\) holds.
For nested calls, where the pending stack is initially larger, the same principle
applies: \(\ret ~ \mach\) must hold the next time the pending stack is the same size or smaller.

To see why WBCF also cares about the stack pointer, consider the attack in
Figure \ref{subfig:WBCF2}. Here the attacker returns with \(\SP' = 998\) instead of the
correct \(\SP = 980\). Given the layout of {\tt main}'s frame: 
\begin{tabular}{| l | l | l | l | l |}
  \multicolumn{1}{r}{\(\SP \downarrow\)} &
  \multicolumn{2}{r}{\(\SP' \downarrow\)} \\
  \hline
  res & sens & sec & ra & ra \\
  \hline
\end{tabular}

\vspace{\abovedisplayskip}

\noindent
In this scenario, when attempting to read {\tt sensitive}, {\tt main} will
read part of the return address, and then it will attempt to output
{\tt res}, but instead output {\tt secret}! Even absent other
kinds of data protection, the stack pointer {\it must} be restored
for the program to behave predictably.

\paragraph*{Stack Integrity}

Like WBCF, stack integrity defines at the call a condition that must hold upon
return. This time the condition applies to all of the memory that a function has
allocated. In Figure \ref{fig:exec1} we see the lifecycle of an allocated frame:
upon allocation, the view labeled is \(\object\), and when a call is made, it instead
becomes \(\sealed\). Intuitively, the integrity of {\tt main}
is preserved if, when control returns to it, it can rely on any \(\sealed\) elements
to be identical to when it made the call.
%
Again, we need to know when a caller has been returned to,
and we use the same mechanism of checking the depth of the call stack.
%
In the case of the call from {\tt main} to {\tt f}, the \(\sealed\) elements are the
addresses 980 through 999 and callee-saved registers such as
the stack pointer. Note that callee-saved registers often change
during the call---but if the caller accesses them after the call, it should find them
restored to their prior value.

While it would be simple to define integrity as ``all sealed elements retain their
values after the call,'' this would be stricter than necessary for many purposes.
For example, it arguably doesn't matter if {\tt f} overwrites {\tt main}'s data provided
that {\tt main} never reads that corrupted data.
This is the justification behind the lazy micro-policy~\cite{??}:
the monitor permits the caller to write into the
callee's frame, but taints that write so that the callee cannot read it later without
triggering a failstop. More generally, it is common to specify the behavior of a system,
including security properties, in terms of traces of observable events rather than details
of the machine state. For any such notion of observable behavior, we can consider
corruption by {\tt f} acceptable as long as it does not affect {\tt main}'s observable behavior.

To characterize this situation, we define a state element to be \emph{irrelevant}
if its value can be replaced with any other value without changing the observable behavior
of the machine. More precisely, for a set of elements \(\components\),
a pair of states \(\mach\) and \(\nach\)
are {\em \(\components\)-variants} if they agree on the value of every
element except those in \(\components\). The elements of \(\components\) are \emph{irrelevant}
in \(\mach\) if every variant state produces an equivalent trace
(see Section \ref{sec:events}); all other elements are \emph{relevant}.
We characterize \emph{observational integrity} as the property that
every relevant element that is \(\sealed\) under the callee's view is restored
to its original value at the return point.

\begin{figure}
  \includegraphics[width=\columnwidth]{variants.png}
  \caption{Integrity Violation}
  \label{fig:variant}
\end{figure}

The example in Figure \ref{subfig:integrity} modifies the value of {\tt sensitive},
which is \(\sealed\). Figure \ref{fig:variant} shows the state just after the call at step 5,
assuming that {\tt sec} is 5. Similar to WBCF, we will define
\(\intProp\) as a predicate on states that holds if
all relevant sealed addresses in \(\mach\) are the same as after step 5.
We once again require that \(\intProp\) hold on the state following the matching return.
That state is reached by step 9. Although {\tt sensitive} has changed, it might be
irrelevant. Consider a variant state in which {\tt sensitive} has any other value, arbitrarily
choosing 43. As execution continues after the return from the original state, it
passes the {\tt if}-test on {\tt sensitive}, whereas the execution from the variant does not, resulting
in differing outputs. Therefore {\tt sensitive} is relevant, so \(\intProp\) does not hold,
and integrity has indeed been violated.

\paragraph*{Caller Confidentiality}

We treat confidentiality as a form of non-interference as well: the confidentiality of a caller
means that its callee's behavior is dependent only on publicly visible data,
not the caller's private state. This also requires that the callee initialize
memory before reading it.
As we saw in the examples, we must consider both the observable events
that the callee produces during the call and the changes that the callee makes to the state that might
affect the caller after the callee returns.

Consider the state \(\mach\) after step 5, whose context is \((V_2,\sigma)\), with the attacker from
Figure \ref{subfig:direct}. We take a variant state over the set of element(s) that are
\(\sealed\) or \(\unsealed\) in \(V_2\), which happens to be the entire stack (see
Figure \ref{fig:variant2}.) If we take a trace of execution from each state until it returns,
the traces may differ, in this case outputting 5 (the original value of {\tt secret}) and
4 (its value in the variant) respectively. This is a violation of
{\it internal confidentiality}.

\begin{figure}
  \includegraphics[width=\columnwidth]{variants2.png}
  \caption{Internal Confidentiality Violation\apt{needs key cells to be colored}}
  \label{fig:variant2}
\end{figure}

But, in Figure \ref{subfig:indirect}, we also saw an attacker that exfiltrated the secret
by reading it and then returning it, in a context where the caller would output the returned
value. Figure \ref{fig:variant3} shows the behavior of the same variants under this attacker,
but in this case, there is no output during the call. Instead the value of {\tt secret} is
extracted and placed in {\tt a0}, the return value register. We wish to identify this as
a confidentiality violation, again by considering variants of the \(\sealed\) and \(\unsealed\)
elements in \(V_2\), but capturing the required property is subtle.  

To illustrate the issues, note that {\tt f} has also stores a 0 below the stack pointer.
%
\begin{figure}
  \includegraphics[width=\columnwidth]{variants3.png}
  \caption{Return-time Confidentiality Violation}
  \label{fig:variant3}
\end{figure}
%
Now consider three elements: the address \(\SP - 4\) (blue), the address \(\SP + 12\) (yellow),
and the register {\tt a0} (red). In the first case, the execution from the variant state
has changed the value at \(\SP - 4\). But both return states agree on that value. Therefore,
while the address was updated, it does not depend on a secret value.

In the second case, the return states disagree on the value of \(\SP + 12\). But in neither
case has that value changed since the original variants. So the difference is inherited from
the original variation, and does not depend on a secret value either. This matters because,
following the return, the caller is allowed to access its stack frame, so those addresses
may influence future execution legitimately.

But in the case of {\tt a0}, the value has changed during the call (in both the original
and the variant, although only one of these would be necessary), and its final value differs between the variants.
Therefore, it must depend on a secret (in fact, the variable {\tt secret}).
Unless {\tt a0} happens to be irrelevant in the original trace, this is a violation of what
we term {\it return-time confidentiality}.

Structurally, return-time confidentiality resembles integrity, but now dealing with
variants. We begin with a state immediately following
a call, \(\mach\). We consider an arbitrary variant state,
\(\nach\), which may vary any element that is \(\sealed\) or \(\unsealed\), 
i.e., any element that is not used legitimately to pass arguments. Caller confidentiality
therefore can be thought of as the callee's insensitivity to elements in its initial state
that are not part of the caller-callee interface.

In place of a predicate we define \(\conf\) as a binary relation on states, 
which holds on eventual return states \((\mach',\context')\) and
\((\nach',\context'')\)
iff all relevant elements that changed between \(\mach\) and \(\mach'\) or \(\nach\) and \(\nach'\)
are the same in \(\mach\) and \(\nach\). We then define
return-time confidentiality as holding if \(\conf\) holds on the matching return states
from \(\mach\) and \(\nach\), respectively.

Finally, we define caller confidentiality as the 
combination of internal and return-time confidentiality.

\paragraph*{Callee Confidentiality}

Although we presented our initial example from the perspective of the caller, a callee
may also have data that must be kept secret from its caller. Consider a callee that makes
a privileged system call to obtain a secret key, and uses that key to perform a specific
task. An untrustworthy or erroneous caller might attempt to read the key out of the callee's
memory after return.

We model this as nearly dual to caller integrity: the callee's secrets are those
values that it has stored in \(\unsealed\) elements. So, callee confidentiality means that
at the state following the callee's return, any element that was \(\unsealed\) under the
callee's view either retains its original value or is irrelevant.

\paragraph*{The Hierarchy of Stack Safety}

The forms of stack safety described above are largely independent of one another, and
can potentially be enforced separately in various combinations. This is important, because
many existing enforcement mechanisms enforce only some of the stack safety
properties. Stricter enforcement may have greater performance impacts, leading to decisions
about trade-offs between different degrees of stack safety.

In principal, these properties can exist in any combination, but in practice we see examples
of certain combinations that form a hierarchy. Many enforcement techniques focus purely on
well-bracketed control flow. Stack canaries aim to prevent certain attacks on the return
address, and shadow stacks with protection (e.g. Return Address Defender \cite{Chiueh2001RAD})
to enforce it completely.

Others combine this protection with some degree of memory protection,
chiefly focusing on integrity. Skorstengaard et al. \cite{SkorstengaardSTK} characterize stack
safety as the combination of WBCF and {\it local state encapsulation} (LSE), and prove that their
capability-based enforcement satisfies both. LSE corresponds to integrity, plus the
confidentiality of the caller's frame, but neither the unallocated stack nor data left
behind in it by previous callees.
They offer a fully abstract overlay semantics, which formalizes WBCF and LSE
together as a safe-by-construction abstract machine. That machine does not allow a callee to
read or write its caller's frame, but might allow it to read data that is left behind below
the caller's frame, which is weaker than our full caller confidentiality and rules out
callee confidentiality entirely. Georges et al. \cite{Georges22:TempsDesCerises} correct
both limitations.

\paragraph*{Moving Forward}

In the next section we will define traces and their (hyper-)properties.
Then in Section \ref{sec:facts} we will define a mechanism to make assertions
about what will happen on a future return, and Section \ref{sec:props} will use these
to give the formal definitions of integrity and confidentiality.

\section{Events and Traces}
\label{sec:events}

We abstract over the events that can be observed in the system, defining them
only as a set \(\OBSS\) that contains at least the element \(\tau\), the silent
event. Other events might represent certain function calls (i.e., system calls)
or writes to special addresses representing mmapped regions.
A {\em trace} is a nonempty, finite or infinite sequence
of events, ranged over by \(\obsT\).
We use ``\(\notfinished{}{}\)'' to represent ``cons'' for traces, reserving ``::''
for list-cons.

We write that execution from a state produces an observation trace
\(\mach,\context \hookrightarrow \obsT\) as follows, coinductively:

\judgmenttwo{\((\mach,\context) \stepstounder{} (\mach',\context',\obs)\)}
            {\((\mach',\context') \hookrightarrow \obsT\)}
            {\((\mach,\context) \hookrightarrow \notfinished{\obs}{\obsT}\)}

We define another relation that takes a trace until we have returned from the
active principal.
We write this \(d \downarrow (\mach,\context) \hookrightarrow \obsT\), where
\(d\) is the depth of the current call.

\judgment{\(|\sigma| < d\)}
         {\(d \downarrow (\mach,(V,\sigma)) \hookrightarrow \tau\)}

\judgmenttwobrlong{\((\mach,(V,\sigma)) \stepstounder{} (\mach',\context',\obs)\)}
                  {\(|\sigma| \geq d\)}
                  {\(d \downarrow (\mach',\context') \hookrightarrow \obsT\)}
                  {\(d \downarrow (\mach,(V,\sigma)) \hookrightarrow \notfinished{\obs}{\obsT}\)}

\paragraph*{Observational Similarity}

We say that two event traces $\obsT_1$ and $\obsT_2$ are {\em similar},
written \(\obsT_1 \eqsim \obsT_2\), if the sequence of non-silent events
is the same. That is, we compare up to deletion of \(\tau\) events.

\begin{minipage}{.4\columnwidth}
  \judgment{}{\(\obsT \eqsim \obsT\)}
\end{minipage}
\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\notfinished{\obs}{\obsT_1} \eqsim \notfinished{\obs}{\obsT_2}\)}
\end{minipage}

\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\notfinished{\tau}{\obsT_1} \eqsim \obsT_2\)}
\end{minipage}
\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\obsT_1 \eqsim \notfinished{\tau}{\obsT_2}\)}
\end{minipage}

\paragraph*{Variants and Irrelevant Values}

Two states are variants with respect to a set of elements, \(\components\),
if they agree on the value of every element not in \(\components\). Our
notion of non-interference involves comparing the traces of such
\(\components\)-variants. We use this to define sets of irrelevant elements.

\definition Machine states \(\mach\) and \(\nach\) are {\em \(\components\)-variants},
written \(\mach \approx_\components \nach\), if, for
all \(\component \not \in \components\), \(\mach[\component] = \nach[\component]\).

\definition An element set \(\components\) in state \((\mach,\context)\) contains
irrelevant values, written \((\mach,\context) \parallel \components\), if for all
\(\nach\) such that \(\mach \approx_{\components} \nach\), if 
\((\mach,\context) \hookrightarrow \obsT\) and
\((\nach,\context) \hookrightarrow \obsT'\), then
\(\obsT \eqsim \obsT'\).

\section{Facts Abouts Calls and Returns}
\label{sec:facts}

Here we define some logical operations to reason about the behavior of the
system over time. These have a temporal-logic flavor, as they reflect
the expected behavior of the system in the future, after a possible return.

\paragraph*{On-return}

The intuition behind integrity (below) is that a caller may expect its
sealed data to be unchanged when control returns to it. In fact, the callee
may overwrite such data---when the data are found in callee-saved registers
this is perfectly legal---as long as it either restores it, or has some guarantee
that its changes will not impact the caller.

We start by defining a second-order logical operator
\(d \uparrow P\), pronounced ``\(P\) holds on return from depth \(d\),''
where \(P\) is a predicate on machine states. This is a coinductive relation
similar to ``weak until'' in temporal logic---it also holds if the program never
returns from depth \(d\).

\judgmenttwo[Returned]
            {\(|\sigma| < d\)}
            {\(P ~ (\mach,(V,\sigma))\)}
            {\((d \uparrow P) ~ (\mach, (V,\sigma))\)}

\judgmenttwobrlong[Step]
                  {\(|\sigma| \geq d\)}
                  {\((d \uparrow P) ~ (\mach', \context')\)}
                  {\((\mach, (V,\sigma)) \stepstounder{\overline{\psi}} (\mach', \context',\obs)\)}
                  {\((d \uparrow P) ~ (\mach, (V,\sigma))\)}

Similarly, for confidentiality, we will want to compare pairs of future states,
so we give a binary equivalent, \(d \Uparrow R\), where
\((\mach,\context) ~ (d \Uparrow R) ~ (\mach',\context')\) holds if \(R\) holds on the
first states that return from depth \(d\) after \((\mach,\context)\) and \((\mach',\context')\),
respectively. Once again, \(\Uparrow\) is coinductive.

\judgmenttwobrlong[Returned]
            {\(|\sigma_1| < d\)}
            {\(|\sigma_2| < d\)}
            {\((\mach_1,(V_1,\sigma_1)) ~ R ~ (\mach_2,(V_2,\sigma_2))\)}
            {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\judgmenttwobrlong[Left]
              {\(|\sigma_1| \geq d\)}
              {\((\mach_1,(V_1,\sigma_1)) \stepstounder{\overline{\psi}} (\mach_1',\context_1',\obs)\)}
              {\((\mach_1',\context_1') ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}
              {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\judgmenttwobrlong[Right]
              {\(|\sigma_2| \geq d\)}
              {\((\mach_2,(V_2,\sigma_2)) \stepstounder{\overline{\psi}} (\mach_2',\context_2',\obs)\)}
              {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,\context_2')\)}
              {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\section{Properties}
\label{sec:props}

We now finally have everything we need to formalize our definitions for stack safety.
Figure \ref{fig:oprules} gives the operation rules for all of the operations we've described
so far.

\begin{figure}

  \judgmentbr[Alloc]
             {\(b = \mach[\SP] + \mathit{off}\)}
             {\(V' = V \llbracket \addr \mapsto \sealed |
               b \leq a < b+\mathit{sz} \land V ~ \addr = \unsealed \rrbracket\)}
             {\(Op ~ \mach ~ (\mathbf{alloc} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}

  \judgmentbr[Call]
             {\(\psi = \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa}\)}
             {\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
               \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
             {\(Op ~ \mach ~ \psi ~ (V,\sigma) =
               (V',V::\sigma)\)}

  \judgment[Ret]
           {\(\sigma = (V,\addr_{ret},\addr_{sp})::\sigma'\)}
           {\(Op ~ \mach ~ \mathbf{return} ~ (\_, \sigma) = (V, \sigma)\)}
           
%\judgment[RetDef]
%         {\(\chi = \mathbf{return}\)}
%         {\(Cop ~ \chi ~ (V, \emplist) ~ \mach \triangleq
%           (V, \emplist)\)}

  \caption{Core operation rules for the simple system}
  \label{fig:oprules}
\end{figure}

\subsection{Well-bracketed Control Flow}

\definition We define a correct return from \(\mach\) as:
%
\[\ret(\mach) = \{(\mach',\context') \mid \mach'[\SP] = \mach[\SP] \land \mach'[\PCname] = \mach[\PCname]+4\}\]

\definition
A system enjoys  well-bracketed control flow if, whenever
\((\mach,\context) \stepstounder{\overline{\psi}} (\mach',(V,\sigma),\obs)\) where 
\(\mathbf{call} ~ \addr ~ \overline{\reg} \in \bar{\psi}\),
\(|\sigma| \uparrow \ret(\mach)\) holds on \((\mach',(V,\sigma))\).

\subsection{Integrity}

\definition Let \(\Delta(\mach,\mach')\) be the set of elements \(\component\)
such that \(\mach[\component] \not = \mach'[\component]\).

\definition We define \(\intProp(\mach,V)\), to hold on a state \((\mach',\context')\) when
\(\{\component \mid V ~ \component = \sealed\} \cap \Delta(\mach,\mach')\)
is irrelevant in \((\mach', \context')\).

\definition A system enjoys {\it stack integrity} if, whenever
\((\mach,\context) \stepstounder{\overline{\psi}} (\mach',(V,\sigma),\obs)\) with
\(\mathbf{call} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \bar{\psi}\),
we have:
\[(|\sigma'| \uparrow \intProp(\mach',V)) ~ (\mach',(V,\sigma))\]

\subsection{Caller Confidentiality}

\definition Let \((\mach,\context)\) be a state with \(\context = (V,\sigma)\), and
\(\components = \{\component \mid V ~ \component \in \{\sealed, \unsealed\}\}\).
Then \((\mach,\context)\) enjoys {\it internal confidentiality} with respect to \(\components\)
if, for any \(\nach\) that is a \(\components\)-variant of \(\mach\), we can take
\(|\sigma| \uparrow (\mach,\context) \hookrightarrow \obsT\) and
\(|\sigma| \uparrow (\nach,\context) \hookrightarrow \obsT'\) and have that
\(\obsT \simeq \obsT'\).

\definition The {\em corrupted set} \(\bar{\Diamond}(\mach,\mach',\nach,\nach')\)
is the set \((\Delta(\mach,\mach') \cup \Delta(\nach,\nach')) \cap \Delta(\mach',\nach')\).
This treats
\(\mach\) and \(\nach\) as the initial states and
\(\mach'\) and \(\nach'\) as their corresponding final states, and is the
set of elements that were corrupted between them.

\definition We define the relation \(\conf(\mach,\nach)\) to hold on a pair of states
\((\mach',\context)\) and \((\nach',\context')\) when
\(\bar{\Diamond}(\mach,\nach,\mach',\nach')\) is irrelevant in \((\mach',\context)\).

\definition A state \((\mach,(V,\sigma))\) enjoys {\it return-time confidentiality}
if, for \(\components = \{\component \mid V ~ \component \in \{\sealed, \unsealed\}\}\)
and any \(\nach\) that is a \(\components\)-variant of \(\mach\), we have:
\[(\mach,(V,\sigma)) ~ (|\sigma| \Uparrow \conf(\mach,\nach) ~ (\nach,(V,\sigma))\]

\definition A system enjoys {\it caller confidentiality} if, whenever
\((\mach,\context) \stepstounder{\overline{\psi}} (\mach',\context',\obs)\)
with \(\mathbf{call} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \psi\),
\((\mach',\context')\) enjoys both internal and return-time confidentiality.

\subsection{Callee Confidentiality}

\definition We define \(\cconf(\mach,V)\), to hold on a state \((\mach',\context')\) when
\(\{\component \mid V ~ \component = \unsealed\} \cap \Delta(\mach,\mach')\)
is irrelevant in \((\mach', \context')\).

\definition A system enjoys {\it callee confidentiality} if, whenever
\((\mach,\context) \stepstounder{\overline{\psi}} (\mach',(V,\sigma),\obs)\)
with \(\mathbf{call} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \psi\),
% or \(\mathbf{tailcall} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \psi\),
we have:
\[(|\sigma| \uparrow \cconf(\mach',V)) ~ (\mach',\context')\]

\section{Advanced Machine}

The advanced machine defines additional operations, and adds a field \(\overline{sa}\) to
the \(\mathbf{call}\) operation, which is a set of triples of a register, a base offset, and a size,
and denotes that the object at the offset from the given register is to be passed as an argument.

\begin{align*}
  \psi \in \Psi ::= & \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa} &
  sa \in \REGS \times \mathbb{Z} \times \mathbb{N} \\
  | & \mathbf{tailcall} ~ \addr_{target}  ~ \overline{\reg_{args}} ~ \overline{sa} &
  sa \in \REGS \times \mathbb{Z} \times \mathbb{N} \\
  | & \mathbf{return} & \\
  | & \mathbf{alloc} ~ \mathit{off ~ sz} & \mathit{off, sz} \in \mathbb{N} \\
  | & \mathbf{dealloc} ~ \mathit{off ~ sz} & \mathit{off, sz} \in \mathbb{N} \\
\end{align*}

%We can extend our basic machine to support public memory allocations that can
%be accessed by anyone until they are deallocated on their owner's return,
%using the \(\mathbf{alloc\_pub}\) operation. This behaves identically to
%\(\mathbf{alloc\_priv}\) except that it marks the locations \(\public\).
%For example, consider the following code:

%{\tt
%  void g() \{

%  ~ ~ int x[3];

%  ~ ~ int y;

%  ~ ~ h(\&x);
    
%  \}
%}

%Here we have allocated several arrays and variables; {\tt y}
%will behave as normal with \(\mathit{alloc\_priv}\), but {\tt x} has its
%address taken and passed to another function. That function might do anything at
%all to {\tt x}, and we have to decide whether it can be said to violate stack
%safety. A reasonable default is to assume that any address-taken local should be
%public, while {\tt y} is still private.

%In the likely event that the space for the return address, {\tt x}, and {\tt y}
%are all allocated in a single step, that step needs multiple labels to capture
%that they are not all private. Its label would instead be:
%\[\begin{split}
%& \psi = [\mathbf{alloc\_priv} ~ (20,4); \\
%  & \mathbf{alloc\_pub} ~ (16,12) ; \mathbf{alloc\_priv} ~ (4,4)]
%\end{split}\]

%Allocating public objects affects the call stack very similarly to private ones,
%but instead of treating them as \(\sealed\) they become \(\public\). This makes these
%regions of memory able to transmit information between caller and callee without
%violating the stack safety properties. Note that there is no way to seal a location
%once it has become \(\public\); only when {\tt g} returns will the memory allocated
%to {\tt x} be available for private use.

%\judgmentbr{\(b = \mach[\SP] - \mathit{off}\)}
%           {\(V' = V \llbracket \addr \mapsto \public |
%             b \leq a < b+\mathit{sz} \land V ~ \addr = \unsealed \rrbracket\)}
%           {\(Pop ~ (\mathbf{alloc\_pub} ~ \mathit{off, sz}) ~ (V,\sigma) ~ \mach \triangleq
%             (V',\sigma)\)}

\subsection{Stack Arguments}

So far, we have not dealt with arguments spilled onto the stack, variadic arguments,
or arguments that are passed by reference without their addresses being taken
explicitly (i.e., structs and arrays passed directly.) Consider the simplest case,
where a function has more than 8 arguments and therefore must (in RISC-V) spill one
to the stack.

{\tt
  void main() \{

  ~ f(1,2,3,4,5,6,7,8,9);

  ~ g(42);
  
  \}
}

\vspace{\abovedisplayskip}

\begin{tabular}{r l | l}
  \labeledrow{0:}{addi sp,sp,-12}{\(\mathbf{alloc} ~ (-12,12)\)}
  \labeledrow{4:}{sd ra,4(sp)}{}
  \labeledrow{8:}{li a5,9}{}
  \labeledrow{12:}{sd a5,0(sp)}{}
  \labeledrow{16:}{li a7,8}{}
  \dots \\
  \labeledrow{48:}{jal 100,ra}{\(\mathbf{call} ~ \{\mathtt{a0-a7}\} ~ \{(\SP,0,4)\}\)}
  \labeledrow{52:}{li a0,42}{}
  \labeledrow{56:}{jal 200,ra}{\(\mathbf{call} ~ \{\mathtt{a0}\} ~ \emplist\)}
  \dots \\
\end{tabular}

Under a typical calling convention, the callee expects the ninth argument to
be passed to it at bottom of the caller's frame, so the caller allocates extra
space for it. Then the call to {\tt f} identifies that we expect a range of four
bytes from the stack pointer to be passed as an object instead of sealed.

So we must refine our call operation to make use of the information that we have about
which memory contain arguments, \(\overline{sa}\). \(\overline{sa}\) is a set of
triples of a register, an offset from the value of that register, and a size.
We first define the helpful set \(\mathit{passed} ~ \overline{sa} ~ \mach\),
then extend the call operation to keep all objects in \(\mathit{passed}\) marked
as \(\object\) and seal everything else.
%
\[\mathit{passed} ~ \overline{sa} ~ \mach \triangleq
\bigcup_{(\reg,b,o) \in \overline{sa}} \{\mach[\reg]+i | b \leq i < b+o\}\]
%
\judgmentbrbr[]
             {\(\psi = \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa}\)}
             {\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
               \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
             {\(V'' = V'\llbracket \addr \mapsto \sealed | V' ~ \addr = \object \land \addr \not \in (\mathit{passed} ~ \overline{sa} ~ \mach) \rrbracket\)}
             {\(Op ~ \mach ~ \psi ~ (V,\sigma) =
               (V',V::\sigma)\)}

The remaining definitions are unchanged. Now a stack-spilled argument can be allocated
and remain accessible in the callee, while its memory is sealed for calls that do not
use that memory as an argument.

This same structure works for pass-by-reference arguments: we know from the type of
the called function which register it expects to hold the address of a given object,
and the size of that object. In this case the offset will always be 0.

When the callee in such a scenario makes a call of its own, the objects passed to it
will in turn be sealed---unless it passes them on, which can happen with pass-by-reference
arguments!

\subsection{Provenance, Capabilities, and Protecting Objects}

What if we want to express a finer-grained notion of safety, in which
such objects are protected unless the function that owns them intentionally
passes a pointer to them?

In order to express such a property, we need our machine to carry some notion
of {\it pointer provenance}---a distinction between a pointer that is intended to
point to a given object, and non-pointer integers as well as pointers to other objects.
Then \(\mathit{passed}\) should recursively identify all objects that
are reachable transitively from valid pointers in registers and accessible memory;
these keep their \(\object\) status, while those that are not reachable become \(\sealed\).

\section{Enforcement}

\paragraph{Protecting Enforcement State}

State beyond registers and memory is always considered \(\public\), and in a PIPE machine this
includes tags. That does not mean that functions should be modifying them! It just means that we
do not consider them to be part of the state being protected. It is less obvious that a function
should be prevented reading a tag, and our properties do not rule it out, although in practice
PIPE does not allow such reads.

\bibliographystyle{IEEEtran}
\bibliography{bcp.bib,local.bib}

\end{document}
