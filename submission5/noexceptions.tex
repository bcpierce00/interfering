%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[10pt,conference]{ieeetran}%\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{amsmath,amsthm,amssymb}

\usepackage{xcolor,listings}

\usepackage{multirow}

\usepackage{stmaryrd}

\usepackage[noadjust]{cite}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\input{macros}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\begin{document}

%% Title information
\title{Formalizing Stack Safety as a Security Property}

\author{
  \IEEEauthorblockN{
    Sean Noble Anderson
  }
  \IEEEauthorblockA{
    Portland State University\\
    ander28@pdx.edu\\
  }
  \and
  \IEEEauthorblockN{
    Leonidas Lampropoulos
  }
  \IEEEauthorblockA{
    University of Maryland, College Park\\
    leonidas@umd.edu\\
  }
  \and
  \IEEEauthorblockN{
    Roberto Blanco
  }
  \IEEEauthorblockA{
    Max Planck Institute for Security and Privacy\\
    roberto.blanco@mpi-sp.org\\
  }
  \linebreakand
  \IEEEauthorblockN{
    Benjamin C. Pierce
  }
  \IEEEauthorblockA{
    University of Pennsylvania\\
    bcpierce@cis.upenn.edu\\
  }
  \and
  \IEEEauthorblockN{
    Andrew Tolmach
  }
  \IEEEauthorblockA{
    Portland State University\\
    tolmach@pdx.edu\\
  }
}



%% Keywords
%% comma separated list
\ifcameraready
\keywords{Stack Safety, Micro-Policies}  %% \keywords are mandatory in final camera-ready submission
\fi

\maketitle

\begin{abstract}

``Stack safety'' is a term associated with a
variety of compiler, run-time, and hardware mechanisms for protecting stack
memory. But unlike heap memory, the stack does not correspond to any single
high-level language concept. The result is that
these mechanisms typically lack precise specifications,
relying instead on informal descriptions and examples of the bad
behaviors that they prevent. Such a specification is especially desirable
for an unusual enforcement mechanism, like the ``lazy'' stack safety
micro-policies proposed by Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}.

We propose a formal characterization of ``observable'' stack safety based on
concepts from language-based security. Stack safety is a
combination of an integrity property (``the private
state in each caller's stack frame is held invariant by the callee'')
and confidentiality properties (``the caller's and callee's behavior are
insensitive to one another's private state,''), both compatible with lazy tagging, as well as
control-flow property.

The stack governs a large number of language features that are often excluded
from discussion of stack safety. Our stack safety property targets a system with
both caller- and callee-saved registers, coroutines, arguments passed on the stack,
exceptions, [tail call elimination, heaps, and concurrency.]

We use these properties to validate Roessler and DeHon's micro-policies
via property-based random testing. Our testing successfully detects violations
in several broken variants, including Roessler and DeHon's original lazy policy.

\end{abstract}

\newcommand{\paragraphx}[1]{\emph{#1.}}

\section{Introduction}

The call stack is a perennial target for low-level attacks, leading to a
range of dire consequences, from leakage or corruption of private stack data
to control-flow hijacking. To foil such attacks, a profusion of
software and hardware protections have been proposed,
%
including stack canaries~\cite{Cowan+98},
bounds checking~\cite{NagarakatteZMZ09,NagarakatteZMZ10,DeviettiBMZ08},
split stacks~\cite{Kuznetsov+14},
shadow stacks~\cite{Dang+15,Shanbhogue+19},
capabilities~\cite{Woodruff+14,Chisnall+15,SkorstengaardLocal,SkorstengaardSTKJFP,Georges+21},
and hardware tagging~\cite{DBLP:conf/sp/RoesslerD18}.
  \ifaftersubmission\apt{Mostly from
  nick; there could be more}\bcp{Yes, going back to MIT days---we should
  include several more of these, if only to give readers the impression that
this is a well-studied mechanism (so formalizing its protections is
useful).}
\fi

The protections offered by such mechanisms are commonly described in terms
of concrete examples of attacks that they can prevent---corruption of return
addresses, buffer overflows, use of uninitialized variables, etc.---leaving
a more abstract characterization to the reader's intuition.  But these
mechanisms can be quite intricate, and the behaviors they aim to prevent are
subtle and varied. It can be hard to guess, based only on informal
intuitions, whether a given mechanism actually blocks all potential
attacks---or, conversely, whether it is overly conservative and disallows
system behaviors that are actually safe.
To settle such questions with confidence,
we need a precise, generic, and formal specification for stack
safety, as a basis both for comparing the security claims of different
enforcement techniques and for validating that these claims
are met by particular implementations. With the notable exception of the model used
in Skorstengaard et al.'s StkTokens work~\cite{SkorstengaardSTKJFP}, which we
discuss in Section \ref{sec:relwork}, there is a dearth of such properties in
the literature.

We propose a novel characterization of stack safety using the tools of language-based
security~\cite{sabelfeld2003language}, decomposing stack safety into
the {\em integrity} and {\em confidentiality} of the caller’s local state
during the callee's execution, and the confidentiality of the calle's local state
afterward, as well as the control flow protection of {\em well-bracketed control
  flow (WBCF)}~\cite{SkorstengaardSTKJFP}. We extend WBCF to a setting with exceptions,
in which there may be multiple points at which a callee can ``return.''

A key technical novelty in these definitions,
compared to standard formulations of confidentiality and integrity from the
security literature, is that they
are ``nested'': {\em each} caller is guaranteed protection from its
immediate callees (which, in turn, need protection from their immediate
callees, etc.).
Caller confidentiality is especially interesting. It is based on a traditional
notion of noninterference, but whereas ordinary noninterference
is an end-to-end hyper-property on whole program runs, caller
confidentiality is a nested form of noninterference that applies to
part of a run, requiring that the callee’s behavior is
invariant under hypothetical scrambling of the caller’s stack
but allowing the caller to access its own data upon return.

Our threat model is very strong, allowing the attacker
to execute arbitrary code, including attempting to smash
the stack to disrupt the program’s control flow. Our stack safety
properties demand that, even in the presence of such attackers, confidentiality
and integrity still apply.
%

For confidentiality and integrity, our properties are {\em observational},
meaning that they allow a function activation to read from and write to other activations'
data, as long as such reads and writes do not affect the observable behavior of the
system. This permissiveness enables our properties to handle lazy enforcement mechanisms.

To demonstrate the utility of our formal characterization, we use these
properties to validate and improve an existing enforcement mechanism, the
{\em stack safety micro-policies} of Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}, re-implemented
in the Coq proof assistant on top of a RISC-V specification.  We
use QuickChick~\cite{Denes:VSL2014,Pierce:SF4}, a property-based testing
tool for Coq, to generate random programs and check
that Roessler and DeHon's micro-policies correctly abort the ones that
attempt to violate one of our properties. Furthermore, we
%
check that the testing framework is able to generate counterexamples
that violate our properties but are \emph{not} halted by incorrect
enforcement variants---both variants that we accidentally created
during our re-implementation of the micro-policy and ones that we
intentionally crafted to be broken in order to increase our confidence
in testing and the enforcement mechanism itself.

We find that Roessler and DeHon's {\em Depth Isolation} micro-policy, in
which memory within each stack frame is tagged with the depth of
the function activation that owns the frame and access is
permitted only when an activation at that depth is currently executing, validates our
stepwise properties. On the other hand, our testing reveals that \emph{Lazy Tagging and Clearing}
violates the temporal aspect of confidentiality in
cases where data can leak across repeated calls to the same callee,
and also violates integrity if the leak uses the caller's frame. We
propose a variant of {\em Lazy Tagging and Clearing} that testably enforces
confidentiality, albeit at some performance cost.
%

%For ease of exposition, we initially assume a single simple
%stack with no sharing between callers and callees: all parameters and return
%values are passed in registers.  Later, we will introduce enhanced versions
%supporting
%(1) passing of scalar stack data,
%(2) callee-saves registers calling conventions, and
%(3) a coroutine system with a static layout.

In sum, we offer the following contributions:

\begin{itemize}
\item We formalize permissive stack confidentiality and integrity and integrity properties.
  Both are parameterized over a notion of external
  observation, and are violated only if accessing secrets or overwriting
  data causes a visible change in the system behavior %(\cref{sec:lazy}).
\item We apply these definitions to a complex system with features such as argument passing
  on the stack, callee-saves registers, exceptions, and a simple coroutine system% (\cref{sec:ext}).
\item We use property-based random testing to validate the relationship between
  our properties and micro-policies %(\cref{sec:testing})
\end{itemize}
%\Cref{sec:relwork,sec:future} discuss related and future work.

Our artifact contains Coq formalizations of our properties for illustrative purposes
as well as the testing implementation.

\section{The Setting}

We state our properties in terms of a generic machine subject to a few constraints,
but our examples and tests are focused on a RISC-V-like machine enhanced with PIPE,
a tag-based reference monitor. Importantly, such a machine has no explicit ``call''
instruction, only {\tt jal} instructions which may be used for that purpose or potentially
other purposes. Similarly, there is no ``return'', only {\tt jalr}, and so on.
We therefore define our properties under the assumption that we can distinguish the
steps that represent these abstract operations from those that do not---for instance,
by labeling instructions as calls, returns, etc.
Such a labeling scheme appears in
our testing environment (Section [TBD]) and is already necessary
to correctly apply tag-based enforcement in practice (cite something about this.)

The set of special operations that are recognized in this way are termed
{\it security operations}. Our properties are phrased as a {\it security semantics},
which extends the underlying machine with additional context about its
security principals and which registers and memory they require to be secure.
This context evolves dynamically through the execution of security operations.
This notion of security may be phrased as
predicates on future states, e.g. assertions of the form
``When control returns to me...'', or relations on traces of future execution
(hyper-properties).

Unlike approaches that give a safe-by-construction semantics
for various security operations and then prove secure compilation to the underlying
enforcement mechanism, our machine is explicitly not safe unless the enforcement
is correctly instantiated; the security semantics gives us the means to judge whether
an enforcement scheme implements stack safety or not. 

We will first give a detailed example of a security semantics for a simple setting
in which our security operations are restricted to making calls and returns, and allocating
private memory within the current function activation. We then extend this model
to the full system that we test, which features:
\begin{itemize}
\item Function calls and returns, with caller- and callee-saved registers
\item Allocation of strictly private and strictly public regions on the stack
\item Arguments spilled onto the stack and/or passed by reference
\item Exceptions
\item Tail-call Elimination
\end{itemize}

Finally, we give a property definition for stack safety in which (some) stack-allocated
objects are governed by a provenance-based capability system, accessible only to those
functions which have recieved a valid pointer to them.

In section (TBD) we discuss the extension of this model into a simple concurrency setup.

\paragraph*{Threat Model}

We must trust that our method of distinguishing security operations is accurate; if it
involves labels placed on code by a compiler, that means trusting that the compiler placed
those labels correctly. If operations occur that are not recognized, those operations
might not be guaranteed to protect their principals---for instance, an unlabeled call
might not protect the caller's data from the callee. On the flip side, applying an incorrect
label most likely means that the property becomes too strong to be enforced.

Otherwise, we do not assume that the code adheres to any particular
calling convention or implements a source language construct.
In particular, while we are agnostic as to the source
language, we certainly aim to support C, and so any source function might contain undefined
behavior resulting in its compilation to arbitrary machine code. A given enforcement
mechanism may place additional constraints, of course, particularly on the behavior of
call and return sequences.

In general, it is impossible to distinguish buggy source code from an attacker; in
our examples we will identify one function or another as an attacker, but we do not
require any static division between trusted and untrusted code, and aim to protect
even buggy code.

This is a strong threat model, but hardware and timing attacks are out of scope,
and our properties are termination insensitive as a result of the enforcement mechanism.

\paragraph*{Limitations}

Our concurrency model is fairly
simplistic, assuming a fixed number of threads each with its own dedicated processor.
We model memory safe stack objects, but not a heap. Regions outside of
stacks can be used however the compiler likes, including as a heap, but no protection is
built in and our properties assume that if a pointer to a stack object is stored there,
it is permanently compromised.

\subsection{The Basic Machine}

The building blocks of the machine are {\em words} and {\em registers}.
Words are ranged over by \(\word\) and, when used as addresses, \(\addr\),
and are drawn from the set \(\WORDS\).
Registers in the set \(\REGS\) are ranged over by \(\reg\), with special names given to the
program counter \(\PCname\) and stack pointer \(\SP\).
They are divided into sets of caller-saved and callee-saved registers;
Figure \ref{fig:RISCVregs} gives an example division for a RISC-V machine as will
appear in our examples and testing, along with the register names that will appear in
example code, and their default security class (explained below).

\begin{figure}
  \begin{tabular}{| l | l | l |}
    \hline
    Set / & Names & Purpose \\
    Class & & \\
    \hline
    \(\mathit{CLR}\) / & {\tt t0} -- {\tt t6} & Caller-saved temps \\
    \(\unsealed\) & {\tt a0} -- {\tt a1} & Caller-saved args / return vals \\
    & {\tt a2} -- {\tt a7} & Caller-saved args \\
    \hline
    \(\mathit{CLE}\) / & {\tt s0} -- {\tt s7} & Callee-saved \\
    \(\sealed\) & {\tt ra} & Return Address \\
    & {\tt sp} & Stack Pointer \\  
    \hline
    \(\mathit{PUBLIC}\) / & {\tt pc} & Program Counter \\
    \(\public\) & & \\
    \hline
  \end{tabular}
  \caption{RISC-V register set}
  \label{fig:RISCVregs}
\end{figure}

Collectively addresses and registers are {\em state elements} \(\component\)
in the set \(\COMPONENTS ::= \WORDS + \REGS\).
%
{\em Machine states} are drawn from a set \(\MACHS\) ranged over by \(\mach\)
that defines a mapping \(\mach[\component]\) from state elements to words.
{\em Events} are drawn from a set \(\OBSS\) and ranged over by \(\obs\), with a
special silent event \(\tau\).
The machine step function
\(\mach \xrightarrow{\overline{\psi},\obs} \mach' \in \MACHS \rightarrow
\MACHS \times \mathit{list} ~ \Psi \times \OBSS\)
is labeled by an ordered list of security operations, defined below,
and an event.
While the tag-based enforcement mechanism may cause the machine to
failstop, we model this as the machine silently diverging by remaining in the
same state. Since failstops are not distinguished from normal execution,
our properties will suffer the limitations typical of
{\it termination insensitive} systems \cite{}. TODO: more detailed explanation later.

\subsection{Security Operations, Principals, and Security State}

Security operations consist of calls (including tailcalls), returns,
allocations and deallocations. Calls and tailcalls identify the address of the call's target,
and the argument registers. Allocations and deallocations are phrased in terms of a pair
of an offset from the stack pointer and a size. Returns don't require any additional information.
%
\begin{align*}
  \psi \in \Psi ::= & \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa} &
  sa \in \REGS \times \mathbb{Z} \times \mathbb{N} \\
  | & \mathbf{tailcall} ~ \addr_{target}  ~ \overline{\reg_{args}} ~ \overline{sa} &
  sa \in \REGS \times \mathbb{Z} \times \mathbb{N} \\
  | & \mathbf{return} & \\
  | & \mathbf{alloc} ~ \mathit{off ~ sz} & \mathit{off, sz} \in \mathbb{N} \\
  | & \mathbf{dealloc} ~ \mathit{off ~ sz} & \mathit{off, sz} \in \mathbb{N} \\
\end{align*}
%
Security principals are function activations: at any given time, pending activations
have security requirements that are recorded in a call stack, and the current
activation tracks context information that will inform its requirements when
it makes a call. Each activation has a {\it view}
of the system that maps each state element to a {\it security class}:
\[sc \in SEC ::= \sealed | \unsealed | \object | \public\]
\[V \in \mathit{VIEW} ::= \COMPONENTS \rightarrow SEC\]

A view indicates, from the perspective of a given principal, whether an element is
private to an inactive principal (\(\sealed\)),
in an allocated object that is accessible to the current principal (\(\object\)),
available to be allocated (\(\unsealed\)), or
always accessible (\(\public\)).
The {\it initial view} \(V_0\) maps all stack locations to \(\unsealed\),
all other locations to \(\public\), and registers based on which set they
belong to (Figure \ref{fig:RISCVregs}): \(\sealed\) for callee-saved,
\(\unsealed\) for caller-saved, and \(\public\) otherwise.

For convenience, we will abbreviate multiple updates of a mapping
\(V[\component_0 \mapsto sc_0][\component_1 \mapsto sc_1]\dots\)
as \(V \llbracket A \mapsto B | C \rrbracket\), where \(A\) and \(B\)
are fomulae with free variables bound to sets in \(C\).

We represent the call stack with a list of views representing pending inactive
principals, ranged over by \(\sigma\). A {\it context} is a pair of the current
activation's view and that list. The initial context is \(\context_0 = (V_0, \emplist)\).
\[\context \in \CONTEXTS ::= \mathit{VIEW \times list ~ VIEW}\]

Every security operation manipulates the call  stack via a function
\(Op : \MACHS \rightarrow \CONTEXTS \rightarrow \psi \rightarrow \CONTEXTS\).

From a machine state and a context, we create a {\it combined state}
\(s = (\mach,\context)\) and a transition \(\stepstounder{}\) on combined states,
labeled with the same operations. Note that \(Op\) takes as its state argument
\(\mach\), the state before the step.

\judgmenttwo{\(\mach \xrightarrow{\overline{\psi}} \mach', \obs \)}
            {\(\mathit{foldl} ~ (Op ~ \mach) ~ \context ~ \overline{\psi} = \context'\)}
            {\((\mach,\context) \stepstounder{\overline{\psi}} (\mach', \context', \obs)\)}

\subsection{Policy State}

In addition to its elements, a system may contain additional ``policy'' state that carries
information relevant to a hardware enforcement mechanism. We assume that policy state
does not influence execution, except to cause the system to failstop, which is modeled
as a state stepping to itself. A failstop is therefore indistinguishable from silent
divergence.

Formally, we can treat our machine state as
\(\mach = (\mach[\cdot],\langle) \mach \rangle \in \MACHS
::= (\COMPONENTS \rightarrow \WORDS) \times \POLS\) for some \(\POLS\).
Then \(\langle \mach \rangle\) is the security state. We assume that if
\(\mach[\component] = \nach[\component]\) for all \(\component\),
\(\mach \stepstounder{\bar{\psi_1}} \mach'\), and \(\nach \stepstounder{\bar{\psi_2}} \nach'\),
then \(\bar{\psi_1} = \bar{\psi_2}\) and either
\(\mach = \mach'\), \(\nach = \nach'\), or \(\mach'[\component] = \nach'[\component]\)
for all \(\component\).

%\section{Calls, Returns, and Private Allocations}
\section{Examples}

In this section we consider a simple setting with calls, returns, and private allocations.
Figure \ref{fig:main} gives a C function {\tt main}, and its compiled RISC-V code,
that takes a secret as its argument and initializes a variable to contain
potentially sensitive data. We will assume for this system that events are stores
to a special address, called {\tt out}.

{\tt main} then calls another function {\tt f},
and afterward may decide to output its secret based on the contents of
its sensitive data. In this case, since {\tt sensitive} is initialized to 0,
it should not---instead it will publish the results of {\tt f}.

\begin{figure}
  \begin{subfigure}{\columnwidth}
    {\tt
      void main(int secret) \{

      ~ ~ int sensitive = 0;

      ~ ~ int res = f();

      ~ ~ if (sensitive == 42) \{

      ~ ~ ~ ~ *out = secret;

      ~ ~ \} else \{

      ~ ~ ~ ~ *out = res;

      ~ ~ \}

      \}}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \begin{tabular}{r l | l}
      \labeledrow{0:}{addi sp,sp,-20}{\(\mathbf{alloc} ~ (-20,20)\)}
      \labeledrow{4:}{sd ra,12(sp)}{}
      \labeledrow{8:}{sw a1,8(sp)}{}
      \labeledrow{12:}{sw zero,4(sp)}{}
      \labeledrow{16:}{jal f,ra}{\(\mathbf{call} ~ \mathtt{f} ~ \emplist ~ \emplist\)}
      \labeledrow{20:}{sw r0,0(sp)}{}
      \labeledrow{24:}{lw a4,4(sp)}{}
      \labeledrow{28:}{li a5,42}{}
      \labeledrow{32:}{bne a4,a5,L1}{}
      \labeledrow{36:}{lw a0,8(sp)}{}
      \labeledrow{40:}{sw a0,out}{}
      \labeledrow{44:}{j L2:}{}
      \labeledrow{L1, 48:}{lw r0,0(sp)}{}
      \labeledrow{52:}{sw a0,out}{}
      \labeledrow{L2, 56:}{ld ra,12(sp)}{}
      \labeledrow{60:}{addi sp,sp,20}{\(\mathbf{dealloc} ~ (0,20)\)}
      \labeledrow{64:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \includegraphics[width=\columnwidth]{stacklayout.png}
  \end{subfigure}

\caption{Example: Main}
\label{fig:main}
\end{figure}

Suppose that {\tt f} is not a (valid) C function at all, but an attacker seeking
to leak {\tt secret}! It might do so in a number of ways, shown as snippets of
assembly code in Figure \ref{fig:f}.

In Figure \ref{subfig:direct}, {\tt f} takes an offset from the stack
pointer, accesses {\tt secret}, and directly outputs it. But more
subtly, even if somehow prevented from outputting {\tt secret} directly, {\tt f}
can instead return that value so that {\tt main} stores it to {\tt out},
as in Figure \ref{subfig:indirect}.

Beyond simply reading {\tt secret}, the attacker might overwrite {\tt sensitive}
with 42, guaranteeing that {\tt main} publishes its own secret unintentionally
(Figure \ref{subfig:integrity})!
Attacks of this kind do not violate {\tt main}'s confidentiality, but its
{\it integrity}.
And in Figure \ref{subfig:WBCF}, the attacker attempts to return to the
wrong instruction, thereby bypassing the check and publishing {\tt secret} regardless,
violating the program's {\it well-bracketed control flow} (WBCF.)

\begin{figure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{lw a4,8(sp)}{}
      \labeledrow{104:}{sw a4,out}{}
      \labeledrow{108:}{li a0,1}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \caption{Leaking {\tt secret} directly}
    \label{subfig:direct}
  \end{subfigure}  
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{lw a4,8(sp)}{}
      \labeledrow{104:}{mov a4,a0}{}
      \labeledrow{108:}{sw zero,-4(sp)}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \caption{Leaking {\tt secret} indirectly}
    \label{subfig:indirect}
  \end{subfigure}  
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{li a5,42}{}
      \labeledrow{104:}{sw a5,4(sp)}{}
      \labeledrow{108:}{li a0,1}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking {\tt sensitive}}
    \label{subfig:integrity}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{addi ra,ra,20}{}
      \labeledrow{104:}{nop}{}
      \labeledrow{108:}{nop}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking control flow}
    \label{subfig:WBCF}
  \end{subfigure}

  \caption{Assembly code for {\tt f} as an attacker}
  \label{fig:f}
\end{figure}

Figure \ref{fig:exec1} shows how the security context
develops over several steps, with labels on the steps that allocate space and then make
a call. Both program counter and stack pointer are initially 0, and the stack pointer
is shown for convenience.
\(\xrightarrow{\overline{\psi}}\) transitions are written \(\downarrow \overline{\psi}\)
in this diagram; events are omitted.

The first step  allocates words for {\tt secret}, {\tt sensitive}, and {\tt res}, as well
as two words for the return address. This will have the
effect of marking those bytes \(\object\), assuming they were previously
\(\unsealed\).

\begin{figure*}
  \begin{subfigure}[t]{.5\textwidth}
    \vskip 0pt
    \begin{tabular}{| l | l || l |}
      \hline
      Step & \(\SP\) & \(\context\) \\
      \hline
      0 & 0 & \(V_0, \emplist\) \\
      \hline
      \multicolumn{3}{l}{\(\downarrow [\mathbf{alloc} ~ (-20,20)]\)} \\
      \hline
      1 & -20 & \(V_1 = V_0 \llbracket -20 \ldots -1, \mapsto \object \rrbracket, \emplist\) \\
      \hline
      \multicolumn{3}{l}{\(\vdots\)} \\
      \hline
      4 & -20 & \(V_1, \emplist\) \\
      \hline
      \multicolumn{3}{l}{\(\downarrow [\mathbf{call} ~ 100 ~ \emplist ~ \emplist]\)} \\
      \hline
      5 & -20 & \(V_2 = V_1 [\mathtt{a0} \mapsto \unsealed],[V_1]\) \\
      \hline
      \multicolumn{3}{l}{\(\vdots\)} \\
      \hline
      8 & -20 & \(V_2,[V_1]\) \\
      \hline
      \multicolumn{3}{l}{\(\downarrow [\mathbf{return}]\)} \\
      \hline
      9 & -20 & \(V_1, \emplist\) \\
      \hline
    \end{tabular}
  \end{subfigure}
  \begin{subfigure}[t]{.4\textwidth}
    \vskip 0px
    \includegraphics[width=\columnwidth]{stack1.png}
    \includegraphics[width=\columnwidth]{stack2.png}
    \includegraphics[width=\columnwidth]{stack3.png}
    \includegraphics[width=\columnwidth]{stack4.png}
  \end{subfigure}

\caption{Execution through return from {\tt f}}
\label{fig:exec1}
\end{figure*}

On a call, the formerly active principal's record is pushed onto the inactive list.
Its return target is the return address of the call, 
and the stack pointer target is the stack pointer at the moment of call.
The callee's view is updated from the caller's such that all \(\object\) locations
become \(\sealed\). (In more complex settings, objects that are intentionally passed
to the callee will not get sealed, but for now we assume no sharing of memory between activations.)
For registers, \(\overline{\reg_{args}}\) tells us which registers are used as arguments,
in this case {\tt a0}. These are mapped to \(\public\), while any non-argument, caller-saved
registers are mapped to \(\unsealed\). All callee-save registers are always \(\sealed\) and
the program counter \(\public\), as they were initialized.

On step 8, {\tt f} returns, and we restore the topmost inactive principal,
{\tt main}'s principal. Now we will discuss the intuition behind how this
structure translates into stack safety properties that rule out different
classes of bad behavior.

\paragraph*{Well-bracketed Control Flow}

But what if {\tt f} returns to an unexpected place (i.e. \(\PCname \neq 20\) or
\(\SP \neq -20\)), as would be the case in attack (d)? We consider this to
violate well-bracketed control flow (WBCF). WBCF is a relationship between
call steps and their corresponding return steps: after the return, the program
counter should be at the instruction following the state before the call,
and the stack pointer should be the same as it was before the call.

Consider the attack in Figure \ref{subfig:WBCF}: the attacker adds
20 to the return address and then returns, thus bypassing the check and outputting
{\tt secret}. Before the call, the program counter is 16 and the stack pointer is 20,
so we define a predicate on states \(ret\) that holds when the
program counter and stack pointer are both 20.

Then we identify the state immediately following the return, if one occurs ---
this will be the first state in which the pending call stack is smaller than it was
inside of the call.
WBCF requires that if \(s\) after that return, \(ret ~ s\) will hold.
For nested calls, where the pending stack is initial larger, the same principal
applies: \(ret ~ s\) must hold the next time the pending stack is the same size or smaller.

To demonstrate why WBCF also cares about the stack pointer, consider an attack which
returns with the stack pointer at -12 instead of -20. Now, compare the layout of the
stack relative to the stack pointer as it should be (\(\SP\)) to the one that results by
increasing the stack pointer before return (\(\SP'\)):

\begin{tabular}{| l | l | l | l | l |}
  \multicolumn{1}{r}{\(\SP \downarrow\)} &
  \multicolumn{2}{r}{\(\SP' \downarrow\)} \\
  \hline
  res & sens & sec & ra & ra \\
  \hline
\end{tabular}

\vspace{\abovedisplayskip}

In this scenario, when attempting to read {\tt sensitive}, {\tt main} will
read part of the return address, and then it will attempt to output
{\tt res}, but instead output {\tt secret}! Even absent other
kinds of data protection, the stack pointer {\it must} be restored
for the program to behave predictably.

\paragraph*{Stack Integrity}

Stack integrity applies a similar notion to all of the memory that a function has
allocated. In Figure \ref{fig:exec1} we see the lifecycle of an allocated frame:
upon allocation, the view labeled is \(\object\), and when a call is made, it instead
becomes \(\sealed\). In more sophisticated systems discussed later, we will distinguish
sealed objects from those that are intentionally passed to the callee.
Intuitively, the integrity of {\tt main}
is preserved if, when control returns to it, it can rely on any \(\sealed\) elements
to be identical to when it made the call.

In the case of the call from {\tt main} to {\tt f}, the \(\sealed\) elements are the
addresses -20 through -1 (allocated as private data) and callee-saved registers such as
the stack pointer and return address. Note that callee-saved registers often change
during the call---but if so, the callee is obligated to restore them to their prior value.

One further caveat: it is possible that {\tt f} might overwrite {\tt main}'s data
in a way that is harmless. Or, an enforcement mechanism might defer the checking
for illegal writes until later in the execution, entering an error state at some
point after return but before the illegal write is able to damage the system.
This is the case in a lazy micro-policy: the monitor permits the caller to write into the
callee's frame, but taints that write so that the callee cannot read it later without
triggering a failstop. In such a scenario we will consider a value {\it irrelevant}
if it could be replaced with any other value without changing the observable behavior
of the machine. More precisely, for a set of elements \(\components\),
a pair of states \(\mach\) and \(\nach\)
are {\em \(\components\)-variants} if they agree on the value of every
element except those in \(\components\), and share the same policy state.
The elements of \(\components\) are said to be
irrelevant in \(\mach\) if every variant state produces an equivalent trace
(see Section \ref{sec:events}.)

So, to define integrity, we again need to know when a caller has been returned to,
and we use the same mechanism of checking the depth of the call stack.
Integrity holds if, at that returned state,
any element that is \(\sealed\) under the callee's view is either restored
to its original value or is irrelevant.

\begin{figure}
  \includegraphics[width=\columnwidth]{variants.png}
  \caption{Integrity Violation}
  \label{fig:variant}
\end{figure}

The example in Figure \ref{subfig:integrity} modifies the value of {\tt sensitive},
which is \(\sealed\). Figure \ref{fig:variant} shows the state at step 5, just after the call,
assuming that {\tt sec} is 5. Similar to WBCF, we will define
\(int ~ s\) as a predicate on states that holds if, of the sealed
addresses in state 5, all values that differ in \(s\) are irrelevant. We once again
require that \(int\) hold on the state following the matching return.

That state is reached at step 9. Since {\tt sensitive} has changed, we must determine whether
it is irrelevant. We consider a variant state in which it has any other value, arbitrarily
choosing 43. As execution continues after the return from the original state, it
passes the check on {\tt sensitive}, while the execution from the variant does not, resulting
in differing outputs. Therefore {\tt sensitive} is not irrelevant, so \(int\) does not hold,
and integrity has been violated.

\paragraph*{Caller Confidentiality}

We treat confidentiality as a form of non-interference as well: the confidentiality of a caller
means that its callee's behavior is dependent only on publicly visible data,
not the caller's private state. This also requires that the callee initialize
memory before reading it.
As we saw in the examples, we must consider both the observable events
that the callee produces during the call and the changes that the callee makes to the state that might
affect the caller after the callee returns.

Consider again the state \((\mach, (V_2,\sigma))\) at step 5, this time with the attacker from
Figure \ref{subfig:direct}. We take a variant state over the set of element(s) that are
\(\sealed\) or \(\unsealed\) in \(V_2\), which happens to be the entire stack (see
Figure \ref{fig:variant2}.) If we take a trace of execution from each state until it returns,
the traces may differ, in this case outputting 5 (the original value of {\tt sec}) and
4 (its value in the variant) respectively. This is a violation of
{\it internal confidentiality}.

\begin{figure}
  \includegraphics[width=\columnwidth]{variants2.png}
  \caption{Internal Confidentiality Violation}
  \label{fig:variant2}
\end{figure}

But, in Figure \ref{subfig:indirect}, we also saw an attacker that exfiltrated the secret
by reading it and then returning it, in a context where the caller would output the returned
value. Figure \ref{subfig:variant3} shows the behavior of the same variants under this attacker,
but in this case, there is no output during the call. Instead the value of {\tt secret} is
extracted and placed in {\tt a0}, the return value register. To illustrate the subtlety
of identifying a leak, {\tt f} has also stored a 0 below the stack pointer.

\begin{figure}
  \includegraphics[width=\columnwidth]{variants3.png}
  \caption{Return-time Confidentiality Violation}
  \label{fig:variant3}
\end{figure}

Consider three elements: the address \(\SP - 4\) (blue), the address \(\SP + 12\) (yellow),
and the register {\tt a0} (red). In the first case, the execution from the variant state
has changed the value at \(\SP - 4\). But both return states agree on that value. Therefore,
while the address was updated, it does not depend on a secret value.

In the second case, the return states disagree on the value of \(\SP + 12\). But in neither
case has that value changed since the original variants. So the difference is inherited from
the original variation, and does not depend on a secret value either. This matters because,
following the return, the caller is allowed to access its stack frame, so those addresses
may influence future execution legitimately.

But in the case of {\tt a0}, the value has changed during the call (on both sides,
although only one would be necessary.) And its final value differs between the variants.
Therefore, it must depend on a secret. Indeed, we happen to know that it depends on {\tt secret}
itself. We must therefore insist that {\tt a0} be irrelevant in the original trace.
We term this property {\it return-time confidentiality}.

Structurally, return-time confidentiality resembles integrity, but now dealing with
variants. We begin with a state immediately following
a call. Let that be \((\mach,\context)\). We suppose an arbitrary variant state,
\((\nach,\context)\), which may vary any element that is \(\sealed\) or \(\unsealed\).
That is, any element that is not used legitimately to pass arguments. Caller confidentiality
therefore can be thought of as the callee's insensitivity to parts of its initial states
that are not part of the caller-callee interface.

In place of a predicate we define \(\mathit{conf}\) as a binary relation on states.
\(\mathit{conf}\) will hold on eventual return states \((\mach',\context')\) and
\((\nach',\context'')\)
if all elements that changed between \(\mach\) and \(\mach'\) or \(\nach\) and \(\nach'\),
and which differ between \(\mach\) and \(\nach\), are irrelevant. We then define
return-time confidentiality as holding if \(conf\) holds on the matching return states
from \(\mach\) and \(\nach\), respectively.

Together, these definitions encompass the confidentiality of the caller from the callee.

\paragraph*{Callee Confidentiality}

Although we presented our initial example from the perspective of the caller, a callee
may also have data that must be kept secret from its caller. Consider a callee that makes
a privileged system call to obtain a secret key, and uses that key to perform a specific
task. An untrustworthy or erroneous caller might attempt to read the key out of the callee's
memory after return.

We model this as nearly dual to caller integrity: the callee's secrets are those
values that it has stored in \(\unsealed\) elements. So, callee confidentiality means that
at the state following the callee's return, any element that was \(\unsealed\) under the
callee's view either retains its original value or is irrelevant.

\paragraph*{The Hierarchy of Stack Safety}

The forms of stack safety described above are largely independent of one another, and
can potentially be enforced separately in various combinations. This is important, because
many existing enforcement mechanisms enforce only some of the stack safety
properties. Stricter enforcement may have greater performance impacts, leading to decisions
about trade-offs between different degrees of stack safety.

In principal, these properties can exist in any combination, but in practice we see examples
of certain combinations that form a hierarchy. Many enforcement techniques focus purely on
well-bracketed control flow. Stack canaries aim to prevent certain attacks on the return
address, and shadow stacks with protection (e.g. Return Address Defender \cite{Chiueh2001RAD})
to enforce it completely.

Others combine this protection with some degree of memory protection,
chiefly focusing on integrity. Skorstengaard et al. \cite{SkorstengaardSTK} characterize stack
safety as the combination of WBCF and {\it local state encapsulation} (LSE), and prove that their
capability-based enforcement satisfies both. LSE corresponds to integrity, plus the
confidentiality of the caller's frame, but neither the unallocated stack nor data left
behind in it by previous callees.
They offer a fully abstract overlay semantics, which formalizes WBCF and LSE
together as a safe-by-construction abstract machine. That machine does not allow a callee to
read or write its caller's frame, but might allow it to read data that is left behind below
the caller's frame, which is weaker than our full caller confidentiality and rules out
callee confidentiality entirely. Georges et al. \cite{Georges22:TempsDesCerises} correct
both limitations.

\paragraph*{Moving Forward}

In the next section we will define traces and their (hyper-)properties.
Then in Section \ref{sec:facts} we will define a mechanism to make assertions
about what will happen on a future return, and Section \ref{sec:props} will use these
to give the formal definitions of integrity and confidentiality.

\section{Events and Traces}
\label{sec:events}

We abstract over the events that can be observed in the system, defining them
only as a set \(\OBSS\) that contains at least the element \(\tau\), the silent
event. Other events might represent certain function calls (i.e., system calls)
or writes to special addresses representing mmapped regions.
A {\em trace} is a nonempty, finite or infinite sequence
of events, ranged over by \(\obsT\).
We use ``\(\notfinished{}{}\)'' to represent ``cons'' for traces, reserving ``::''
for list-cons.

We write that execution from a state produces an observation trace
\(\mach,\context \hookrightarrow \obsT\) as follows, coinductively:

\judgmenttwo{\((\mach,\context) \stepstounder{} (\mach',\context',\obs)\)}
            {\((\mach',\context') \hookrightarrow \obsT\)}
            {\((\mach,\context) \hookrightarrow \notfinished{\obs}{\obsT}\)}

We define another relation that takes a trace until we have returned from the
active principal.
We write this \(d \downarrow (\mach,\context) \hookrightarrow \obsT\), where
\(d\) is the depth of the current call.

\judgment{\(|\sigma| < d\)}
         {\(d \downarrow (\mach,(V,\sigma)) \hookrightarrow \tau\)}

\judgmenttwobrlong{\((\mach,(V,\sigma)) \stepstounder{} (\mach',\context',\obs)\)}
                  {\(|\sigma| \geq d\)}
                  {\(d \downarrow (\mach',\context') \hookrightarrow \obsT\)}
                  {\(d \downarrow (\mach,(V,\sigma)) \hookrightarrow \notfinished{\obs}{\obsT}\)}

\paragraph*{Observational Similarity}

We say that two event traces $\obsT_1$ and $\obsT_2$ are {\em similar},
written \(\obsT_1 \eqsim \obsT_2\), if the sequence of non-silent events
is the same. That is, we compare up to deletion of \(\tau\) events.

\begin{minipage}{.4\columnwidth}
  \judgment{}{\(\obsT \eqsim \obsT\)}
\end{minipage}
\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\notfinished{\obs}{\obsT_1} \eqsim \notfinished{\obs}{\obsT_2}\)}
\end{minipage}

\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\notfinished{\tau}{\obsT_1} \eqsim \obsT_2\)}
\end{minipage}
\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\obsT_1 \eqsim \notfinished{\tau}{\obsT_2}\)}
\end{minipage}

\paragraph*{Variants and Irrelevant Values}

Two states are variants with respect to a set of elements, \(\components\),
if they agree on the value of every element not in \(\components\). Our
notion of non-interference involves comparing the traces of such
\(\components\)-variants. We use this to define sets of irrelevant elements.

\definition Machine states \(\mach\) and \(\nach\) are {\em \(\components\)-variants},
written \(\mach \approx_\components \nach\), if, for
all \(\component \not \in \components\), \(\mach[\component] = \nach[\component]\),
and \(\langle\mach\rangle = \langle\nach\rangle\).

\definition An element set \(\components\) in state \((\mach,\context)\) contains
irrelevant values, written \((\mach,\context) \parallel \components\), if for all
\(\nach\) such that \(\mach \approx_{\components} \nach\), if 
\((\mach,\context) \hookrightarrow \obsT\) and
\((\nach,\context) \hookrightarrow \obsT'\), then
\(\obsT \eqsim \obsT'\).

\section{Facts Abouts Calls and Returns}
\label{sec:facts}

Here we define some logical operations to reason about the behavior of the
system over time. These have a temporal-logic flavor, as they reflect
the expected behavior of the system in the future, after a possible return.

\paragraph*{On-return}

The intuition behind integrity (below) is that a caller may expect its
sealed data to be unchanged when control returns to it. In fact, the callee
may overwrite such data---when the data are found in callee-saved registers
this is perfectly legal---as long as it either restores it, or has some guarantee
that its changes will not impact the caller.

We start by defining a second-order logical operator
\(d \uparrow P\), pronounced ``\(P\) holds on return from depth \(d\),''
where \(P\) is a predicate on machine states. This is a coinductive relation
similar to ``weak until'' in temporal logic---it also holds if the program never
returns from depth \(d\).

\judgmenttwo[Returned]
            {\(|\sigma| < d\)}
            {\(P ~ (\mach,(V,\sigma))\)}
            {\((d \uparrow P) ~ (\mach, (V,\sigma))\)}

\judgmenttwobrlong[Step]
                  {\(|\sigma| \geq d\)}
                  {\((d \uparrow P) ~ (\mach', \context')\)}
                  {\((\mach, (V,\sigma)) \stepstounder{\overline{\psi}} (\mach', \context',\obs)\)}
                  {\((d \uparrow P) ~ (\mach, (V,\sigma))\)}

Similarly, for confidentiality, we will want to compare pairs of future states,
so we give a binary equivalent, \(d \Uparrow R\), where
\((\mach,\context) ~ (d \Uparrow R) ~ (\mach',\context')\) holds if \(R\) holds on the
first states that return from depth \(d\) after \((\mach,\context)\) and \((\mach',\context')\),
respectively. Once again, \(\Uparrow\) is coinductive.

\judgmenttwobrlong[Returned]
            {\(|\sigma_1| < d\)}
            {\(|\sigma_2| < d\)}
            {\((\mach_1,(V_1,\sigma_1)) ~ R ~ (\mach_2,(V_2,\sigma_2))\)}
            {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\judgmenttwobrlong[Left]
              {\(|\sigma_1| \geq d\)}
              {\((\mach_1,(V_1,\sigma_1)) \stepstounder{\overline{\psi}} (\mach_1',\context_1',\obs)\)}
              {\((\mach_1',\context_1') ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}
              {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\judgmenttwobrlong[Right]
              {\(|\sigma_2| \geq d\)}
              {\((\mach_2,(V_2,\sigma_2)) \stepstounder{\overline{\psi}} (\mach_2',\context_2',\obs)\)}
              {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,\context_2')\)}
              {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\section{Properties}
\label{sec:props}

We now finally have everything we need to formalize our definitions for stack safety.
Figure \ref{fig:oprules} gives the operation rules for all of the operations we've described
so far.

\begin{figure}

  \judgmentbr[Alloc]
             {\(b = \mach[\SP] + \mathit{off}\)}
             {\(V' = V \llbracket \addr \mapsto \sealed |
               b \leq a < b+\mathit{sz} \land V ~ \addr = \unsealed \rrbracket\)}
             {\(Op ~ \mach ~ (\mathbf{alloc} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}

  \judgmentbr[Call]
             {\(\psi = \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa}\)}
             {\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
               \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
             {\(Op ~ \mach ~ \psi ~ (V,\sigma) =
               (V',V::\sigma)\)}

  \judgment[Ret]
           {\(\sigma = (V,\addr_{ret},\addr_{sp})::\sigma'\)}
           {\(Op ~ \mach ~ \mathbf{return} ~ (\_, \sigma) = (V, \sigma)\)}
           
%\judgment[RetDef]
%         {\(\chi = \mathbf{return}\)}
%         {\(Cop ~ \chi ~ (V, \emplist) ~ \mach \triangleq
%           (V, \emplist)\)}

  \caption{Core operation rules for the simple system}
  \label{fig:oprules}
\end{figure}

\subsection{Well-bracketed Control Flow}

\definition We define a correct return from \(\mach\) as:
%
\[ret(\mach) = \{(\mach',\context') \mid \mach'[\SP] = \mach[\SP] \land \mach'[\PCname] = \mach[\PCname]+4\}\]

\definition
A system enjoys  well-bracketed control flow if, whenever
\((\mach,\context) \stepstounder{\overline{\psi}} (\mach',(V,\sigma),\obs)\) where 
\(\mathbf{call} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \bar{\psi}\),
%or \(\mathbf{tailcall} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \psi\)
\(|\sigma| \uparrow ret(\mach)\) holds on \((\mach',(V,\sigma))\).

\subsection{Integrity}

\definition Let \(\Delta(\mach,\mach')\) be the set of elements \(\component\)
such that \(\mach[\component] \not = \mach'[\component]\).

\definition We define \(int(\mach,V)\), to hold on a state \((\mach',\context')\) when
\(\{\component \mid V ~ \component = \sealed\} \cap \Delta(\mach,\mach')\)
is irrelevant in \((\mach', \context')\).

\definition A system enjoys {\it stack integrity} if, whenever
\((\mach,\context) \stepstounder{\overline{\psi}} (\mach',(V,\sigma),\obs)\) with
\(\mathbf{call} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \bar{\psi}\),
% or \(\mathbf{tailcall} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \psi\),
we have:
\[(|\sigma'| \uparrow int(\mach',V)) ~ (\mach',(V,\sigma))\]

\subsection{Caller Confidentiality}

\definition Let \((\mach,\context)\) be a state with \(\context = (V,\sigma)\), and
\(\components = \{\component \mid V ~ \component \in \{\sealed, \unsealed\}\}\).
Then \((\mach,\context)\) enjoys {\it internal confidentiality} with respect to \(\components\)
if, for any \(\nach\) that is a \(\components\)-variant of \(\mach\), we can take
\(|\sigma| \uparrow (\mach,\context) \hookrightarrow \obsT\) and
\(|\sigma| \uparrow (\nach,\context) \hookrightarrow \obsT'\) and have that
\(\obsT \simeq \obsT'\).

\definition The {\em corrupted set} \(\bar{\Diamond}(\mach,\mach',\nach,\nach')\)
is the set \((\Delta(\mach,\mach') \cup \Delta(\nach,\nach')) \cap \Delta(\mach',\nach')\).
This treats
\(\mach\) and \(\nach\) as the initial states and
\(\mach'\) and \(\nach'\) as their corresponding final states, and is the
set of elements that were corrupted between them.

\definition We define the relation \(conf(\mach,\nach)\) to hold on a pair of states
\((\mach',\context)\) and \((\nach',\context')\) when
\(\bar{\Diamond}(\mach,\nach,\mach',\nach')\) is irrelevant in \((\mach',\context)\).

\definition A state \((\mach,(V,\sigma))\) enjoys {\it return-time confidentiality}
if, for \(\components = \{\component \mid V ~ \component \in \{\sealed, \unsealed\}\}\)
and any \(\nach\) that is a \(\components\)-variant of \(\mach\), we have:
\[(\mach,(V,\sigma)) ~ (|\sigma| \Uparrow \mathit{conf}(\mach,\nach) ~ (\nach,(V,\sigma))\]

\definition A system enjoys {\it caller confidentiality} if, whenever
\((\mach,\context) \stepstounder{\overline{\psi}} (\mach',\context',\obs)\)
with \(\mathbf{call} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \psi\),
%or \(\mathbf{tailcall} ~ \addr ~ \overline{\reg} \in \psi\),
\((\mach',\context')\) enjoys both internal and return-time confidentiality.


\subsection{Callee Confidentiality}

\definition We define \(\mathit{cconf}(\mach,V)\), to hold on a state \((\mach',\context')\) when
\(\{\component \mid V ~ \component = \unsealed\} \cap \Delta(\mach,\mach')\)
is irrelevant in \((\mach', \context')\).

\definition A system enjoys {\it callee confidentiality} if, whenever
\((\mach,\context) \stepstounder{\overline{\psi}} (\mach',(V,\sigma),\obs)\)
with \(\mathbf{call} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \psi\),
% or \(\mathbf{tailcall} ~ \addr ~ \overline{\reg} ~ \overline{sa} \in \psi\),
we have:
\[(|\sigma| \uparrow \mathit{cconf}(\mach',V)) ~ (\mach',\context')\)

\section{Shared Memory}

%We can extend our basic machine to support public memory allocations that can
%be accessed by anyone until they are deallocated on their owner's return,
%using the \(\mathbf{alloc\_pub}\) operation. This behaves identically to
%\(\mathbf{alloc\_priv}\) except that it marks the locations \(\public\).
%For example, consider the following code:

%{\tt
%  void g() \{

%  ~ ~ int x[3];

%  ~ ~ int y;

%  ~ ~ h(\&x);
    
%  \}
%}

%Here we have allocated several arrays and variables; {\tt y}
%will behave as normal with \(\mathit{alloc\_priv}\), but {\tt x} has its
%address taken and passed to another function. That function might do anything at
%all to {\tt x}, and we have to decide whether it can be said to violate stack
%safety. A reasonable default is to assume that any address-taken local should be
%public, while {\tt y} is still private.

%In the likely event that the space for the return address, {\tt x}, and {\tt y}
%are all allocated in a single step, that step needs multiple labels to capture
%that they are not all private. Its label would instead be:
%\[\begin{split}
%& \psi = [\mathbf{alloc\_priv} ~ (20,4); \\
%  & \mathbf{alloc\_pub} ~ (16,12) ; \mathbf{alloc\_priv} ~ (4,4)]
%\end{split}\]

%Allocating public objects affects the call stack very similarly to private ones,
%but instead of treating them as \(\sealed\) they become \(\public\). This makes these
%regions of memory able to transmit information between caller and callee without
%violating the stack safety properties. Note that there is no way to seal a location
%once it has become \(\public\); only when {\tt g} returns will the memory allocated
%to {\tt x} be available for private use.

%\judgmentbr{\(b = \mach[\SP] - \mathit{off}\)}
%           {\(V' = V \llbracket \addr \mapsto \public |
%             b \leq a < b+\mathit{sz} \land V ~ \addr = \unsealed \rrbracket\)}
%           {\(Pop ~ (\mathbf{alloc\_pub} ~ \mathit{off, sz}) ~ (V,\sigma) ~ \mach \triangleq
%             (V',\sigma)\)}

\subsection{Stack Arguments}

So far, we have not dealt with arguments spilled onto the stack, variadic arguments,
or arguments that are passed by reference without their addresses being taken
explicitly (i.e., structs and arrays passed directly.) Consider the simplest case,
where a function has more than 8 arguments and therefore must (in RISC-V) spill one
to the stack.

{\tt
  void main() \{

  ~ f(1,2,3,4,5,6,7,8,9);

  ~ g(42);
  
  \}
}

\vspace{\abovedisplayskip}

\begin{tabular}{r l | l}
  \labeledrow{0:}{addi sp,sp,-12}{\(\mathbf{alloc} ~ (-12,12)\)}
  \labeledrow{4:}{sd ra,4(sp)}{}
  \labeledrow{8:}{li a5,9}{}
  \labeledrow{12:}{sd a5,0(sp)}{}
  \labeledrow{16:}{li a7,8}{}
  \dots \\
  \labeledrow{48:}{jal 100,ra}{\(\mathbf{call} ~ \{\mathtt{a0-a7}\} ~ \{(\SP,0,4)\}\)}
  \labeledrow{52:}{li a0,42}{}
  \labeledrow{56:}{jal 200,ra}{\(\mathbf{call} ~ \{\mathtt{a0}\} ~ \emplist\)}
  \dots \\
\end{tabular}

Under a typical calling convention, the callee expects the ninth argument to
be passed to it at bottom of the caller's frame, so the caller allocates extra
space for it. Then the call to {\tt f} identifies that we expect a range of four
bytes from the stack pointer to be passed as an object instead of sealed.

So we must refine our call operation to make use of the information that we have about
which memory contain arguments, \(\overline{sa}\). \(\overline{sa}\) is a set of
triples of a register, an offset from the value of that register, and a size.
We first define the helpful set \(\mathit{passed} ~ \overline{sa} ~ \mach\),
then extend the call operation to keep all objects in \(\mathit{passed}\) marked
as \(\object\) and seal everything else.
%
\[\mathit{passed} ~ \overline{sa} ~ \mach \triangleq
\bigcup_{(\reg,b,o) \in \overline{sa}} \{\mach[\reg]+i | b \leq i < b+o\}\]
%
\judgmentbrbr[]
             {\(\psi = \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa}\)}
             {\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
               \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
             {\(V'' = V'\llbracket \addr \mapsto \sealed | V' ~ \addr = \object \land \addr \not \in (\mathit{passed} ~ \overline{sa} ~ \mach) \rrbracket\)}
             {\(Op ~ \mach ~ \psi ~ (V,\sigma) =
               (V',V::\sigma)\)}

The remaining definitions are unchanged. Now a stack-spilled argument can be allocated
and remain accessible in the callee, while its memory is sealed for calls that do not
use that memory as an argument.

This same structure works for pass-by-reference arguments: we know from the type of
the called function which register it expects to hold the address of a given object,
and the size of that object. In this case the offset will always be 0.

When the callee in such a scenario makes a call of its own, the objects passed to it
will in turn be sealed---unless it passes them on, which can happen with pass-by-reference
arguments!

\subsection{Provenance, Capabilities, and Protecting Objects}

What if we want to express a finer-grained notion of safety, in which
such objects are protected unless the function that owns them intentionally
passes a pointer to them?

In order to express such a property, we need our machine to carry some notion
of {\it pointer provenance}---a distinction between a pointer that is intended to
point to a given object, and non-pointer integers as well as pointers to other objects.
Then \(\mathit{passed}\) should recursively identify all objects that
are reachable transitively from valid pointers in registers and accessible memory;
these keep their \(\object\) status, while those that are not reachable become \(\sealed\).
         
\bibliographystyle{IEEEtran}
\bibliography{bcp.bib,local.bib}

\end{document}
