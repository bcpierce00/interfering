%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[10pt,conference]{ieeetran}%\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{amsmath,amsthm,amssymb}

\usepackage[table]{xcolor}

\usepackage{threeparttable}

\usepackage{wasysym}

\usepackage{listings}

\usepackage{tikz}

\usepackage{array,multirow}

\usepackage{stmaryrd}

\usepackage[noadjust]{cite}

\usepackage[multiple]{footmisc}

\usepackage[hyphens]{url}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\input{macros}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\begin{document}

%% Title information
\title{Formalizing Stack Safety as a Security Property}

\author{
  \IEEEauthorblockN{
    Sean Noble Anderson
  }
  \IEEEauthorblockA{
    Portland State University\\
    ander28@pdx.edu\\
  }
  \and
  \IEEEauthorblockN{
    Roberto Blanco
  }
  \IEEEauthorblockA{
    Max Planck Institute for Security and Privacy\\
    roberto.blanco@mpi-sp.org\\
  }
  \and
  \IEEEauthorblockN{
    Leonidas Lampropoulos
  }
  \IEEEauthorblockA{
    University of Maryland, College Park\\
    leonidas@umd.edu\\
  }
  \linebreakand
  \IEEEauthorblockN{
    Benjamin C. Pierce
  }
  \IEEEauthorblockA{
    University of Pennsylvania\\
    bcpierce@cis.upenn.edu\\
   }
  \and
  \IEEEauthorblockN{
    Andrew Tolmach
  }
  \IEEEauthorblockA{
    Portland State University\\
    tolmach@pdx.edu\\
  }
}

%% Keywords
%% comma separated list
\ifcameraready
\keywords{Stack Safety, Micro-Policies}  %% \keywords are mandatory in final camera-ready submission
\fi

\maketitle

\begin{abstract}

The term {\em stack safety} is used for a variety of compiler,
run-time, and hardware mechanisms for protecting stack memory. Unlike
``the heap,'' the ISA-level stack does not correspond to a single
high-level language concept: different compilers use it in different
ways to support procedural and functional abstraction mechanisms from
a wide range of languages.  This protean nature makes it difficult to
nail down what it means to correctly enforce stack safety.

We propose a formal characterization of stack safety using concepts
from language-based security. Rather than packaging all aspects of
stack safety into a monolithic property, we decompose it into an
integrity property and a confidentiality property for each of the
caller and the callee, plus a control-flow property---five properties
in all.
%
This formulation is motivated by a particular class of enforcement
mechanisms, the ``lazy'' stack safety micro-policies studied by
Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}, which permit
functions to write into one another's frames, but which taint the
changed locations so that the frame's owner cannot access it. No
existing characterization of stack safety captures this style of
safety. We capture it here by stating our properties in terms of the
observable behavior of the system.

% The stack plays a central role in realizing large number of
% high-level language features, some of which are typically excluded
% from discussions of stack safety.
Our properties go further than previous formal definitions of stack
safety, supporting caller- and callee-saved registers, arguments
passed on the stack, \ifexceptions tail-call elimination, and
exceptions.  \else and tail-call elimination. \fi
% \bcp{Out of all of
%   these, only exceptions are high-level language features!}
\proposecut{They are modular by design so that they can be further extended to
other features.}
%
We validate our properties by using them to distinguish between
correct and incorrect implementations of Roessler and DeHon's
micro-policies using property-based random testing. Our test harness
successfully identifies several broken variants, including Roessler
and DeHon's lazy policy; a repaired version of their policy does pass
our tests.

\end{abstract}

\newcommand{\paragraphx}[1]{\emph{#1.}}

\section{Introduction}
\label{sec:intro}

\subsection{The Call Stack and Its Security}

{\em Functions} in high-level languages, and related abstractions such
as subroutines, procedures, methods, etc., are units of computation
that call one another to modularly realize larger computations.
%
At a low level, each function activation manages its own data---local
variables, etc.---as well as information about the \emph{caller} to
which it must return.
%
The \emph{call stack} is the fundamental data structure used to
implement functions, aided by an ABI that defines how registers are shared
between activations.

From a security perspective, attacks on the call stack
are attacks on the function abstraction itself.
%
Indeed, the stack is an ancient~\cite{phrack96:smashingthestack} and
perennial~\cite{mitre-cwe,DBLP:conf/raid/VeendCB12,
  DBLP:conf/sp/SzekeresPWS13,
  DBLP:conf/sp/HuSACSL16,msrc-bluehat,chromium-security}
target for low-level attacks, sometimes involving control-flow
hijacking via corrupting the return address, sometimes memory corruption
more generally.
%
%The most recent release of the authoritative CWE Top 25
%Most Dangerous Software Weaknesses~\cite{
%list\footnote{\url{https://cwe.mitre.org/top25/archive/2022/2022_cwe_top25.html}}
%shows that general classes of vulnerabilities closely related to the stack
%% \rb{(though not exclusively)}
%consistently rank at and near the top of the list, e.g., \#1 Out-of-bounds Write, \#5 Out-of-bounds Read, and \#7 Use After Free.
%
%Moreover, many other types of faults involve some kind of control or data flow
%using data stored in the stack
The variety in attacks on the stack is mirrored in the range of
software and hardware protections that aim to prevent them,
%
including stack canaries~\cite{Cowan+98},
bounds checking~\cite{NagarakatteZMZ09,NagarakatteZMZ10,DeviettiBMZ08},
split stacks~\cite{Kuznetsov+14},
shadow stacks~\cite{Dang+15,Shanbhogue+19},
capabilities~\cite{Woodruff+14,Chisnall+15,SkorstengaardLocal,SkorstengaardSTKJFP,Georges22:TempsDesCerises},
and hardware tagging~\cite{DBLP:conf/sp/RoesslerD18,Gollapudi+23}.

Enforcement mechanisms can be brittle, successfully eliminating one
attack while leaving room for others. To avoid an endless game of
whack-a-mole, we seek formal properties of safe behavior that can be
proven or at least rigorously tested. Such properties can be used as
the specification against which enforcement can be validated; even
enforcement mechanisms that do not fulfill a property benefit from the
ability to articulate why and when they may fail.

Of the mechanisms listed above, many are fundamentally ill suited for
offering formal guarantees: may impede attackers, but do not provide
universal protection. Shadow stacks, for instance, aim to ``restrict
the flexibility available in creating gadget chains''
\cite{Shanbhogue+19}, not to categorically rule out attacks. Other
mechanisms, such as Softbound\cite{NagarakatteZMZ09} and Code-pointer
integrity\cite{Kuznetsov+14}, do aim for stronger guarantees, but even
these do not make formal promises.  Indeed, to our knowledge, the sole
line of work making a formal claim to protect stack safety is the
secure calling conventions of Skorstengaard et
al.~\cite{SkorstengaardSTKJFP} and Georges et
al.~\cite{Georges22:TempsDesCerises}.

Other mechanisms besides these should also be amenable to strong
formal guarantees.  In particular, Roessler and DeHon
\cite{DBLP:conf/sp/RoesslerD18} have presented an array of tag-based
micro-policies~\cite{pump_oakland2015} for stack safety that offer
strong guarantees.  Their most realistic micro-policy, called Lazy
Tagging and Clearing (\(LTC\)), makes an interesting performance
trade-off: it allows function activations to improperly write into one
another's stack frames, but ensures that the owner of the corrupted
memory cannot access it afterward.

Under this policy, one function activation {\em can} corrupt another's
memory---just not in ways that do not affect observable behavior.
Therefore, \(LTC\) would not fulfill Georges et al.'s property
(appropriately adapted to the tagged setting).  But \(LTC\) does
arguably enforce stack safety.  A looser, more observational
definition of stack safety is needed to fit this situation.

We propose here a formal characterization of stack safety, based on
the intuition of protecting function activations from each other.
\bcp{Aren't other formal definitions based on the same intuition?}
\sna{Two differences we're trying to get across. First, protecting
  functions vs. protecting stack frames: we bring registers into
  the story because it isn't just about the stack as a data structure.
  Second: describing security in terms of functions' high-level
  security goals (f is trying to extract a secret) rather than
  describing operationally what they can and can't do (as the CHERI
  folks do.)}
\bcp{Both useful!  But neither comes across, at least not very clearly.}
We use the tools of language-based
security~\cite{sabelfeld2003language}, decomposing stack safety into a family of
properties describing
the {\em integrity} and {\em confidentiality} of the callerâ€™s local state
and the callee's behavior during the callee's execution, plus the
{\em well-bracketed control flow} (\(\wbcf\)) property articulated by
Skorstengaard et al.~\cite{SkorstengaardSTKJFP}.
While our properties are motivated by the desire to specify \(LTC\) precisely,
they are stated abstractly in hopes that they may also apply
to other enforcement mechanisms.

We find, via property-based random testing, that \(LTC\) is flawed in
a way that undermines both integrity and confidentiality; after
correcting this flaw, the repaired \(LTC\) satisfies all our
properties.  Moreover, we show that our properties can be extended to
describe calling conventions with features such as callee-save
registers, argument-passing on the stack, and tail-call
elimination. We modify \(LTC\) to protect these features and apply
random testing to validate this extended protection mechanism against
the extended properties.

\subsection {Towards Formal Properties for Stack Safety}

\apt{Too much real estate to sell the generality, flexibility, etc. of our approach.
  Again, we are not providing evidence that these attributes are true, although we can
  and should conjecture claim them in passing.}

\bcp{Agreed.  I think in particular the first paragraph can just go
  (I've deleted it).
  But this is not enough.  Also, the section is trying to do at least
  two things---talk about earlier work and how we compare, and talk
  about why we believe a flexible approach is good.  The whole
  discussion is a bit wooly and confusing.}

\sna{Should probably move the discussion of the CHERI work to related work,
  and focus on justifying our approach.}
\bcp{+1}

% Security properties should be motivated by security requirements
% and flexible enough to support various sets of security requirements depending on the
% goals of the system that they validate.
% Following this reasoning, our vision of stack safety is as a collection of properties
% describing the security needs of a system with regard to its functional abstractions.
% While the stack still plays a key role, our stack safety is about functions. The most
% dramatic consequence of this is that we treat registers as part of a function activation's
% state, and hold them to the same protection requirements as stack memory.

Flexibility is important for talking rigorously about stack safety.
%
The reason is that, while many security properties can be described at
the level of a high-level programming language and translated to a
target machine by a secure compiler, stack safety cannot be defined in
this way, since ``the stack'' is completely transparent in the
definitions of most source languages.\footnote{Contrast Azevedo de
  Amorim et al.'s work on heap safety
  \cite{DBLP:conf/post/AmorimHP18}: the concept of the heap figures
  directly in high-level language semantics and its security is
  therefore amenable to a high-level treatment.}
%
But neither is it satisfying as an entirely low-level property;
indeed, at the lowest level, the specification of a ``well-behaved
stack'' is almost vacuous. The ISA is not concerned with such
questions as whether a caller's frame should be readable or writable
to its callee: those are the purview of high-level languages built
atop the hardware stack.

\bcp{There's a broken transition here: the paragraph below is not an
  instance of the paragraph above.  Indeed, I don't see how they
  connect at all.}

For instance, the stack-safety specification from Georges et al.
\cite{Georges22:TempsDesCerises} supposes a machine with only straightforward call-and-return
control flow, and therefore defines {\em Well-bracketed Control Flow} (\(\wbcf\)) under the assumption that
a callee should always return to its caller. But, in the presence of tail-call elimination, a
return from a callee to a distant ancestor is perfectly reasonable. Similarly, the preceding work by
Skorstengaard et al. \cite{SkorstengaardSTKJFP} made no mention of confidentiality, even though
their enforcement mechanism did protect the confidentiality of the caller (though not the callee).

These two cases represent two dimensions in which a stack safety definition requires
flexibility: what constitutes safe behavior differs between systems that support different
features, and different contexts have different top-level security requirements.

We are not attempting to present a universal definition of stack safety for all systems,
but we do construct our definition with these dimensions of flexibility in mind.
Our model is decomposed into:
(1) a lightweight model of the system and its security-relevant features, from which we
derive the integrity, confidentiality, and control-flow requirements of each call and
return, (2) the application of that model to a particular target system, and (3)
the formal criteria for the integrity, confidentiality, and control flow
requirements.

We show that we can extend our system to new features by first introducing a simple
example model, and then moving to a more realistic model with multiple new features:
callee-save registers, argument passing on the stack, and tail-call elimination, simply
by refining a few definitions in (1). This becomes the model that we validate.
In \cref{app:ptr}, we extend the model further with arbitrary sharing of memory-safe
pointers. Our criteria in (3) cover a total of five core requirements:
\(\wbcf\), caller integrity (\(\clri\)), caller confidentiality (\(\clrc\)),
\(\clei\) subsumes \(\clrc\) and \(\clec\) subsumes
\(\clri\), but otherwise the properties are orthogonal and can be taken piecemeal
according to the needs of a system.

In particular, many enforcement techniques focus purely on
well-bracketed control flow. For instance, stack canaries aim to prevent certain attacks on the return
address, and shadow stacks with protection (e.g. Return Address Defender~\cite{Chiueh2001RAD})
to enforce it completely. Others combine this protection with some degree of memory protection,
chiefly focusing on integrity. Interestingly, Skorstengaard et al.~\cite{SkorstengaardSTKJFP}
describe their ``local state encapsulation'' in terms of integrity, but it is equivalent
to the combination of \(\clri\) and \(\clrc\). In fact, the follow-up work by
Georges et al. \cite{Georges22:TempsDesCerises} was inspired by the properties presented in this paper to extend their formal discussion to include confidentiality.
When checking if a preliminary version of these properties applied to
the Cerise calling convention, they noted that the Skostengaard et al. calling convention
did not enforce callee confidentiality, and made sure that their version would
\cite{Georges22:personalcommunication}, in addition to building it into their formalism.
This demonstrates the benefit of our choice to explicitly state properties in security
terms: specifying security mechanisms is hard, and when the specification takes the
form of a ``correct by construction'' machine, it is easy to neglect a non-obvious
security requirement.

%These are defined
%formally in \cref{sec:props}. . We further define two
%important combinations of properties: full stack safety \(\textnormal{\sc FSS}\),
%and caller safety \(\textnormal{\sc ClrS}\). Of these, most existing concepts of
%stack safety correspond to either \(\wbcf\) or \(\textnormal{\sc ClrS}\), which are
%concerned with the basic guarantees that the caller recieves during the call, while
%\(\textnormal{\sc FSS}\) is a natural extension that enforces a higher-level concept
%of a correct interface between caller and callee. \sna{Blah. Needs work.}

Flexibility aside, we show that our properties are useful for their purpose:
distinguishing correct enforcement from incorrect. We use
QuickChick~\cite{Denes:VSL2014,Pierce:SF4}, a property-based testing
tool for Coq, to generate random programs and check that the
stack safety {\em micro-policies} of Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}
correctly detect the ones that attempt to violate one of our properties.

%Furthermore, we
%check that the testing framework is able to generate counterexamples
%that violate our properties but are \emph{not} halted by incorrect
%variants of the enforcement mechanisms---including the original \(LTC\)!
% SNA: we talk about this in the testing section, and it's a "best practice" for
% PBT, but not really a contribution in and of itself

\subsection{Contributions}

In sum, we offer the following contributions:

\begin{itemize}
\item We give a novel characterization of stack safety as a collection
of properties: confidentiality and integrity for callee and caller,
plus well-bracketed control-flow. Different subsets of these
properties characterize the security guarantees offered by mechanisms
that support only some aspects of stack safety\bcp{where do we show
  this?}. \sna{Nowhere clearly enough. We need to make it clear in section 6
  that Nick and Andre are included there---only aiming at caller-side,
  not callee-side. The discussion of CHERI work should touch on this as well.}
The properties they are parameterized over a notion of
external observation, allowing them to characterize lazy enforcement
mechanisms.
\item We instantiate\bcp{extend??} these definitions in a realistic
setting with features such as\bcp{``such as''? are there others?}
argument passing on the stack, callee-saves registers, and tail-call
elimination.\bcp{How about: We extend these core definitions to
  describe a realistic setting with argument passing on the stack,
  callee-saves registers, and tail-call elimination.}  Our model is
modular enough that handling these features is straightforward.
\bcp{Throughout, the way we talk about the ``core model'' and its
  intantiation / extension / enrichment with realistic features feels
  quite weasily.}
\item We validate a tag-based enforcement mechanism, \emph{Lazy
  Tagging and Clearing}, via property-based random testing, find that
it falls short, and propose and validate a fix.
\end{itemize}

In the next section, we give a brief overview of the structure of
properties.  In \cref{sec:example}, we walk through a function call in
a simple example machine and discuss how each of our properties
applies to it, informally. In the process we motivate the properties
from a security perspective.
%
In \cref{sec:formal} we formalize the machine model,
its {\em security semantics}, and the stack safety properties built on these.
In \Cref{sec:enforcement} we describe the micro-policies that we test,
in \cref{sec:testing} the testing framework itself, and in
\cref{sec:relwork,sec:future} related and future work.

The accompanying artifact contains formal definitions (in Coq) of our
properties, plus our testing framework.  It does not include proofs:
we use Coq primarily for the QuickChick testing library and for
ensuring that our definitions are unambiguous.  Formal proofs are left
as future work.

\subsection{Threat Model and Limitations}
\apt{This could probably go better at end of next section.}

When our properties are used to evaluate a system, the threat model
will depend on the details of that system. However, there are some
constraints that our design puts on any system. In particular, we must
trust that the method of distinguishing security-relevant operations
is accurate.  For example, if the enforcement mnechanism involves
labels placed on code by a compiler, we must trust that the compiler
placed those labels correctly. For instance, if a compiled function
call is not marked as such, then the caller's data might not be
protected from the callee.  Conversely, marking too many operations as
calls will simply cause otherwise safe programs to be rejected.

\bcp{Still working here:}

We do not assume that low-level code adheres to any fixed calling
convention or implements any particular source-language constructs.
Indeed, if the source
language is C, then high-level programs might contain undefined
behavior, in which case they might be compiled to arbitrary machine
code. A given enforcement
mechanism or target architecture might place additional constraints, particularly on the behavior of
call and return sequences. For instance, extant implementations tend to assume
implicitly that callee-saved registers have their values maintained by whichever compiler
generated their code. Our properties explicitly state this as a requirement,
which could be enforced by a micro-policy, a well-behaved compiler, or other enforcement technique.
Roessler and DeHon include the compiler in their trusted computing
base, so our properties
are stronger than necessary to characterize their micro-policies.

In general, it is impossible to distinguish buggy machine code from an
attacker.  In our examples we will identify one function or another as
an attacker, but we do not require any static division between trusted
and untrusted code, and we aim to protect even buggy code.

This is a strong threat model, but it does omit some important aspects
of stack safety in real systems: in particular, it does not address
concurrency.  Hardware and timing attacks are also out of scope.
%

%We consider a simple attacker, and discuss the harm that it can do to
%its caller. This perspective is essential to understanding how lazy enforcement can
%be safe: even if an attacker can overwrite some memory, if it cannot accomplish
%subvert the observable behavior of the caller, the system is safe.

\section{Framework and Assumptions}
\label{sec:ideas}

Stack safety properties need to describe the behavior of machine code. but they naturally
talk about function activations and stack contents---abstractions that
are (typically) not visible at machine level. To bridge this gap,
our properties are defined in terms of a {\em security semantics} layered on top of
the standard execution semantics of the machine.  This semantics identifies certain
state transitions of the machine as {\em security-relevant operations}, which update
a notional {\em security context}.  This context consists of
an (abstract) stack of function activations, each associated with a {\em view}
that maps each element of the machine state (memory location or register)
to a {\em security class} (active, sealed, etc.) which specifies how the activation
can access the element.
The action of the security-relevant operations on the context is defined by a set
of rules that depend on how the machine code makes use of the stack and registers
to implement the function abstraction.

We define stack safety properties in
terms of traces of {\em observable events} rather than internal machine states.
This is particularly important for
specifying lazy enforcement, which works because the deferred check catches problems
before they can affect the trace.
Events might include certain function calls (i.e., system calls performing I/O)
or writes to special addresses representing memory-mapped regions, etc.
%The nature of events is a parameter of the framework, and can be specialized
%to any notion of observable behavior.

Finally, we define our high-level security properties---integrity, confidentiality,
and well-bracketed control flow---in terms of the security classes of state
elements. These take the form of predicates on states and contexts that use variants,
assertions about future return states, and comparison of event traces to capture the
desired behavior without reference to internal implementation details.
We introduce these properties by example in \cref{sec:example}. In the remainder
of this section we describe the underlying framework in more detail.

Our security semantics has similarities to the overlay semantics proposed
by Skorstengaard et al.~\cite{SkorstengaardSTKJFP}, but it does not restrict
the behavior of the underlying machine in any way. Rather, it tracks additional context
about the history of security-relevant operations, which inform the criterion a machine
must satisfy in order to correctly obey the properties.\apt{Sharpen this comparison?}

%The security principals in our account of stack safety are function
%activations: the context tracks, for the active function and all pending
%functions, their {\em view} of the security state of the system.

\paragraph*{Machine model}
We assume a conventional processor ISA (e.g. RISC-V, x86-64, etc.), with registers including a program counter
and stack pointer, and a standard ABI.
We make no particular assumptions about the provenance of the machine code; in particular,
we do not assume the use of any particular compiler.
The machine may possibly be enhanced with security
enforcement mechanisms such as hardware tags~\cite{pump_hasp2014,Gollapudi+23} or capabilities~\cite{Woodruff+14}. If so,
we assume that the behavior of these mechanisms is incorporated into the basic
step semantics of the machine, but we distinguish the \emph{payload} part of
each state element---the part relevant to the behavior of the basic machine---from
any additional enforcement information (such as a tag).

\paragraph*{Security Semantics and Property Structure}
A security semantics extends a machine
with additional context about the identity of current and pending
functions (which act as security principals) and about the registers and memory they require
to be secure. This added context is purely notional;
it does not affect the real machine. The security context
evolves dynamically through the execution of security-relevant operations,
which include events like calls, returns, and frame manipulations.
Our security properties are phrased in terms of this context, often as predicates
on future states, e.g. of the form ``when control returns to the current function...'',
or as relations on traces of future execution (hyper-properties).

The security-relevant operations typically correspond to underling machine instructions,
but the correspondence may not be obvious just by inspecting the machine code.
For example, in the tagged RISC-V machine we use in our examples and tests,
calls and returns are conventionally performed by {\tt jal} (``jump-and-link'')
and {\tt jalr} (``jump-and-link-register'') instructions, respectively, but these
instructions might also be used for other things. An instantiation of an operation
might require some external mechanism for distinguishing which instructions are
security-relevant. The most obvious technique, which we use in our examples, is to add labels to
selected instructions, but the theory does not depend on this choice.
Whatever the mechanism for labeling transitions, we end up with an annotated version of the
machine transition function \(\mach \xrightarrow{\bar{\psi}, \obs} \mach'\), where \(\obs\) is
an event and \(\bar{\psi}\) is a list of security-relevant operations.

We then lift this into a transition between pairs of machine states and contexts,
by applying a set of operation-specific rules to transform each state and context to
a new context, in parallel with the ordinary transition on states.
The most important rules describe call and return operations.
A call pushes a new activation view onto the context stack and changes the class of the
caller's data to protect it from the new callee; a return reverses these steps.
Other operations can serve to signal how parts of the stack frame are being used to store
or share data, and their corresponding rules alter the classes of different
state elements accordingly.

Exactly which operations and rules are needed depends on
what code features we wish to support.
The set of security-relevant operations (\(\Psi\)) covered in this paper is given in
\cref{tab:psi}. Those that we explore in detail in \cref{sec:example,sec:formal}
are shown in white: calls, returns, and the allocation and deallocation of local memory.
In \cref{sec:extensions}, we cover additional operations and and parameters
to support various forms of memory sharing \ifexceptions , exceptions, \fi
and tail-call elimination; these are shown in light gray. These are also
the operations we test in \cref{sec:testing}.
We give a proof-of-concept formalization of a more sophisticated, heap-like sharing model
based on pointer provenance in \cref{app:ptr}, represented by the dark gray
operations, but we do not test this.

\newcommand{\example}{\rowcolor{black!0}}
\newcommand{\testing}{\rowcolor{black!10}}
\newcommand{\theory}{\rowcolor{black!25}}

\begin{table}
\begin{center}
  \begin{tabular}{| l | l |}
    \hline
    \(\psi \in \Psi\) & Parameters \\
    \hline
    \example \(\mathbf{call}\) & target address, argument registers \\
    \testing & stack arguments (base, offset \& size) \\
    \example \(\mathbf{return}\) & \\
    \example \(\mathbf{alloc}\) & offset \& size \\
    \testing & public flag \\
    \example \(\mathbf{dealloc}\) & offset \& size \\
    \testing \(\mathbf{tail call}\) & (same as for \(\mathbf{call}\)) \\
    \hline
    \multicolumn{2}{|c|}{{\it Pointer provenance operations}} \\
    \hline
    \theory \(\mathbf{promote}\) & register, offset \& size \\
    \theory \(\mathbf{propagate}\) & source register/address \\
    \theory & destination register/address \\
    \theory \(\mathbf{clear}\) & target register/address \\
    \hline
  \end{tabular}
\end{center}
\caption{Security-relevant operations and their parameters.}
  \label{tab:psi}
\end{table}

\paragraph*{Views and security classes}

The security context consists of a stack of views: functions that map
each state element to a {\it security class}. The security classes are
\(\public\), \(\unsealed\), \(\object\), and \(\sealed\).

State elements that are outside of the stack---general-purpose memory used for
globals and the heap, as well as the code region and globally-relevant
registers---are always labeled \(\public\). We place security requirements on some
\(\public\) elements for purposes of \(\wbcf\), and a given enforcement mechanism
might restrict their access (e.g., by rendering code immutable) but for integrity
and confidentiality purposes they are considered accessible at all times.

For a newly active function, every element that is available for use but uninitialized
is seen as \(\unsealed\). From the perspective of the caller, the callee has no obligations
regarding its use of \(\unsealed\) elements.

Arguments are seen as \(\object\), meaning that their contents may be safely used.
When the active function allocates memory, that memory will also be \(\object\).
Then, on a call, \(\object\) elements that are not being used to communicate with
the new callee will become \(\sealed\)---reserved for an inactive principal,
and expected to be unchanged when it becomes active again.

%% \paragraph*{Variants and Non-interference}

%% To characterize situations in which values should be kept secret or prevented
%% from influencing future execution, we borrow the idea of \emph{variant} states
%% from the theory of non-interference. Non-interference is a way of expressing
%% the flow of information through the system. If we begin with a state \(\mach\)
%% and modify it to \(\nach = \mach[\component \mapsto v]\) for arbitrary \(v\),
%% then execute both \(\mach\) and \(\nach\), any differences in the outputs they
%% produce leaks information about \(\component\).

\paragraph*{Instantiating the Framework}

The property framework has many parameters to support broad applicability.
Conceptually, the following steps are needed to instantiate the framework to a specific machine
and code style: (i) define the base machine semantics; (ii) distinguish payload
from enforcement parts (if any) of the machine state; (iii) identify the set of
security-relevant operations and rules required by the code style; (iv) determine
how to label (or otherwise associate) machine instruction with security-relevant
operations as appropriate; (v) specify the form of observable events.

In \cref{sec:example} we give such an instantiation, and use it to explain the
security properties.  But we stress that the property definitions themselves
are \emph{independent} of the parameter instantiation.

\apt{Suggest moving threat model to here.}

\section{Properties by Example}
\label{sec:example}

In this section we introduce our security property definitions by means
of small code examples, using a simplified set of security-relevant operations for
calls, returns, and private allocations.
\Cref{fig:main} gives C code and possible corresponding compiled 64-bit RISC-V code
for a function {\tt main}, which
takes an argument {\tt secret} and initializes a local variable {\tt sensitive} to contain
potentially sensitive data.
Then {\tt main} calls another function {\tt f},
and afterward performs a test on {\tt sensitive} to decide whether
to output {\tt secret}.  Since {\tt sensitive} is initialized to 0,
the test should fail, and {\tt main} should instead output the return value of {\tt f}.
Output is performed by writing to the special global {\tt out},
and we assume that such writes are the only observable events in the system.

The C code is compiled using the standard RISC-V calling conventions~\cite{RISC-V-CC}.
In particular, the first function argument and the function
return value are both passed in {\tt a0}.
Memory is byte-addressed and the stack grows towards
lower addresses. We assume that {\tt main} begins at address 0 and its callee {\tt f} at address 100.

\begin{figure}
  \newcommand{\figonebox}[1][]{\genbox{30px}{lightgray}{#1}}
  \begin{subfigure}{\columnwidth}
    {\tt
      volatile int out;

      void main(int secret) \{

      ~ ~ int sensitive = 0;

      ~ ~ int res = f();

      ~ ~ if (sensitive == 42)

      ~ ~ ~ ~ out = secret;

      ~ ~ else

      ~ ~ ~ ~ out = res;

      \}}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \begin{tabular}{r l | l}
      \labeledrow{0:}{addi sp,sp,-20}{\(\mathbf{alloc} ~ (-20,20)\)}
      \labeledrow{4:}{sd ra,12(sp)}{}
      \labeledrow{8:}{sw a0,8(sp)}{}
      \labeledrow{12:}{sw zero,4(sp)}{}
      \labeledrow{16:}{jal f,ra}{\(\mathbf{call} ~ \emplist \)}
      \labeledrow{20:}{sw a0,0(sp)}{}
      \labeledrow{24:}{lw a4,4(sp)}{}
      \labeledrow{28:}{li a5,42}{}
      \labeledrow{32:}{bne a4,a5,L1}{}
      \labeledrow{36:}{lw a0,8(sp)}{}
      \labeledrow{40:}{sw a0,out}{}
      \labeledrow{44:}{j L2:}{}
      \labeledrow{L1, 48:}{lw a0,0(sp)}{}
      \labeledrow{52:}{sw a0,out}{}
      \labeledrow{L2, 56:}{ld ra,12(sp)}{}
      \labeledrow{60:}{addi sp,sp,20}{\(\mathbf{dealloc} ~ (0,20)\)}
      \labeledrow{64:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \center
    \vspace{\abovedisplayskip}
    \(\xleftarrow{\figonebox[\dots] \stackrel{\textsc{\normalsize sp}}{\figonebox[\tt res]}
    \stackrel{\textsc{\normalsize 4(sp)}}{\figonebox[\tt sens]}
    \stackrel{\textsc{\normalsize 8(sp)}}{\figonebox[\tt sec]}
    \stackrel{\textsc{\normalsize 12(sp)} \hfill}{\figonebox[\(\mathtt{ra_1}\)]\figonebox[\(\mathtt{ra_2}\)]}
    }\)
  \end{subfigure}

  \caption{Example: C and assembly code for {\tt main}, and layout of its stack frame; addresses increase to the right and the stack grows to the left.}
  \label{fig:main}
\end{figure}

We now consider how {\tt f} might misbehave and violate desirable
stack safety properties associated with {\tt main}. To put the violations in a
security framework, suppose that {\tt f} is actually an attacker seeking
to leak {\tt secret}. It might do so in a number of ways, shown as snippets of
assembly code in \cref{fig:f}.
%
Leakage is most obviously viewed as a violation of {\tt main}'s {\it confidentiality}.
In \cref{subfig:direct}, {\tt f} takes an offset from the stack
pointer, accesses {\tt secret}, and directly outputs it. But more
subtly, even if somehow prevented from outputting {\tt secret} directly, {\tt f}
can instead return that value so that {\tt main} stores it to {\tt out},
as in \cref{subfig:indirect}.
%
Beyond simply reading {\tt secret}, the attacker might overwrite {\tt sensitive}
with 42, guaranteeing that {\tt main} publishes its own secret unintentionally
(\cref{subfig:integrity}).
Attacks of this kind do not violate {\tt main}'s confidentiality, but
rather its {\it integrity}.
In \cref{subfig:WBCF}, the attacker arranges to return to the
wrong instruction, thereby bypassing the check and publishing {\tt secret} regardless,
violating the program's {\it well-bracketed control flow} (\(\wbcf\).)
%
In \cref{subfig:WBCF2}, a different attack violates \(\wbcf\), this time
by returning to the correct program counter but with the wrong stack pointer.%
\footnote{We pad the last two variants with {\tt nop}s just so that all the
snippets have the same length, which keeps the step numbering uniform in~\cref{fig:exec1}.}

\begin{figure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{lw a4,8(sp)}{}
      \labeledrow{104:}{sw a4,out}{}
      \labeledrow{108:}{li a0,1}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \caption{Leaking {\tt secret} directly}
    \label{subfig:direct}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{lw a4,8(sp)}{}
      \labeledrow{104:}{mov a4,a0}{}
      \labeledrow{108:}{sw zero,-4(sp)}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \caption{Leaking {\tt secret} indirectly}
    \label{subfig:indirect}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{li a5,42}{}
      \labeledrow{104:}{sw a5,4(sp)}{}
      \labeledrow{108:}{li a0,1}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking {\tt sensitive}}
    \label{subfig:integrity}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{addi ra,ra,16}{}
      \labeledrow{104:}{nop}{}
      \labeledrow{108:}{nop}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking control flow}
    \label{subfig:WBCF}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{addi sp,sp,8}{}
      \labeledrow{104:}{nop}{}
      \labeledrow{108:}{nop}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking stack pointer integrity}
    \label{subfig:WBCF2}
  \end{subfigure}

  \caption{Example: assembly code alternatives for {\tt f} as an attacker.}
  \label{fig:f}
\end{figure}

The security semantics for this program is based
on the security-relevant events noted in the right columns of \cref{fig:main,fig:f},
namely execution of instructions that allocate or deallocate space,
make a call, or make a return.

Our security semantics attach a security context to the machine state,
which consists of a view \(V\) and a stack \(\sigma\) of pending activations' views.
\Cref{fig:exec1} shows how the security context evolves over the first few
steps of the program.  (The formal details of the security semantics are described in
\cref{sec:formal}, and the context evolution rules are formalized in \cref{fig:basicops}.)
Execution begins at the start of {\tt main}, where the program counter (\(\PCname\)) is zero,
and with the stack pointer (\(\SP\)) at address 1000.
State transitions are numbered and labeled with a list of security operations, written
\(\downarrow \overline{\psi}\) between steps.

The initial view \(V_0\) maps all stack addresses below \(\SP\) to \(\unsealed\) and the remainder of
memory to \(\public\). The sole used argument register, {\tt a0}, is mapped to \(\object\);
other caller-save registers are mapped to \(\unsealed\) and callee-save registers to \(\sealed\).
Step 1 allocates a word each for {\tt secret}, {\tt sensitive}, and {\tt res}, as well
as two words for the return address. This has the
effect of marking those bytes \(\object\).
(We use \(V\llbracket\cdot\rrbracket\) to denote updates to \(V\).)

\begin{figure*}
  \begin{tabular}{|r|r||l|r}
    \cline{1-3}
    \(\PCname\) & \(\SP\) & Context &
    \multirow{3}{*}{\(\underbrace{\dots \freebox \freebox \freebox \freebox \freebox
        \freebox \freebox \freebox \freebox \freebox}_\unsealed
      \! \underbrace{\stackrel{\stackrel{\SP}{\downarrow}}{\pubbox} \!\! \pubbox \pubbox \dots}_\public
      ~ \stackrel{\mathtt{a0}}{\pubbox\pubbox} ~ \stackrel{\mathtt{a1}}{\freebox\freebox}
      ~ \stackrel{\mathtt{s0}}{\sealbox\sealbox}
      \)} \\
    \cline{1-3}
    0 & 1000 & \(V_0, \emplist\)
    \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{\(1 \Big\downarrow [\mathbf{alloc} ~ (-20,20)]\)}} & \\
    \multicolumn{3}{l}{} &
    \multirow{3}{*}{\(\underbrace{\dots \freebox \freebox \freebox \freebox \freebox}_\unsealed
      \! \underbrace{\stackrel{\stackrel{\SP}{\downarrow}}{\objbox} \!\! \objbox \objbox \objbox \objbox}_\object
      \! \underbrace{\pubbox \pubbox \pubbox \dots}_\public
      ~ \stackrel{\mathtt{a0}}{\pubbox\pubbox} ~ \stackrel{\mathtt{a1}}{\freebox\freebox}
      ~ \stackrel{\mathtt{s0}}{\sealbox\sealbox}
      \)}
    \\
    \cline{1-3}
    4 & 980 & \(V_1 = V_0 \llbracket 980..999 \mapsto \object\rrbracket, \emplist\) &
    \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{2-4 \(\Big\downarrow \emplist\)}} \\ \multicolumn{3}{l}{} \\
    \cline{1-3}
    16 & 980 & \(V_1, \emplist\) & \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{\(5 \Big\downarrow [\mathbf{call} ~ 100 ~ \emplist]\)}} & \\
    \multicolumn{3}{l}{} &
    \multirow{3}{*}{\(\underbrace{\dots \freebox \freebox \freebox \freebox \freebox}_\unsealed
      \! \underbrace{\stackrel{\stackrel{\SP}{\downarrow}}{\sealbox} \!\! \sealbox \sealbox \sealbox \sealbox}_\sealed
      \! \underbrace{\pubbox \pubbox \pubbox \dots}_\public
      ~ \stackrel{\mathtt{a0}}{\freebox\freebox} ~ \stackrel{\mathtt{a1}}{\freebox\freebox}
      ~ \stackrel{\mathtt{s0}}{\sealbox\sealbox}
      \)}
    \\
    \cline{1-3}
    100 & 980 & \(V_2 = V_1 \llbracket 980..999 \mapsto \sealed, \mathtt{a0} \mapsto \unsealed\rrbracket,[V_1]\) & \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{6-8 \(\Big\downarrow \emplist\)}} \\ \multicolumn{3}{l}{} \\
    \cline{1-3}
    112 & 980 & \(V_2,[V_1]\) \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{\(9 \Big\downarrow [\mathbf{return}]\)}} & \\
    \multicolumn{3}{l}{} & \multirow{3}{*}{\(\underbrace{\dots \freebox \freebox \freebox \freebox \freebox}_\unsealed
      \! \underbrace{\stackrel{\stackrel{\SP}{\downarrow}}{\objbox} \!\! \objbox \objbox \objbox \objbox}_\object
      \! \underbrace{\pubbox \pubbox \pubbox \dots}_\public
      ~ \stackrel{\mathtt{a0}}{\pubbox\pubbox} ~ \stackrel{\mathtt{a1}}{\freebox\freebox}
      ~ \stackrel{\mathtt{s0}}{\sealbox\sealbox}
      \)}
    \\
    \cline{1-3}
    20 & 980  & \(V_1, \emplist\) &
    \\
    \cline{1-3}
    \multicolumn{2}{l}{} \\
  \end{tabular}
  \caption{Execution of example up through the return from {\tt f}. In stack diagrams, addresses increase to the right, stack grows to the left, and boxes represent 4-byte words.}
\label{fig:exec1}
\end{figure*}
%
At step 5, the current principal's record is pushed onto the inactive list.
Its return target is the return address of the call,
and the stack pointer target is the stack pointer at the moment of call.
The callee's view is updated from the caller's such that all \(\object\) memory locations
become \(\sealed\). (For now we assume no sharing of memory between activations; data is
passed only through argument registers, which remain active. In the presence of memory
sharing, some memory would remain active, too.)
Function {\tt f} does not take any arguments; if it did, any registers containing them would be
mapped to \(\object\), while any non-argument, caller-saved
registers are mapped to \(\unsealed\). In the current example, only register {\tt a0} has a change in
security class. All callee-save registers remain \(\sealed\) for all calls.
Thus, if we varied the assembly code for the example slightly so that {\tt sensitive} was stored
in a callee-save register (e.g. {\tt s0}) rather than in memory, its security class would still be \(\sealed\)
at the entry to {\tt f}.
%
At step 9, {\tt f} returns, and the topmost inactive view, that of {\tt main}, is restored.

We now show how this security semantics can be used to define notions of confidentiality,
integrity, and correct control flow in such a way that many classes of
bad behavior, including the attacks in \cref{fig:f}, are
detected as security violations.

\paragraph*{Well-bracketed Control Flow}

To begin with, what if {\tt f} returns to an unexpected place (i.e. \(\PCname \neq 20\) or
\(\SP \neq 980\))? We consider this to violate \(\wbcf\). \(\wbcf\) is a relationship between
call steps and their corresponding return steps: just after the return, the program
counter should be at the next instruction following the call,
and the stack pointer should be the same as it was before the call.
Both of these are essential. In \cref{subfig:WBCF}, the attacker adds
16 to the return address and then returns, thus bypassing the {\tt if}-test in the code and outputting
{\tt secret}.
In \cref{subfig:WBCF2}, the attacker returns with \(\SP' = 988\) instead of the
correct \(\SP = 980\). In this scenario, given the layout of {\tt main}'s frame,
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
  \multicolumn{1}{r}{\(\SP \downarrow\)} &
  \multicolumn{2}{r}{\(\SP' \downarrow\)} \\
  \hline
  {\tt res} & {\tt sens} & {\tt sec} & \(\mbox{\tt ra}_1\) & \(\mbox{\tt ra}_2\) \\
  \hline
\end{tabular}
\end{center}

\vspace{\abovedisplayskip}

\noindent
{\tt main}'s attempt to read {\tt sensitive} will instead
read part of the return address, and its attempt to output
{\tt res} will instead output {\tt secret}.

Before the call, the program counter is 16 and the stack pointer is 980.
So we define a predicate on states that should hold just after the return:
\(\ret\ \mach \triangleq \mach[\PCname] = 20 \wedge \mach[\SP] = 980\).
%
We can identify the point just after the return (if a return occurs)
as the first state in which the pending call stack is smaller than it was
just after the call.
\(\wbcf\) requires that if \(\mach\) is the state at that point, then \(\ret ~ \mach\) holds.
%For nested calls, where the pending stack is initially larger, the same principle
%applies: \(\ret ~ \mach\) must hold the next time the pending stack is the same size or smaller.

% Even absent other
% kinds of data protection, the stack pointer {\it must} be restored
% for the program to behave predictably.

\paragraph*{Stack Integrity}

Like \(\wbcf\), stack integrity defines a condition at the call that must hold upon
return. This time the condition applies to all of the memory in the function's
frame. In \cref{fig:exec1} we see the lifecycle of an allocated frame:
upon allocation, the view labels it \(\object\), and when a call is made, it instead
becomes \(\sealed\). Intuitively, the integrity of {\tt main}
is preserved if, when control returns to it, any \(\sealed\) elements
are identical to when it made the call.
%
Again, we need to know when a caller has been returned to,
and we use the same mechanism of checking the depth of the call stack.
%
In the case of the call from {\tt main} to {\tt f}, the \(\sealed\) elements are the
addresses 980 through 999 and callee-saved registers such as
the stack pointer. Note that callee-saved registers often change
during the call---but if the caller accesses them after the call, it should find them
restored to their prior value.

While it would be simple to define integrity as ``all sealed elements retain their
values after the call,'' this would be stricter than necessary. Suppose that
a callee overwrites some data of its caller, but the caller never accesses that data
(or only does so after re-initializing it.) This would be harmless, with the callee
essentially using the caller's memory as scratch space, but the caller never seeing any change.
\rb{(Indeed, this even applies to callee-saved registers, if it is worth mentioning)}

For a set of elements \(\components\),
a pair of states \(\mach\) and \(\nach\) are {\em \(\components\)-variants} if
their values an only disagree on elements in \(\components\).
We say that the elements of \(\components\) are \emph{irrelevant}
in \(\mach\) if they can be replaced by arbitrary other values without changing the
observable behavior of the machine. All other elements are \emph{relevant}.

(This story is slightly over-simplified. If an enforcement mechanism maintains
additional state associated with elements, such as tags, we don't want that
state to vary; only the payloads should. Formal definitions of variants and
relevance that incorporate this wrinkle are given in \cref{sec:props}.)

We define \emph{caller integrity} (\(\clri\))  as the property that
every relevant element that is \(\sealed\) under the callee's view is restored
to its original value at the return point.

\newcommand{\figfourbox}[1][]{\genbox{20px}{red}{#1}}

\begin{figure}
  \centering
  \[
  \stackrel{\texttt{res}}{\figfourbox[0]}
  \stackrel{\texttt{sens}}{\figfourbox[0]}
  \stackrel{\texttt{sec}}{\figfourbox[5]}
  \stackrel{\texttt{ra}}{\figfourbox[0]\figfourbox[0]}\]
  %
  \[\big\Downarrow\]
  %
  \[
  \stackrel{\texttt{res}}{\figfourbox[0]}
  \stackrel{\texttt{sens}}{\genbox{20px}{red!50}{\bf 42}}
  \stackrel{\texttt{sec}}{\figfourbox[5]}
  \stackrel{\texttt{ra}}{\figfourbox[0]\figfourbox[0]}\]
  %
  \[\overbrace{
    \stackrel{\texttt{res}}{\figfourbox[0]}
    \stackrel{\texttt{sens}}{\genbox{20px}{\leftvariant}{\bf 42}}
    \stackrel{\texttt{sec}}{\figfourbox[5]}
    \stackrel{\texttt{ra}}{\figfourbox[0]\figfourbox[0]}
    \hspace{1cm}
    \stackrel{\texttt{res}}{\figfourbox[0]}
    \stackrel{\texttt{sens}}{\genbox{20px}{\rightvariant}{\bf 0}}
    \stackrel{\texttt{sec}}{\figfourbox[5]}
    \stackrel{\texttt{ra}}{\figfourbox[0]\figfourbox[0]}}\]
  \[\stackrel{\hookrightarrow \mathtt{out}}{\genbox{20px}{white}{5}} \hspace{0.5cm}
  %\raisebox{\height}{\not\approx}
  \hspace{0.5cm}
  \stackrel{\hookrightarrow \mathtt{out}}{\genbox{20px}{white}{1}}\]
  \caption{Integrity Violation}
  \label{fig:variant}
\end{figure}

In our example setting, the observation trace consists of the sequence
of values written to {\tt out}.
The example in \cref{subfig:integrity} modifies the value of {\tt sensitive},
which is \(\sealed\). \Cref{fig:variant} shows the state just after the call at step 5,
assuming that {\tt sec} is 5. Similar to \(\wbcf\), we define
\(\intProp\) as a predicate on states that holds if
all relevant sealed addresses in \(\mach\) are the same as after step 5.
We require that \(\intProp\) hold on the state following the matching return,
which is reached by step 9. Here {\tt sensitive} has obviously changed, but is it relevant?
Consider a variant state in which {\tt sensitive} has any other value, arbitrarily
choosing 43. As execution continues after the return
from the original state, it passes the {\tt if}-test on {\tt sensitive}, whereas the execution
from the variant does not, resulting in differing outputs. Therefore {\tt sensitive} \emph{is} relevant,
so \(\intProp\) does not hold, and integrity has indeed been violated.

\paragraph*{Caller Confidentiality}

We treat confidentiality as a form of non-interference as well: the confidentiality of a caller
means that its callee's behavior is dependent only on publicly visible data,
not the caller's private state. This also requires that the callee initialize
memory before reading it.
As we saw in the examples, we must consider both the observable events
that the callee produces during the call and the changes that the callee makes to the state that might
affect the caller after the callee returns.

Consider the state \(\mach\) after step 5, whose context is \((V_2,\sigma)\), with the attacker from
\cref{subfig:direct}. We take a variant state over the set of elements that are
\(\sealed\) in \(V_2\) (see \cref{fig:variant2}.)
            [TODO: fix figure to avoid varying the entire stack.]
If we take a trace of execution from each state until it returns,
the traces may differ, in this case outputting 5 (the original value of {\tt secret}) and
4 (its value in the variant) respectively. This is a violation of
{\it internal confidentiality} (formalized in \cref{tab:props}, line 3a).

\newcommand{\leftbox}[1][]{\genbox{20px}{\leftvariant}{#1}}
\newcommand{\rightbox}[1][]{\genbox{20px}{\rightvariant}{#1}}

\begin{figure}
    \centering
    \[
    \stackrel{\texttt{res}}{\figfourbox[0]}
    \stackrel{\texttt{sens}}{\figfourbox[0]}
    \stackrel{\texttt{sec}}{\figfourbox[5]}
    \stackrel{\texttt{ra}}{\figfourbox[0]\figfourbox[0]}\]
    %
    \[\overbrace{
    \stackrel{\texttt{res}}{\leftbox[0]}
    \stackrel{\texttt{sens}}{\leftbox[0]}
    \stackrel{\texttt{sec}}{\leftbox[5]}
    \stackrel{\texttt{ra}}{\leftbox[0]\leftbox[0]}
    \hspace{1cm}
    %
    \stackrel{\texttt{res}}{\rightbox[2]}
    \stackrel{\texttt{sens}}{\rightbox[3]}
    \stackrel{\texttt{sec}}{\rightbox[4]}
    \stackrel{\texttt{ra}}{\rightbox[5]\rightbox[6]}}
    \]
    \[%\raisebox{.5\height}{\Bigg\Downarrow} \hspace{1cm}
    \stackrel{\hookrightarrow \mathtt{out}}{\genbox{20px}{white}{5}} \hspace{0.5cm}
    %\raisebox{\height}{\not\approx}
    \hspace{0.5cm}
    \stackrel{\hookrightarrow \mathtt{out}}{\genbox{20px}{white}{4}} \hspace{1cm}
    %    \raisebox{.5\height}{\Bigg\Downarrow}
    \]
    \[
    \stackrel{\texttt{res}}{\leftbox[0]}
    \stackrel{\texttt{sens}}{\leftbox[0]}
    \stackrel{\texttt{sec}}{\leftbox[5]}
    \stackrel{\texttt{ra}}{\leftbox[0]\leftbox[0]}
    \hspace{1cm}
    %
    \stackrel{\texttt{res}}{\rightbox[2]}
    \stackrel{\texttt{sens}}{\rightbox[3]}
    \stackrel{\texttt{sec}}{\rightbox[4]}
    \stackrel{\texttt{ra}}{\rightbox[5]\rightbox[6]}
    \]

  \caption{Internal Confidentiality Violation}
  \label{fig:variant2}
\end{figure}

But, in \cref{subfig:indirect}, we also saw an attacker that exfiltrated the secret
by reading it and then returning it, in a context where the caller would output the returned
value. \Cref{fig:variant3} shows the behavior of the same variants under this attacker,
but in this case, there is no output during the call. Instead the value of {\tt secret} is
extracted and placed in {\tt a0}, the return value register. We wish to identify this as
a confidentiality violation, again by considering variants of the \(\sealed\)
elements in \(V_2\), but capturing the required property is subtle.

To illustrate the issues, note that {\tt f} has also stored a 0 below the stack pointer.
%
\begin{figure}
    \centering
    \[
    \stackrel{\texttt{res}}{\figfourbox[0]}
    \stackrel{\texttt{sens}}{\figfourbox[0]}
    \stackrel{\texttt{sec}}{\figfourbox[5]}
    \stackrel{\texttt{ra}}{\figfourbox[0]\figfourbox[0]}\]
    %
    \[\overbrace{
    \stackrel{\texttt{res}}{\leftbox[0]}
    \stackrel{\texttt{sens}}{\leftbox[0]}
    \stackrel{\texttt{sec}}{\leftbox[5]}
    \stackrel{\texttt{ra}}{\leftbox[0]\leftbox[0]}
    \hspace{1cm}
    %
    \stackrel{\texttt{res}}{\rightbox[2]}
    \stackrel{\texttt{sens}}{\rightbox[3]}
    \stackrel{\texttt{sec}}{\rightbox[4]}
    \stackrel{\texttt{ra}}{\rightbox[5]\rightbox[6]}}
    \]
    \[\large\Downarrow \hspace{1cm} \mathtt{a0} \leftbox[0] \hspace{1cm}
    \rightbox[7] \mathtt{a0} \hspace{1cm} \large\Downarrow\]
    \[
    \stackrel{\texttt{res}}{\leftbox[0]}
    \stackrel{\texttt{sens}}{\leftbox[0]}
    \stackrel{\texttt{sec}}{\leftbox[5]}
    \stackrel{\texttt{ra}}{\leftbox[0]\leftbox[0]}
    \hspace{1cm}
    %
    \stackrel{\texttt{res}}{\rightbox[2]}
    \stackrel{\texttt{sens}}{\rightbox[3]}
    \stackrel{\texttt{sec}}{\rightbox[4]}
    \stackrel{\texttt{ra}}{\rightbox[5]\rightbox[6]}
    \]
    \[\mathtt{a0} \leftbox[0] \hspace{1cm}
    \mathtt{a0} \rightbox[0]\]
  \caption{Return-time Confidentiality Violation}
  \label{fig:variant3}
\end{figure}
%
Now consider three elements: the address \(\SP - 4\), the address \(\SP + 12\),
and the register {\tt a0}. The execution from the right-hand state to its return
has changed the value at \(\SP - 4\), but that value matches that of the
left-hand variant at its return. Therefore, the change does not represent a leak.

The return states disagree on the value of \(\SP + 12\). But in neither
case has that value changed since the original variants. So the difference is inherited from
the original variation, and does not represent a leak either. We do not continue executing the
variant state after return, so these values will not cause the caller to behave differently.

But in the case of {\tt a0}, the value has changed during the call (in both the original
and the variant, although only one of these would be necessary), and its final value
differs between the variants.
Therefore, it must depend on a secret (in fact, the variable {\tt secret}).
Unless {\tt a0} happens to be irrelevant to the caller, this is a violation of what
we term {\it return-time confidentiality} (formalized in \cref{tab:props}, line 3b).

Structurally, return-time confidentiality resembles integrity, but now dealing with
variants. We begin with a state immediately following
a call, \(\mach\). We consider an arbitrary variant state,
\(\nach\), which may vary any element that is \(\sealed\) or \(\unsealed\),
i.e., any element that is not used legitimately to pass arguments. Caller confidentiality
therefore can be thought of as the callee's insensitivity to elements in its initial state
that are not part of the caller-callee interface.

We define a binary relation \(\confProp\) on pairs of states,
which holds on eventual return states \(\mach'\) and \(\nach'\)
if all relevant elements are {\em uncorrupted} relative to \(\mach\) and \(\nach\).
An element is {\em corrupted} if it differs between \(\mach'\) and \(\nach'\),
and it either changed between \(\mach\) and \(\mach'\) or between \(\nach\) and \(\nach'\).

Finally, we define \emph{caller confidentiality} (\(\clrc\)) as the
combination of internal and return-time confidentiality (\cref{tab:props}, line 3).

\paragraph*{The Callee's Perspective}

We presented our initial example from the perspective of the caller, but a callee
may also have privilege that its caller lacks, and which must be protected from the
caller. Consider a function that makes a privileged system call to obtain a secret key,
and uses that key to perform a specific task. An untrustworthy or erroneous caller might
attempt to read the key out of the callee's memory after return, or to influence the callee
to cause it to misuse the key itself!

Where the caller's confidentiality and integrity are concerned with protecting specific,
identifiable state---the caller's stack frame---their callee equivalents are concerned
with enforcing the expected interface between caller and callee. Communication between
the principals should occur only through the state elements that are designated for the
purpose: those labeled \(\public\) and \(\object\).

Applying this intuition using our framework, \emph{callee confidentiality} (\(\clec\))
turns out to resemble \(\clri\), extended to every element that is not marked \(\object\)
or \(\public\) at call-time. The callee's internal behavior is represented by those
elements that change over the course of its execution, and which are not part of the
interface with the caller. At return, those elements should become irrelevant to the
subsequent behavior of the caller.

Similarly, in \emph{callee integrity} (\(\clei\)), only elements marked \(\object\)
or \(\public\) at the call should influence the behavior of the callee. It may seem
odd to call this integrity, as the callee does not have a private state. But
an erroneous callee that performs a read-before-write within its stack
frame, or which uses a non-argument register without initializing it, is vulnerable
to its caller seeding those elements with values that will change its behavior.
The fact that well-behaved callees have integrity by definition is probably why
callee integrity is not typically discussed.

\section{Formalization}
\label{sec:formal}

We now give a formal description of our machine model, security semantics,
and properties. Our definitions abstract over: (1) the details of  the target machine
architecture and ABI, (2) the set of security-relevant operations and their effects on
the security context, (3) the set of observable events, and (4) a notion of value compatibility.
%They can be extended to a more sophisticated machine model by expanding the set of
%security-relevant events and defining
\apt{Any more? Give a more listlike presentation?
  Need to clarify exactly what the parameters are, both in the initial
  presentation and in the various extensions, including the appendix.}
\sna{Need to come back to this.}

\subsection{Machine}
The building blocks of a machine are {\em words} and {\em registers}.
Words are ranged over by \(\word\) and, when used as addresses, \(\addr\),
and are drawn from the set \(\WORDS\).
Registers in the set \(\REGS\) are ranged over by \(\reg\), with the stack pointer
given the special name \(\SP\);
some registers may be classified as caller-saved or callee-saved.
Along with the program counter, \(\PCname\), these are referred to as
{\em state elements} \(\component\) in the set \(\COMPONENTS ::= \PCname | \WORDS | \REGS\).

A {\em machine state} \(\mach \in \MACHS\) is a map from state elements to a set \(\mathcal{V}\) of
\emph{values}.
Each value \(v\) contains a \emph{payload} word, written \(|v|\).
We write \(\mach[\component]\) to denote the value of \(\mach\) at
\(\component\)  and \(\mach[v]\) as shorthand for \(\mach[|v|]\).
The details of value structure depend on the specific machine being modelled;
intuitively, the payload represents the part of the value that is relevant to
the behavior of the basic machine, while the rest of the value may contain
information relevant to a hardware enforcement mechanism (such as a tag).
We assume a given \emph{compatibility} equivalence relation \(\sim\) on values,
and lift it element-wise to states.
In the tagged setting, values are compatible if their tags are identical.
\apt{This is still not very convincing!
  empting to say that only the payloads can affect the step function, but that is
  too strong, right? (OK for tags, but not, e.g. capabilities?}
\sna{Yeah, I don't think we can say something universal about when values should be
  considered compatible.}

The machine has a step function \(\mach \xrightarrow{\bar{\psi},\obs} \mach'\).
Except for the annotations over the arrow, this function just encodes the usual
ISA description of the machine's instruction set. The annotations serve to connect
the machine's operation to our security setting:
\(\bar{\psi}\) is a list of security-relevant operations drawn from an assumed given set \(\Psi\),
and \(\obs\) is an (potentially silent) observable event; these are described further below.

\subsection{Security semantics}

The security semantics operates in parallel with the machine.
Each state element (memory word or register) is given a \emph{security class}
\(l \in \{\public, \object, \sealed, \unsealed\}\).
A \emph{view} \(V \in \mathit{VIEW}\) maps elements to security classes.
For any security class \(l\), we write \(l(V)\)
to denote the set of elements \(\component\) such that \(V ~ \component = l\).
The {\it initial view} \(V_0\) maps all stack locations to \(\unsealed\),
all other locations to \(\public\), and registers based on which set they
belong to: \(\sealed\) for callee-saved, \(\unsealed\) for caller-saved except for those
that contain arguments at the start of execution, which are \(\object\), and \(\public\) otherwise.

A (security) \emph{context} is
a pair of the current activation's view and
a list of views representing the call stack (pending inactive
principals), ranged over by \(\sigma\).
%
\[\context \in \CONTEXTS ::= \mathit{VIEW \times list ~ VIEW}\]
%
The initial context is \(\context_0 = (V_0, \emplist)\).

\Cref{sec:example} describes informally how the security context evolves as the system performs
security-relevant operations. Formally, we combine each machine state with a context
to create a {\it combined state} \(s = (\mach,\context)\) and lift the transition
to \(\stepstounder{}\) on combined states.
At each step, the context updates based on an assumed given function
\(Op : \MACHS \rightarrow \CONTEXTS \rightarrow \psi \rightarrow \CONTEXTS\).
Since a single step might correspond to multiple operations, we apply
\(Op\) as many times as needed, using \(\mathit{foldl}\).

\judgmenttwo{\(\mach \xrightarrow{\overline{\psi},\obs} \mach' \)}
            {\(\mathit{foldl} ~ (Op ~ \mach) ~ \context ~ \overline{\psi} = \context'\)}
            {\((\mach,\context) \stepstounder{\overline{\psi},\obs} (\mach', \context')\)}

A definition of \(Op\) is most convenient to present decomposed into
rules for each operation. We have already seen the intuition behind the rules for
\(\mathbf{alloc}\), \(\mathbf{call}\), and \(\mathbf{ret}\).
For the machine described in the example, the \(Op\) rules would be those
found in \cref{fig:basicops}.
Note that \(Op\) takes as its first argument the state {\it before} the step.

\begin{figure}
    \[\mathit{range} ~ \reg ~ \mathit{off} ~ \mathit{sz} ~ \mach \triangleq
    \{\mach[\reg]+i | \mathit{off} \leq i < \mathit{off+sz}\}\]

  \judgmentbr{\(\components = \mathit{range} ~ \SP ~ \mathit{off} ~ \mathit{sz} ~ \mach \cap
    \{\addr \mid V ~ \addr = \unsealed\}\)}
             {\(V' = V \llbracket \addr \mapsto \sealed \mid \addr \in \components \rrbracket\)}
             {\(Op ~ \mach ~ (\mathbf{alloc} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}
    %
    \judgmentbr{\(b = \mach[\SP] + \mathit{off}\)}
               {\(V' = V \llbracket \addr \mapsto \unsealed |
                 b \leq a < b+\mathit{sz} \land V ~ \addr = \object \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{dealloc} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}
    %
    \judgmentbr{\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
                 \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
               {\(V'' = V'\llbracket \addr \mapsto \sealed | V' ~ \addr = \object \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}})
                 ~ (V,\sigma) = (V'',V::\sigma)\)}
    %
    \judgment{\(\sigma = (V,\addr_{ret},\addr_{sp})::\sigma'\)}
             {\(Op ~ \mach ~ \mathbf{return} ~ (\_, \sigma) = (V, \sigma')\)}
  \caption{Basic Operations}
  \label{fig:basicops}
\end{figure}

\subsection{Events and Traces}
\label{sec:events}

We abstract over the events that can be observed in the system, assuming just
oa given set \(\OBSS\) that contains at least the element \(\tau\), the silent
event. Other events might represent certain function calls (i.e., system calls)
or writes to special addresses representing memory-mapped regions.
A {\em trace} is a nonempty, finite or infinite sequence
of events, ranged over by \(\obsT\).
We use ``\(\notfinished{}{}\)'' to represent ``cons'' for traces, reserving ``::''
for list-cons.

We are particularly interested in traces that end just after a function returns.
We define these in terms of the depth \(d\) of the security context's call stack \(\sigma\).
We write \(d \hookrightarrow s\) for the trace of execution from a state \(s\)
up to the first point where the stack depth is smaller than \(d\), defined
coinductively by these rules:

\judgment{\(|\sigma| < d\)}
         {\(d \hookrightarrow (\mach,(V,\sigma)) = \tau\)}

\judgmentthree{\((\mach,(V,\sigma)) \stepstounder{} (\mach',\context',\obs)\)}
              {\(|\sigma| \geq d\)}
              {\(d \hookrightarrow (\mach',\context') = \obsT\)}
              {\(d \hookrightarrow (\mach,(V,\sigma)) = \notfinished{\obs}{\obsT}\)}

\noindent
When \(d = 0\), the trace will always be infinite; in this case we
omit \(d\) and just write \(\hookrightarrow s\).

Two event traces $\obsT_1$ and $\obsT_2$ are {\em similar},
written \(\obsT_1 \eqsim \obsT_2\), if the sequence of non-silent events
is the same. That is, we compare up to deletion of \(\tau\) events.
Note that this results in an infinite silent trace being similar to
any trace. So, a trace that silently diverges due to a failstop\apt{?? haven't explained this!}  will
fulfill this property vacuously.

\begin{minipage}{.4\columnwidth}
  \judgment{}{\(\obsT \eqsim \obsT\)}
\end{minipage}
\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\notfinished{\obs}{\obsT_1} \eqsim \notfinished{\obs}{\obsT_2}\)}
\end{minipage}

\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\notfinished{\tau}{\obsT_1} \eqsim \obsT_2\)}
\end{minipage}
\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\obsT_1 \eqsim \notfinished{\tau}{\obsT_2}\)}
\end{minipage}

\subsection{Variants, corrupted sets, and ``on-return'' assertions}
\label{sec:props}

Two (compatible) states are variants with respect to a set of elements \(\components\)
if they agree on the value of every element not in \(\components\).
Our notion of non-interference involves comparing the traces of such
\(\components\)-variants. We use this to define sets of irrelevant elements.

\definition The \emph{difference set} of two machine states \(\mach\) and \(\mach'\),
written \(\Delta(\mach,\mach')\),
is the set of elements \(\component\) such that \(\mach[\component] \not = \mach'[\component]\).

\definition Machine states \(\mach\) and \(\nach\) are {\em \(\components\)-variants},
written \(\mach \approx_\components \nach\), if \(\mach\sim\nach\) and
\(\Delta(\mach,\nach) \subseteq \components\).
%if all \(\component \not \in \components\), \(\mach[\component] = \nach[\component]\)
%and for all \(\component \in \components\), \(\mach[\component] \sim \nach[\component]\).

\definition An element set \(\components\) is \emph{irrelevant} to state \((\mach,\context)\),
written \((\mach,\context) \parallel \components\), if for all
\(\nach\) such that \(\mach \approx_{\components} \nach\),
\(\hookrightarrow (\mach,\context)  \eqsim \hookrightarrow (\nach,\context)\).


When comparing the behavior of variant states, we need a notion of how their
differences have influenced them.
\definition The {\em corrupted set} \(\bar{\Diamond}(\mach,\mach',\nach,\nach')\)
is the set \((\Delta(\mach,\mach') \cup \Delta(\nach,\nach')) \cap \Delta(\mach',\nach')\).

If we consider two execution sequences, one from \(\mach\) to \(\mach'\)
and the other from \(\nach\) to \(\nach'\),
then \(\bar{\Diamond}(\mach,\mach',\nach,\nach')\) is the set of elements that
change in one or both executions and end up with different values. Intuitively,
this captures the effect of any differences between \(\mach\) and \(\nach\), i.e.,
the set of values that are ``corrupted'' by those differences.

Our ``on-return'' assertions are defined using a second-order logical operator
\(d \uparrow P\), pronounced ``\(P\) holds on return from depth \(d\),''
where \(P\) is a predicate on machine states. This is a coinductive relation
similar to ``weak until'' in temporal logic---it also holds if the program never
returns from depth \(d\).

\judgmenttwo[Returned]
            {\(|\sigma| < d\)}
            {\(P ~ \mach\)}
            {\((d \uparrow P) ~ (\mach, (V,\sigma))\)}

\judgmenttwobrlong[Step]
                  {\(|\sigma| \geq d\)}
                  {\((d \uparrow P) ~ (\mach', \context')\)}
                  {\((\mach, (V,\sigma)) \stepstounder{\overline{\psi},\obs} (\mach', \context')\)}
                  {\((d \uparrow P) ~ (\mach, (V,\sigma))\)}

Similarly, we give a binary equivalent for use in confidentiality. We define \(\Uparrow\) so that
\((\mach,\context) ~ (d \Uparrow R) ~ (\mach',\context')\) holds if \(R\) holds on the
first states that return from depth \(d\) after \((\mach,\context)\) and \((\mach',\context')\),
respectively. Once again, \(\Uparrow\) is coinductive.

\judgmentthree[Returned]
              {\(|\sigma_1| < d\)}
              {\(|\sigma_2| < d\)}
              {\(\mach_1 ~ R ~ \mach_2\)}
              {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\judgmenttwobrlong[Left]
                  {\(|\sigma_1| \geq d\)}
                  {\((\mach_1,(V_1,\sigma_1)) \stepstounder{\overline{\psi},\obs} (\mach_1',\context_1')\)}
                  {\((\mach_1',\context_1') ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}
                  {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\judgmenttwobrlong[Right]
                  {\(|\sigma_2| \geq d\)}
                  {\((\mach_2,(V_2,\sigma_2)) \stepstounder{\overline{\psi},\obs} (\mach_2',\context_2')\)}
                  {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2',\context_2')\)}
                  {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\subsection{Properties}

\begin{table*}[h]
  \setlength{\tabcolsep}{1pt}
  \center
  \begin{tabular}{l r l l l}
    \rowcolor{black!20}
    1
    & \(\wbcf \triangleq\) & \((|\sigma'| \uparrow \ret) ~ (\mach', (V',\sigma'))\)
    & \(\textnormal{ where } \ret ~ \mach'' \triangleq \)
    \(\mach''[\SP] = \mach[\SP]\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is called}\) \\
    & \(\textnormal{ for all calls } (\mach,(V,\sigma)) \stepstounder{} (\mach',(V',\sigma'))\) \\
    %
    \rowcolor{black!20}
    & & & \(\textnormal{ and } \mach''[\PCname] = \mach[\PCname]+4\) & \\
    %
    \rowcolor{black!10}
    2
    & \(\clri \triangleq\) & \((|\sigma| \uparrow \intProp) ~ (\mach,(V,\sigma))\)
    & \(\textnormal{ where } \intProp ~ \mach' \triangleq
    \mach' \parallel (\sealed(V) \cap \Delta(\mach,\mach'))\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
    %
    \rowcolor{black!20}
    3
    & \(\clrc \triangleq\) & \(\forall \nach \textnormal{ s.t. } \mach \approx_{\components} \nach,\)
    & \(\textnormal{ where } \components = \sealed(V)\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is (tail)called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
    \rowcolor{black!20}
    3a & & \(|\sigma| \hookrightarrow (\mach,(V,\sigma)) \simeq |\sigma| \hookrightarrow (\nach,(V,\sigma))\) & & \\
    \rowcolor{black!20}
    3b & & \(\textnormal{ and } (\mach,(V,\sigma)) ~ (|\sigma| \Uparrow \confProp) ~ (\nach,(V,\sigma))\)
    & \(\textnormal{ where } (\mach' ~ \confProp ~ \nach') \triangleq
    \mach' \parallel \bar{\Diamond}(\mach,\nach,\mach',\nach')\) & \\
    %
    \rowcolor{black!10}
    4
    & \(\clec \triangleq\) & \((|\sigma| \uparrow \cconfProp) ~ (\mach,(V,\sigma))\)
    & \(\textnormal{ where } \cconfProp ~ \mach' \triangleq
    \mach' \parallel (\Delta(\mach,\mach') - \components)\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is (tail)called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
    \rowcolor{black!10}
    & & & \(\textnormal{ where } \components = \public(V) \cup \object(V)\) & \\
    %
    \rowcolor{black!20}
    5
    & \(\clei \triangleq\) & \(\forall \nach \textnormal{ s.t. } \mach \approx_{\components} \nach,\)
    & \(\textnormal{ where } \components = \COMPONENTS - (\public(V) \cup \object(V))\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is (tail)called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
   \rowcolor{black!20}
    5a & & \(|\sigma| \hookrightarrow (\mach,(V,\sigma)) \simeq |\sigma| \hookrightarrow (\nach,(V,\sigma))\) & & \\
    \rowcolor{black!20}
    5b & & \(\textnormal{ and } (\mach,(V,\sigma)) ~ (|\sigma| \Uparrow \cintProp) ~ (\nach,(V,\sigma))\)
    & \(\textnormal{ where } (\mach' ~ \cintProp ~ \nach') \triangleq
    \mach' \parallel \bar{\Diamond}(\mach,\nach,\mach',\nach')\) & \\
  \end{tabular}
  \caption{Properties}
  \label{tab:props}
\end{table*}

Finally, the core property definitions are given in \cref{tab:props},
arranged to show their commonalities and distinctions. Each definition gives a criterion
quantified over states \(s\) that immediately follow call steps.
If an execution includes a transition \(s' \stepstounder{\overline{\psi}} s\)
where \(\mathbf{call} ~ \addr ~ \overline{\reg} \in \bar{\psi}\), then \(s\) is the target
of a call.
%Likewise, if \(\mathbf{tail call} ~ \addr ~ \overline{\reg} \in \bar{\psi}\), then
%\(s\) is the target of a tail call.
As a shorthand, we write that each property is defined
by a criterion that must hold ``for all call targets \(s\),'' or, in the case of \(\wbcf\),
``for all call steps \(s \stepstounder{} s'\).''

\paragraph*{1.~\(\wbcf\)}
Given a call step \((\mach,(V,\sigma)) \stepstounder{} (\mach',(V',\sigma'))\),
we define the predicate \(\ret\) to hold on states \(\mach''\)
whose stack pointer matches that of \(\mach\)
and whose program counter is at the next instruction. A system enjoys \(\wbcf\) if,
for every call transition, \(\ret\) holds just after the callee returns (i.e.,
the call stack shrinks).

\paragraph*{2.~\(\clri\)}
When the call target is \((\mach,(V,\sigma))\), we define the predicate \(\intProp\) to hold
on states \(\mach'\) if any elements that are both sealed in \(V\) and in the difference
set between \(\mach\) and \(\mach'\) are irrelevant. A system enjoys \(\clri\) if, for every
call, \(\intProp\) holds just after the corresponding return.

\paragraph*{3.~\(\clrc\)}
When the call target is \((\mach,(V,\sigma))\), we begin by taking an arbitrary \(\nach\)
that is a \(\components\)-variant of \(\mach\), where \(\components\) is the set of sealed elements
in \(V\). We require that two clauses hold. On line 3a, the behavior of a trace from
\((\mach,(V,\sigma))\) up to its return must match that of \((\nach,(V,\sigma))\).
On line 3b, we define a relation \(\confProp\) that relates states \(\mach'\) and \(\nach'\)
if their corrupted set (relative to \(\mach\) and \(\nach\)) is irrelevant, and require
that it hold just after the returns from the callees that start at \((\mach,(V,\sigma))\) and \((\nach,(V,\sigma))\).
A system enjoys \(\clrc\) if both clauses hold for every call.

\paragraph*{4.~\(\clec\)}
We consider the callee's private behavior to be any changes that it makes to the state
outside of legitimate channels---elements marked \(\object\) or \(\public\). The remainder
should be kept secret, which is to say, irrelevant to future execution. Similar to \(\clri\), given a call target
\((\mach,(V,\sigma))\), we define a predicate \(\cconfProp\) to hold
on states \(\mach'\) if the difference set between \(\mach\) and \(\mach'\), excluding
\(\object\) or \(\public\) locations, is irrelevant.
A system enjoys \(\clec\) if, for every call, \(\cconfProp\) holds just after the corresponding return.

\paragraph*{5.~\(\clei\)}
Callee integrity means that the caller does not influence it outside of legitimate
channels. The caller's influence can be seen internally, or in corrupted data on return,
just like the caller's secrets would be under \(\clrc\). So, for a call target
\((\mach,(V,\sigma))\), we take an arbitrary \(\nach\) that is a \(\components\)-variant
of \(\mach\), where \(\components\) is the set all elements that are not \(\object\)
or \(\public\). The remainder of the property is identical to \(\clrc\).


\section{Expanding the Machine}
\label{sec:extensions}

The system we model in \cref{sec:example,sec:formal} is very simple, but our properties
are designed to make it easy to add features. To support argument passing on the stack,
we just new parameters to the existing security-relevant operations, and refine how they
update the security context. The remainder of the properties do not change at all.
To add tail-calls, we add and define a new operation, and since it is a kind of call,
we add it to the definition of call targets.
The rules for the expanded machine are given in \cref{fig:advops}; the
rules in \cref{fig:basicops} can be recaptured by instantiating
\(\mathbf{call}\) with \(\overline{sa}\) as the empty set, and \(\mathbf{alloc}\)
with flag \(\mathbf{f}\).

\subsection{Sharing Stack Memory}
In our examples, we have presented a vision of stack safety in which
the interface between caller and callee is in the registers that pass
arguments and return values. This is frequently not the case in a realistic
setting. Arguments may be passed on the stack because there are too many
to pass in registers, as an implementation of variadic arguments, or
because they are composite types that inherently have
pass-by-reference semantics. The caller may also pass an object by reference
in the C++ style, or take its address and pass it as a pointer.

We refine our call operation to make use of the information that we have about
which memory contain arguments, \(\overline{sa}\). \(\overline{sa}\) is a set of
triples of a register, an offset from the value of that register, and a size.
We first define the helpful set \(\mathit{passed} ~ \overline{sa} ~ \mach\),
then extend the call operation to keep all objects in \(\mathit{passed}\) marked
as \(\object\) and seal everything else (\cref{sfig:stkargs}).

Using this mechanism, an call-by-value argument passed on the stack at an \(\SP\)-relative offset
is specified by the triple \((\SP, \mathit{off}, \mathit{sz})\).
In this case, only the immediate callee gains access to the argument location.
A C++-style call-by-reference argument where the reference is passed in \(\reg\)
is instead specified by \((\reg, 0, \mathit{sz})\), which allows
the reference to be passed further down the stack.
\apt{What if the reference is itself passed on the stack?}
\sna{Ugh. We could make \(\reg\) an arbitrary element, but that seems like it's getting
  dangerously close to indirection.}

If the address of an object is taken directly and passed as a
pointer, we simply classify the object as ``public'' and give it no protection against
access by other functions. We extend the \(\mathbf{alloc}\) operation
with a boolean flag, where {\bf t} indicates
that the allocation is public, and {\bf f} that it is private.
If space for multiple objects is allocated in a single step,
that step can make multiple allocation operations, each labeled appropriately.
Public objects are labeled \(\public\) rather than \(\object\), so they are
never sealed at a call (\cref{sfig:publicalloc}).
Providing more fine-grained control over sharing is desirable, but
seems to require a considerably more complex model; \cref{app:ptr} describes one
approach, based on capabilities.


\begin{figure*}
  \begin{subfigure}{0.4\textwidth}
    \[\mathit{range} ~ \reg ~ \mathit{off} ~ \mathit{sz} ~ \mach \triangleq
    \{\mach[\reg]+i | \mathit{off} \leq i < \mathit{off+sz}\}\]

    \judgmentbr{\(\components = \mathit{range} ~ \SP ~ \mathit{off} ~ \mathit{sz} ~ \mach \cap
                 \{\addr \mid V ~ \addr = \unsealed\}\)}
               {\(V' = V \llbracket \addr \mapsto \sealed \mid \addr \in \components \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{alloc} ~ \mathbf{f} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}

    \judgmentbr{\(\components = \mathit{range} ~ \SP ~ \mathit{off} ~ \mathit{sz} ~ \mach \cap
                 \{\addr \mid V ~ \addr = \unsealed\}\)}
               {\(V' = V \llbracket \addr \mapsto \public \mid \addr \in \components \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{alloc} ~ \mathbf{t} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}
    %
    \judgmentbr{\(b = \mach[\SP] + \mathit{off}\)}
               {\(V' = V \llbracket \addr \mapsto \unsealed |
                 b \leq a < b+\mathit{sz} \land V ~ \addr = \object \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{dealloc} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}

    \caption{Memory Allocation}
    \label{sfig:publicalloc}
  \end{subfigure}
  \begin{subfigure}{0.6\textwidth}
    %
    \[\mathit{passed} ~ \overline{sa} ~ \mach = \bigcup_{(\reg,\mathit{off},\mathit{sz}) \in \overline{sa}}
    \mathit{range} ~ \reg ~ \mathit{off} ~ \mathit{sz} ~ \mach\]
    %
    \judgmentbr{\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
                 \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
               {\(V'' = V'\llbracket \addr \mapsto \sealed | V' ~ \addr = \object \land \addr \not \in (\mathit{passed} ~ \overline{sa} ~ \mach) \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa})
                 ~ (V,\sigma) = (V'',V::\sigma)\)}
    %
    \judgmentbr{\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
                 \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
               {\(V'' = V'\llbracket \addr \mapsto \unsealed | V' ~ \addr = \object \land \addr \not \in (\mathit{passed} ~ \overline{sa} ~ \mach) \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{tail call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa})
                 ~ (V,\sigma) = (V',\sigma)\)}
    %
    \judgment{\(\sigma = (V,\addr_{ret},\addr_{sp})::\sigma'\)}
             {\(Op ~ \mach ~ \mathbf{return} ~ (\_, \sigma) = (V, \sigma')\)}

    \caption{Calls with Argument Passing on the Stack}
    \label{sfig:stkargs}
  \end{subfigure}
  \caption{Operations supporting tail calls and argument passing on stack.}
  \label{fig:advops}
\end{figure*}

\subsection{Tail Calls}

The operation rule for a tail call is similar to that for a normal call.
We do not push the caller's view onto the stack,
but replace it outright. This means that a tail call does not increase the size of
the call stack, and therefore for purposes of our properties, all tail
calls will
be considered to return simultaneously when the eventual {\bf return} operation
pops the top of the stack.

Since the caller will not be returned to, it does not need integrity, but
it should still enjoy confidentiality. We set its frame to \(\unsealed\) rather
than \(\sealed\) to express this. In terms of \cref{tab:props}, we replace
``call'' with ``call or tail call'' in \(\clrc\), \(\clec\), and \(\clei\).

\section{Enforcement}
\label{sec:enforcement}

We implement and test two micro-policies from Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}:
{\em Depth Isolation} (\(DI\)) and {\em Lazy Tagging and Clearing} (\(LTC\)).
They share a common structure: each function activation is assigned a ``color'' \(n\)
representing its identity. Stack locations belonging to that activation are
tagged \(\tagStackDepth{n}\), and while the activation is running, the tag on the
program counter (PC tag) is \(\tagPCDepth{n}\). Stack locations not part of
any activation are tagged \(\tagNoDepth\).

In \(DI\), \(n\) always corresponds to the depth of the stack when
the function is called. A function must initialize its entire frame upon entry
in order to tag it, and then clear the frame before returning.
During normal execution, the micro-policy rules only permit load and
store operations when the target memory is tagged {\em with the same depth}
as the current {\PCname} tag, or, for store operations, if the target memory
is tagged \(\tagNoDepth\).

In \(LTC\), rather than initialize the entire frame at entry,
a function initializes each location's tag when it writes to it. It does
not check if those writes are legal! If the PC tag is \(\tagPCDepth{n}\),
then any stack location that recieves a store will be tagged \(\tagStackDepth{n}\).
On a load, the micro-policy failstops if the source memory location
is tagged \(\tagNoDepth\) or \(\tagStackDepth{n}\) for some \(n\) that
doesn't match the PC tag.

\paragraph*{Blessed Sequences}

To implement this discipline, {\em blessed instruction sequences} appear at
the entry and exit of each function, which manipulate tags as just described
while performing the usual tasks of saving/restoring the return address to/from
the stack and adjusting the stack pointer. A blessed sequence uses further tags
to guarantee that the full sequence executes from the beginning---no jumping into the middle.

\paragraph*{Protecting Registers}

Roessler and DeHon do not protect registers, since the compiler is in the
trusted computing base, but we support threat models that do not.
In particular, \(\clri\) requires callee-saved
registers to be saved and restored properly. We extend \(DI\) and \(LTC\)
so that callee-saved registers, too, are tagged with the color of the
function that is using them. In \(DI\) they are tagged as part of the entry
sequence, while in \(LTC\) they are tagged when a value is placed in them.

\section{Validation through Random Testing}
\label{sec:testing}

There are several ways to evaluate whether an enforcement mechanism enforces the above
stack safety properties. Ideally such validation would be done through formal proof over
the semantics of the enforcement-augmented machine.
However, while there are no fundamental barriers to producing such a proof,
it would be considerable work to carry out for a full ISA like RISC-V and
complex enforcement mechanisms like Roessler and DeHon's micro-policies.
We therefore choose to systematically \emph{test} their micro-policies.
\leo{moved the following here, I think it makes more sense here.}
Our primary testing targets are the eager {\em Depth Isolation}
and the {\em Lazy Per-Activation Tagging and Clearing} micro-policies.


We use a Coq specification of the RISC-V architecture~\cite{Bourgeat2021AMF},
and extend it with a runtime monitor implementing a stack safety
micro-policy. We choose the Coq proof assistant as the setting for our implementation
to ensure that our coinductive trace definitions are well-formed, to reason about
them\leo{where do we mention reasoning about them?}, and to leverage the power of the QuickChick property-based testing framework~\cite{Pierce:SF4}.

\subsection{Test Generation}
To use QuickChick, we developed random test-case generators that produce
(1) an initial RISC-V machine state, including the program to be executed;
(2)
  an initial policy state, tagging instructions and the stack regions appropriately
  for the micro-policy (see \cref{sec:enforcement})
(3)
  a map from instructions to lists of their operations in the security semantics.

To write such generators we build on the work of Hri\c{t}cu et
al. \cite{TestingNI:ICFP, DBLP:journals/jfp/HritcuLSADHPV16}, which
introduced {\em generation by execution}, a technique that produces
programs that lead to longer executions---and hopefully towards more
interesting behaviors as a result. We extend their technique with an additional
degree of statefulness: when complex calling conventions need to be
observed to avoid early failstops, extra care must be taken to ensure
that any such calls are well behaved. For example, immediately
following a call we need to initialize any stack-allocated variables
with an increased probability. However, at the same time, we need to
allow for the possibility of {\em not} performing such an initialization
to allow for potential attack vectors to manifest.

Moreover, we need to further extend typical hyperproperty testing
schemes to handle the nested nature of confidentiality: rather than
just generating two initial machines that are variants of one another
and letting them execute to test for noninterference, we generate a
new variant {\em every time a call is made} and check confidentiality
for the subtrace produced from that variant state until its
corresponding return. As a result, a ``single'' confidentiality test
compactly checks multiple nested calls.


\subsection{Mutation Testing}

To ensure the effectiveness of testing against our formal properties, we
use {\em mutation testing}~\cite{JiaH11}. In mutation testing, we inject errors
(mutations) in a program that should cause the property of interest (here,
stack safety) to fail, and ensure that the testing framework can find
them. The bugs we use for our evaluation are either artificially generated
by us (deliberately weakening the micro-policy in ways that we expect
should break its guarantees), or actual bugs that we discovered through
testing our implementation. We elaborate on some such bugs below.

For example, when loading from a stack location, {\em Depth Isolation}
needs to enforce that the tag on the location being read
is $\tagStackDepth{n}$ for some number $n$ and that the tag of the
current $\PCname$ is $\tagPCDepth{n}$ for the same depth $n$. We can relax
that restriction by not checking the depth equality (row {\em
  LOAD\_NO\_CHECK\_DI}).

Similarly, when storing to a stack location, the correct micro-policy
needs to ensure that the tag on the memory location is either
$\tagNoDepth$ or has again the same depth as the current $\PCname$
tag. Relaxing that constraint causes violations to the integrity
property (row {\em STORE\_NO\_CHECK}).

\begin{table}[]
\centering
\begin{tabular}{c|c|c|c}
  Bug & Property Violated & MTTF (s) & Tests \\
  \hline
      {\em LOAD\_NO\_CHECK\_DI}  & Confidentiality & 24.2 & 13.3 \\
      {\em STORE\_NO\_CHECK} & Integrity & 26.9 & 26 \\
      {\em HEADER\_NO\_INIT} & Integrity & 69.5 & 76.3 \\
  \hline
  \hline
      {\em PER\_DEPTH\_TAG} & Obs. Integrity & 189.7 & 8342.5  \\
      {\em LOAD\_NO\_CHECK\_LT}  & Obs. Integrity & 23.5 & 12.0 \\
      {\em LOAD\_NO\_CHECK\_LT}  & Confidentiality & 19.2 & 695.5 \\
      {\em STORE\_NO\_UPDATE} & Obs. Integrity & 70 & 80.6  \\
      {\em STORE\_NO\_UPDATE} & Confidentiality & 4.9 & 88.5 \\
  \hline
\end{tabular}
\vspace*{1em}
\caption{MTTF for finding bugs in erroneous policy enforcement mechanisms}
\vspace*{-2em}
\label{tab:bug-table}
\end{table}

\subsection{Results}

The mean-time-to-failure (MTTF) and average number of tests for various bugs can be found in
\cref{tab:bug-table}, along with the average number of tests
it took to find the failure. Experiments were run in a desktop
machine equipped with i7-4790K CPU @ 4.0GHz with 32GB RAM.

Naturally, testing also revealed a number of errors in our
implementation of the enforcement mechanism (the original was written in C++
and targeted ARM machine code;
%\bcp{right?}\leo{yeah}
we re-implemented it in Coq targeting RISC-V).  These errors range
from trivial typos to ones that require an intriguingly complex setup
to provoke.  The most interesting bug (included in the table as row
{\em HEADER\_NO\_INIT}) was that, on our first try, the blessed call
sequence %/policy combination\apt{??}
did not initialize all locations for the
newly allocated stack frame correctly, but left some of them as
$\tagNoDepth$. This allowed for a potential integrity violation, but
only if a rather complicated sequence of events occured.
The smallest counterexample requires calling a function {\tt f},
which fails to initialize some of its frame,
but writes into an uninitialized location $l$ later, treating \(l\) as outside
the stack. Then {\tt f} calls a further function {\tt g} (which should have
the effect of sealing $l$ for integrity purposes). {\tt g} attempts to write to $l$,
which is allowed because the enforcement mechanism still has
$l$ tagged as $\tagNoDepth$, but violates the integrity property on {\tt f}'s data.
%\sna{I believe what went wrong was that we were off-by-one in {\tt main}'s initialization,
%  and the write from {\tt f} was already a violation.}

As for \(LTC\), the original micro-policy, implemented as {\em PER\_DEPTH\_TAG},
fails in testing, in cases where data is leaked between sequential calls.
To round out our mutation testing we also check {\em LOAD\_NO\_CHECK\_LT},
equivalent to its counterpart in depth isolation,
and a version where stores succeed but fails to propagate the PC tag, {\em STORE\_NO\_UPDATE}.
It turns out that {\em PER\_DEPTH\_TAG} is a comparatively subtle bug,
taking twice as long to catch as the next longest.

Our properties have allowed us to identify an enforcement mechanism as
not really stack safe, and to validate a possible fix.

\section{Related Work}
\label{sec:relwork}

The centrality of the functional abstraction and its security are behind the
many software and hardware mechanisms proposed for its protection
\cite{Cowan+98, NagarakatteZMZ09, NagarakatteZMZ10, DeviettiBMZ08,
Kuznetsov+14, Dang+15, Shanbhogue+19, Woodruff+14, Chisnall+15,
SkorstengaardLocal, SkorstengaardSTKJFP, Georges22:TempsDesCerises,
DBLP:conf/sp/RoesslerD18, Gollapudi+23}, which we survey in
Sections~\ref{sec:intro} and~\ref{sec:future}. Among these techniques, the micro-policies of Roessler and
DeHon \cite{DBLP:conf/sp/RoesslerD18} are the most flexible in supporting a
range of safeguards including lazy enforcement, and suitable for formulating
and validating strong security guarantees.

The only other line of work that sets out to rigorously characterize the
security of the stack is the Cerise family of program logics
\cite{SkorstengaardLocal, SkorstengaardSTKJFP, Georges22:TempsDesCerises}.
These works define secure calling conventions for capability machines and
overlay semantics that internalize those conventions and their related stack
safety properties, phrased in terms of logical relations. Most flavors of
Cerise define experimental types of hardware capabilities in order to obtain
efficient calling conventions. Their latest paper
\cite{Georges22:TempsDesCerises} uses our work to extend their notion of stack
safety to include caller confidentiality (\(\clrc\)).

HardBound \cite{DeviettiBMZ08} is another hardware extension related to
capabilities, which closely follows SoftBound \cite{NagarakatteZMZ09} and
simply aims to accelerate its enforcement of memory safety for compiled code.

\section{Future Work}
\label{sec:future}

\subsection{Testing The Stack Safety Menagerie}

In \cref{sec:intro}, we discussed several dimensions of flexibility
that our properties should exhibit: applicability to different system
features and different security requirements. Another important dimension
is applicability to multiple enforcement mechanisms.
In the following, we discuss how different existing mechanisms
might fit into our model.

\paragraph{Stack Canaries and Shadow Stacks}
%
Canaries are special values inserted by the caller at certain points of the
stack and checked for corruption on return, for protection against
some classes of control-flow hijacking (i.e., overwriting the caller's
return address through a buffer overflow that would also overwrite the canary
detectably). A shadow stack is a mirror image of the control-relevant parts of the stack,
maintained by the program to attempt to detect attempts to hijack its control
flow and ``restrict the flexibility available in creating gadget chains''
\cite{Shanbhogue+19}.
%
Interestingly, these are lazy enforcement mechanisms, in that
the attack may occur and be detected some time later, as long as
it is detected before it can become dangerous. That would make our
formalism a good fit for defining their security, except that
they are chiefly hardening techniques: they increase the difficulty
of some control-flow attacks on the stack, but cannot provide absolute
guarantees on \(\wbcf\) under a normal attacker model.

\paragraph{Split Stacks}
%
Code-pointer integrity seeks to prevent control-flow hijacking attacks by
splitting the program memory, including the stack, into a regular region and a
safe region; objects that require protection are placed in the latter and
all accesses to them protected by static or dynamic checks.
%
Like shadow stacks, bounds checking is a high-level technique designed to
protect code written in a high-level language, but it is designed to
offer protection under a stronger threat model that also considers the presence
of untrusted code. As with most other software-based methods, it purports to
protect exclusively the control-relevant parts of the stack, making no claims
about its data-relevant parts---we would solely be testing \(\wbcf\).

\paragraph{Bounds Checking}
%
Under a bounds checking discipline such as SoftBound \cite{NagarakatteZMZ09}, all the pointers
in a program are extended with some disjoint metadata, and these are combined
to gate memory accesses. These approaches enforce a form of \emph{memory safety},
and we would therefore expect them to enforce \(\clri\) and \(\clrc\). They aim
to enforce \(\wbcf\) by cutting off attacks that involve memory-safety violations,
but that may not be sufficient.

Bounds checking approaches require substantial compiler cooperation. This is not a
problem for our properties in general, but it is not very compatible with
generation-by-execution of low-level code. A better choice would be to generate
high-level code using a tool like CSmith \cite{DBLP:conf/pldi/YangCER11}, or prove the properties instead.

\paragraph{Capabilities}

Capability machines can represent and manipulate
unforgeable tokens of authority over specific regions of
memory using a combination of dedicated out-of-band memory and
specialized ISA instructions.
%
The most prominent contemporary representative of this long line of work is
CHERI \cite{DBLP:conf/sp/WatsonWNMACDDGL15}, a modern architecture designed to provide efficient fine-grained
memory protection and compartmentalization.
%
Previous work has used these principles in simplified models to implement
\emph{secure calling conventions} for the call
stack \cite{SkorstengaardLocal,SkorstengaardSTKJFP,Georges22:TempsDesCerises}. Those conventions can be seen as combining the
protections of bounds-checking with software-based methods of control-flow protection.

There are several proposals around the use of CHERI capabilities to enforce stack safety,
including mechanisms that use the standard CHERI hardware (which includes local
capabilities) \cite{SkorstengaardLocal},
and others that propose entirely new types of capabilities, such as linear
\cite{SkorstengaardSTK}, uninitialized \cite{Georges+21}, lifetime
\cite{Tsampas+19}, and directed \cite{Georges22:TempsDesCerises} capabilities.
It is a high priority to test this most recent work by Georges et al., as it is designed
to be similar to all of our properties except for \(\clei\).

\subsection{More Features}

Several common features did not make it into this work: most importantly concurrency and
exceptions. We plan to extend the model to include them, and then test them. We would also
like to test the model given in \cref{app:ptr} for arbitrary memory-safe pointer sharing.

\subsection{Proofs}

\bibliographystyle{IEEEtran}
\bibliography{bcp.bib,local.bib}

\appendix

\subsection{Provenance, Capabilities, and Protecting Objects}
\label{app:ptr}

What if we want to express a finer-grained notion of safety, in which
stack objects are protected unless the function that owns them intentionally
passes a pointer to them? This can be thought of as a {\it capability}-based
notion of security. Capabilities are unforgeable tokens that grant access to
a region of memory, typically corresponding to valid pointers to that region.
So, in order to express such a property, we need our machine to carry some notion
of {\it pointer provenance}---a distinction between a pointer that is intended to
point to a given object, and non-pointer integers as well as pointers to other objects.

One such provenance model is Memarian et al.'s PVI \cite{provenance}, in which pointers are
annotated with the identity of the object they first pointed to. This annotation is
propagated when the pointer is copied and when operations are performed on it---even
integer-only operations.

We can model this as a trio of additional security-relevant operations: one which
declares a register to contain a valid pointer, one which transmits the provenance
of a pointer from one element to another, and one which clears the provenance
(for instance, when a pointer is modified in place in a way that makes it invalid.)

In addition to the normal call stack, our security context will carry a map \(\rho\) from
elements to memory regions, represented as a base and a bound \(\context = (V, \sigma, \rho)\).
Existing operations are extended to keep the value of \(\rho\) the same, and the new operations
work as follows:

\judgmentbr[Promote]
           {\(\psi = \mathbf{promote} ~ \reg_{dst} ~ (\reg_{base},\mathit{off},\mathit{sz})\)}
           {\(\rho' = \rho[\reg_{dst} \mapsto \mathit{range} ~ \reg_{base} ~ \mathit{off} ~ \mathit{sz}]\)}
           {\(Op ~ \mach ~ \psi ~ (V,\sigma,\rho) = (V,\sigma,\rho')\)}

\judgmentbr[Propagate]
           {\(\psi = \mathbf{propagate} ~ \component_{src} ~ \component_{dst}\)}
           {\(\rho' = \rho[\component_{dst} \mapsto \rho[\component_{src}]]\)}
           {\(Op ~ \mach ~ \psi ~ (V,\sigma,\rho) = (V,\sigma,\rho')\)}

\judgment[Clear]
         {\(\psi = \mathbf{clear} ~ \component\)}
         {\(Op ~ \mach ~ \psi ~ (V,\sigma,\rho) = (V,\sigma,\rho[\component \mapsto \emptyset])\)}

We now have a notion of provenance, and must integrate it into the definition of
stack safety. We essentially generalize the above notion of passing: we will consider
a caller to have intentionally passed an object if that object is reachable by
a capability that has been passed to the callee. This includes capabilities passed
indirectly, by being stored in an object that is in turn passed. Formally, we call
this set \(\mathit{capped}\), and define it recursively:
%
\[\begin{split}
& \mathit{capped} ~ \components ~ \rho \triangleq \bigcup_{\component \in \components} \{\component\} \cup \mathit{capped} ~ \components' ~ \rho \textnormal{ where} \\
& \components' = \{\component' |\rho[\component] = (\mathit{base},\mathit{bound})
\land \mathit{base} \leq \component' < \mathit{bound}\} \\
\end{split}\]

We then tweak the call operation to seal only objects that are in \(\mathit{capped}\), or
the previously defined \(\mathit{passed}\).

\judgmentbrbrbr[]
               {\(\psi = \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa}\)}
               {\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
                 \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
               {\(\components = \{\addr | V' ~ \addr = \object \land \addr \not \in (\mathit{passed} ~ \overline{sa} ~ \mach) \cup (\mathit{capped} ~ \overline{\reg_{args}} ~ \rho) \}\)}
               {\(V'' = V'\llbracket \components \mapsto \sealed \rrbracket\)}
               {\(Op ~ \mach ~ \psi ~ (V,\sigma,\rho) =
                 (V'',V::\sigma,\rho)\)}

Note that we have a degree of monotonicity here. Once an object is sealed (because its
capability has not been passed to a callee), subsequent nested calls can never unseal it.
On the other hand, an object that is passed via a pointer may be passed on indefinitely.

\end{document}
