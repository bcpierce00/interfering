%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[10pt,conference]{ieeetran}%\settopmatter{printfolios=true,printccs=false,printacmref=false}

\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption
\usepackage{amsmath,amsthm,amssymb}

\usepackage[table]{xcolor}

\usepackage{threeparttable}

\usepackage{wasysym}

\usepackage{listings}

\usepackage{tikz}

\usepackage{array,multirow}

\usepackage{stmaryrd}

\usepackage[noadjust]{cite}

\usepackage[multiple]{footmisc}

\usepackage[hyphens]{url}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\input{macros}

\makeatletter
\newcommand{\linebreakand}{%
  \end{@IEEEauthorhalign}
  \hfill\mbox{}\par
  \mbox{}\hfill\begin{@IEEEauthorhalign}
}
\makeatother

\begin{document}

%% Title information
\title{Formalizing Stack Safety as a Security Property}

\author{
  \IEEEauthorblockN{
    Sean Noble Anderson
  }
  \IEEEauthorblockA{
    Portland State University\\
    ander28@pdx.edu\\
  }
  \and
  \IEEEauthorblockN{
    Roberto Blanco
  }
  \IEEEauthorblockA{
    Max Planck Institute for Security and Privacy\\
    roberto.blanco@mpi-sp.org\\
  }
  \and
  \IEEEauthorblockN{
    Leonidas Lampropoulos
  }
  \IEEEauthorblockA{
    University of Maryland, College Park\\
    leonidas@umd.edu\\
  }
  \linebreakand
  \IEEEauthorblockN{
    Benjamin C. Pierce
  }
  \IEEEauthorblockA{
    University of Pennsylvania\\
    bcpierce@cis.upenn.edu\\
   }
  \and
  \IEEEauthorblockN{
    Andrew Tolmach
  }
  \IEEEauthorblockA{
    Portland State University\\
    tolmach@pdx.edu\\
  }
}

%% Keywords
%% comma separated list
\ifcameraready
\keywords{Stack Safety, Micro-Policies}  %% \keywords are mandatory in final camera-ready submission
\fi

\maketitle

\begin{abstract}

The term {\em stack safety} is associated with a
variety of compiler, run-time, and hardware mechanisms for protecting stack
memory. But, unlike ``the heap,'' the stack does not correspond to a single
high-level language concept. Rather, it corresponds to the fundamental
abstraction of functions, in all of the forms that they can take.
The protean nature of the functional abstraction makes stack safety
difficult to specify.

We propose a formal characterization of stack safety based on
concepts from language-based security. Stack safety is decomposed
into an integrity property and a confidentiality property for each
of the caller and the callee, as well as a control-flow property.

Our motivating enforcement mechanism,
the ``lazy'' stack safety micro-policies proposed by
Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}, permit functions
to write into one another's frames, but taint the result so that the frame's
owner cannot access it. No existing characterization of stack safety
captures this style of safety. We capture it by defining the properties in
terms of the observable behavior of the system.

The stack interacts with a large number of language features that are often excluded
from discussion of stack safety. Our properties support a system with
both caller- and callee-saved registers, arguments passed on the stack,
tail-call elimination, \ifexceptions exceptions \fi. They are modular by
design, in order to be further extensible to other features.

We validate our properties by using them to distinguish correct implementations
of Roessler and DeHon's micro-policies from incorrect ones
via property-based random testing. Our testing successfully detects violations
in several broken variants, including Roessler and DeHon's original lazy policy.
A fixed version of the policy does pass our tests.

\end{abstract}

\newcommand{\paragraphx}[1]{\emph{#1.}}

\section{Introduction}
\label{sec:intro}

\subsection{The Call Stack and Its Security}

Computer programming relies on functions (or subroutines, procedures, methods, etc.)
as a foundational abstraction. Functions are units of computation that
can call one another to realize larger computations in a modular fashion.
%
At a low level, a function activation manages its own data, such as local variables, as
well as information about its \emph{caller} function, to which it must return.
The \emph{(call) stack} is the fundamental data structure charged with realizing
this abstraction, aided by an ABI that defines how registers are shared between activations.
From a security perspective, attacks on the stack are really attacks
on the fundamental functional abstraction itself, and are therefore every bit as
pervasive.

Indeed, the call stack is an ancient~\cite{one06phrack} and
perennial~\cite{mitre-cwe,DBLP:conf/raid/VeendCB12,
  DBLP:conf/sp/SzekeresPWS13,
  DBLP:conf/sp/HuSACSL16,msrc-bluehat,chromium-security}
target for low-level attacks, frequently involving control-flow hijacking via corrupting
the return address, and memory corruption more generally.

%
%The most recent release of the authoritative CWE Top 25
%Most Dangerous Software Weaknesses~\cite{
%list\footnote{\url{https://cwe.mitre.org/top25/archive/2022/2022_cwe_top25.html}}
%shows that general classes of vulnerabilities closely related to the stack
%% \rb{(though not exclusively)}
%consistently rank at and near the top of the list, e.g., \#1 Out-of-bounds Write, \#5 Out-of-bounds Read, and \#7 Use After Free.
%
%Moreover, many other types of faults involve some kind of control or data flow
%using data stored in the stack 
The variety in attacks on the stack is mirrored by the
software and hardware protections that aim to prevent them,
%
including stack canaries~\cite{Cowan+98},
bounds checking~\cite{NagarakatteZMZ09,NagarakatteZMZ10,DeviettiBMZ08},
split stacks~\cite{Kuznetsov+14},
shadow stacks~\cite{Dang+15,Shanbhogue+19},
capabilities~\cite{Woodruff+14,Chisnall+15,SkorstengaardLocal,SkorstengaardSTKJFP,Georges22:TempsDesCerises},
and hardware tagging~\cite{DBLP:conf/sp/RoesslerD18,Gollapudi+23}.

\apt{Add brief case for having formal guarantees.}
Of these mechanisms, many are fundamentally ill-suited for offering formal guarantees,
as they aim to impede attackers but cannot provide universal protection. Shadow stacks,
for instance, aim to ``restrict the flexibility available in creating gadget chains''
\cite{Shanbhogue+19}, not to categorically
rule out attacks. Other mechanisms such as Softbound\cite{NagarakatteZMZ09} and Code-pointer
integrity\cite{Kuznetsov+14} are stronger, but most do not give formal guarantees.
Indeed, the secure calling conventions of Skorstengaard et al. \cite{SkorstengaardSTKJFP} and
Georges et al. \cite{Georges22:TempsDesCerises} is so far the sole line of work
to offer stack safety formalizations.

Among the mechanisms that should be amenable to making strong formal guarantees,
Roessler and DeHon \cite{DBLP:conf/sp/RoesslerD18} present an array of tag-based
micro-policies~\cite{pump_oakland2015} for stack safety.
Their most realistic micro-policy, ``Lazy Tagging and Clearing'' (\(LTC\)),
makes an interesting trade-off for performance: it allows function activations to write
into one another's stack frames, but ensures that the owner of the corrupted memory cannot access
it afterward. Because the memory does get corrupted, \(LTC\) would not fulfill an
adaptation Georges et al.'s property to the tagged setting.

Neverthess, we believe that there is a useful sense in which \(LTC\) does (attempt to) enforce stack safety. 
It does not always protect individual addresses on the stack, but it tries to 
prevent one function activation from interfering with another's behavior---protecting 
the functional abstraction, which we argue is the real essence of stack safety.

In this work, we propose a formal characterization of stack safety based on the idea
of protecting each function activation from other activations.
We use the formal tools of language-based
security~\cite{sabelfeld2003language}, decomposing stack safety into a family of
properties describing
the {\em integrity} and {\em confidentiality} of the callerâ€™s local state
and the callee's behavior during the callee's execution, plus the control-flow protection
of {\em well-bracketed control flow} (\(\wbcf\))~\cite{SkorstengaardSTKJFP}.
While our properties are motivated by a desire to specify \(LTC\), 
they are built from first principles in the hopes that they will be broadly applicable
to a range of enforcement mechanisms.

The structure of our properties enables them to be easily expanded to characterize systems with
sophisticated features such as callee-save registers, argument-passing on the stack,
and tailcall-elimination. We likewise extend \(LTC\) to protect these features, and
test the extended micro-policy against our property. We find that, apart from the extensions, \(LTC\)
is flawed in a way that undermines integrity and confidentiality, and we correct
this flaw.

\paragraph*{Towards Formal Properties for Stack Safety}

Security mechanisms can be brittle, successfully eliminating one attack while leaving room for
other, similar attacks. To avoid this, we seek formal properties of safe behavior that can be proven
or tested. Such properties become the specification against which enforcement can be validated;
even enforcement mechanisms that do not fulfill a property benefit from the ability to articulate
\emph{why} they fail.

Security properties should be motivated by security requirements
and flexible enough to support various sets of security requirements depending on the
goals of the system that they validate.
Following this reasoning, our vision of stack safety is as a collection of properties
describing the security needs of a system with regard to its functional abstractions.
While the stack still plays a key role, our stack safety is about functions. The most
dramatic consequence of this is that we treat registers as part of a function activation's
state, and hold them to the same protection requirements as stack memory.

Flexibility is especially important for stack safety.
%
Most security properties are described at the level of a high-level programming language,
and translated to the target-level via a secure compiler. Stack safety cannot
be defined in this way, as the abstraction is completely transparent in most source languages.
(Contrast Azevedo de Amorim et al.'s \cite{DBLP:conf/post/AmorimHP18} work on heap safety,
which is broadly consistent across language semantics, and therefore
amenable to a very general high-level approach.)
%
But it is not satisfying as a low-level property; at the level of the ABI, the
specification of a well-behaved stack is minimal. The ABI is not concerned with such
questions as whether a caller's frame should be readable or writable to its callee,
leaving those concerns to be determined by a high-level language built atop the stack.
These are high-level security questions.

For instance, the stack-safety specification from Georges et al.
\cite{Georges22:TempsDesCerises} supposes a machine with only straightforward call-and-return
control flow, and therefore defines {\em Well-bracketed Control Flow} (\(\wbcf\)) under the assumption that
a callee should always return to its caller. But, in the presence of tail-call elimination, a
return from a callee to a distant ancestor is perfectly reasonable. Similarly, the preceding work by
Skorstengaard et al. \cite{SkorstengaardSTKJFP} made no mention of confidentiality, even though
their enforcement mechanism did protect the confidentiality of the caller (though not the callee).

These two cases represent two dimensions in which a stack safety definition requires
flexibility: what constitutes safe behavior differs between systems that support different
features, and different contexts have different top-level security requirements.

We are not attempting to present a universal definition of stack safety for all systems,
but we do construct our definition with these dimensions of flexibility in mind.
Our model is decomposed into:
(1) a lightweight model of the system and its security-relevant features, from which we
derive the integrity, confidentiality, and control-flow requirements of each call and
return, (2) the application of that model to a particular target system, and (3)
the formal criteria for the integrity, confidentiality, and control flow
requirements.

We show that we can extend our system to new features by first introducing a simple
example model, and then moving to a more realistic model with multiple new features:
callee-save registers, argument passing on the stack, and tailcall elimination, simply
by refining a few definitions in (1). This becomes the model that we validate.
In \cref{app:ptr}, we extend the model further with arbitrary sharing of memory-safe
pointers. Our criteria in (3) cover a total of five core requirements:
\(\wbcf\), caller integrity (\(\clri\)), caller confidentiality (\(\clrc\)),
callee integrity (\(\clei\)), and callee confidentiality (\(\clec\)).
They are related to one another as seen in
\cref{fig:propertydiagram}: \(\clei\) subsumes \(\clrc\) and \(\clec\) subsumes
\(\clri\), but otherwise the properties are orthogonal and can be taken piecemeal
according to the needs of a system.

In particular, many enforcement techniques focus purely on
well-bracketed control flow. Stack canaries aim to prevent certain attacks on the return
address, and shadow stacks with protection (e.g. Return Address Defender \cite{Chiueh2001RAD})
to enforce it completely. Others combine this protection with some degree of memory protection,
chiefly focusing on integrity. Interestingly, Skorstengaard et al. \cite{SkorstengaardSTKJFP}
describe their ``local state encapsulation'' in terms of integrity, but it is equivalent
to the combination of \(\clri\) and \(\clrc\). The follow-up work by 
Georges et al. \cite{Georges22:TempsDesCerises} cites an early preprint of our work
as inspiration for extending their formal discussion to include confidentiality.
When checking if a preliminary version of these properties applied to
the Cerise calling convention, they noted that the Skostengaard et al. calling convention
did not enforce callee confidentiality, and made sure that their version would
\cite{Georges22:personalcommunication}, in addition to building it into their formalism.
This demonstrates the benefit of our choice to explicitly state properties in security
terms: specifying security mechanisms is hard, and when the specification takes the
form of a ``correct by construction'' machine, it is easy to neglect a non-obvious
security requirement.

%These are defined
%formally in \cref{sec:props}. . We further define two
%important combinations of properties: full stack safety \(\textnormal{\sc FSS}\),
%and caller safety \(\textnormal{\sc ClrS}\). Of these, most existing concepts of
%stack safety correspond to either \(\wbcf\) or \(\textnormal{\sc ClrS}\), which are
%concerned with the basic guarantees that the caller recieves during the call, while
%\(\textnormal{\sc FSS}\) is a natural extension that enforces a higher-level concept
%of a correct interface between caller and callee. \sna{Blah. Needs work.}

Flexibility aside, we show that our properties are useful for their purpose:
distinguishing correct enforcement from incorrect. We use
QuickChick~\cite{Denes:VSL2014,Pierce:SF4}, a property-based testing
tool for Coq, to generate random programs and check that the
stack safety {\em micro-policies} of Roessler and DeHon~\cite{DBLP:conf/sp/RoesslerD18}
correctly detect the ones that attempt to violate one of our properties.

%Furthermore, we
%check that the testing framework is able to generate counterexamples
%that violate our properties but are \emph{not} halted by incorrect
%variants of the enforcement mechanisms---including the original \(LTC\)!
% SNA: we talk about this in the testing section, and it's a "best practice" for
% PBT, but not really a contribution in and of itself

\subsection{Contributions}

In sum, we offer the following contributions:

\begin{itemize}
\item A novel characterization of stack safety as a collection of properties:
  confidentiality and integrity for callee and caller, and well-bracketed
  control-flow. These can characterize the security requirements of systems
  that support only some portion of stack safety, and are
  parameterized over a notion of external observation, so that they can characterize
  lazy enforcement.
\item An instantiation of these definitions in a realistic
  setting with features such as argument passing
  on the stack, callee-saves registers, and tail-call elimination.
  Our model is modular enough that handling these features is straightforward.
\item The validation, via property-based randomize testing, of tag-based
  stack-safety policies. We find that \emph{Lazy Tagging and Clearing}
  falls short of stack safety, and propose a fix.
\end{itemize}

Our artifact contains Coq formalizations of our properties for illustrative purposes
as well as their testing implementation. There are no proofs---we use Coq primarily
for the QuickChick library and to ensure that the formalizations are unambiguous.
Proofs are future work.

\subsection{Threat Model and Limitations}

When our properties are used to evaluate a system, the threat model will depend on the
details of that system. However, there are some constraints that our design puts on
any system. We must trust that the method of distinguishing security-relevant operations is accurate; if it
involves labels placed on code by a compiler, that means trusting that the compiler placed
those labels correctly. If operations occur that are not recognized, those operations
might not be guaranteed to protect their principals---for instance, an unlabeled call
might not protect the caller's data from the callee. On the flip side, applying an incorrect
label most likely means that the property becomes too strong to be enforced.

Otherwise, we do not assume that the code adheres to any particular
calling convention or implements a source language construct.
In particular, while we are agnostic as to the source
language, we certainly aim to support C, and so any source function might contain undefined
behavior resulting in its compilation to arbitrary machine code. A given enforcement
mechanism or target architecture might place additional constraints, particularly on the behavior of
call and return sequences. For instance, extant implementations tend to assume
implicitly that callee-saved registers have their values maintained by whichever compiler
generated their code. Our properties explicitly state this as a requirement,
which could be enforced by a micro-policy, a well-behaved compiler, or other enforcement technique.

In general, it is impossible to distinguish buggy machine code from an attacker; in
our examples we will identify one function or another as an attacker, but we do not
require any static division between trusted and untrusted code, and we aim to protect
even buggy code.

This is a strong threat model, but it does omit some important aspects of
security enforcement: in particular, hardware and timing attacks are out of scope, and
we do not address concurrency---that is a high-priority future work.
%

\section{Key Ideas}
\label{sec:ideas}

\apt{Not a good place for this.}
The paper is structured as follows. In \cref{sec:example}, we will walk through a function
call in a simple example machine and discuss how each of our properties applies to it,
informally. In the process we discuss some motivation for the properties, from a security
perspective. We consider a simple attacker, and discuss the harm that it can do to
its caller. This perspective is essential to understanding how lazy enforcement can
be safe: even if an attacker can overwrite some memory, if it cannot accomplish
subvert the observable behavior of the caller, the system is safe.

In \cref{sec:machine} we formalize the machine model, 
its {\em security semantics}, and the stack safety properties built on these. 
\Cref{sec:testing} describes the testing framework, and
\cref{sec:relwork,sec:future} discuss related and future work.
The remainder of this section introduces ideas that will be important throughout.

\paragraph*{Building a Property}

Our properties follow a three-part structure that we term a {\em security semantics}.
First we define the {\em security-relevant operations} that we support, and an extra
piece of state called a {\em security context}, which updates alongside each step
of execution. The security context gives each function activation a {\em view},
mapping each element to a {\em security class}.

Then, we define our high-level security requirements---integrity, confidentiality,
and well-bracketed control flow---in terms of the security classes of state
elements. These take the form of predicates on states and contexts that use variants,
assertions about future return states, and comparison of event traces to capture the
desired behavior without reference to implementation details.

Separately, in order to apply a property to a specific machine, we do need to consider
implementation details, by determining how the machine's execution translates into
security relevant operations. But this step is transparent to the properties themselves.

\apt{Something about how the machine can incorporate enforcement, with examples
(tags, capabilities, SW only). }

\paragraph*{Security Semantics and Property Structure}

A security extends a machine
with additional context about the identity of current and pending
functions (which act as security principals) and about the registers and memory they require
to be secure. This added context is purely notional;
it does not affect the real machine. The security context
evolves dynamically through the execution of security-relevant operations,
which include events like calls, returns, and frame manipulations.
Our security properties are phrased in terms of this context, often as predicates
on future states, e.g. of the form ``when control returns to the current function...'',
or relations on traces of future execution (hyper-properties).

The security-relevant operations typically correspond to underling machine instructions,
but the correspondence may not be obvious just by inspecting the machine code.
For example, in the tagged RISC-V machine we use in our examples and tests,
calls and returns are conventionally performed by {\tt jal} (``jump-and-link'')
and {\tt jalr} (``jump-and-link-register'') instructions, respectively, but these
instructions might also be used for other things. An instantiation of an operation
might require some external mechanism for distinguishing which instructions are
security-relevant. The most obvious technique, which we use in our examples, is to add labels to
selected instructions, but the theory does not depend on this choice.

Whatever the mechanism for labeling transitions, we get a base
machine transition \(\mach \xrightarrow{\bar{\psi}, \obs} \mach'\), where \(\obs\) is
an event (see below) and \(\bar{\psi}\) is a list of security-relevant operations.
%We then lift this into a transition between pairs of machine states and contexts,
%\((\mach,\context) \stepstounder{\overline{\psi},\obs} (\mach', \context')\).

The set of security-relevant operations (\(\Psi\)) covered in this paper is given in
\cref{tab:psi}. Those that we explore in detail in \cref{sec:example}
are shown in white: calls, returns, and the allocation and deallocation of local memory.
In \cref{sec:machine}), we cover additional operations that add parameters to the examples,
to support various forms of memory sharing \ifexceptions , exceptions, \fi
and tail-call elimination. These are shown in light gray.
We give a proof-of-concept formalization of a more sophisticated, heap-like sharing model
based on pointer provenance in \cref{app:ptr}, represented by the dark grey
operations, but we do not test this.

\newcommand{\example}{\rowcolor{black!0}}
\newcommand{\testing}{\rowcolor{black!10}}
\newcommand{\theory}{\rowcolor{black!25}}

\begin{table}
\begin{center}
  \begin{tabular}{| l | l |}
    \hline
    \(\psi \in \Psi\) & Parameters \\
    \hline
    \example \(\mathbf{call}\) & target address, argument registers \\
    \testing & stack arguments (base, offset \& size) \\
    \example \(\mathbf{return}\) & \\
    \example \(\mathbf{alloc}\) & offset \& size \\
    \testing & public flag \\
    \example \(\mathbf{dealloc}\) & offset \& size \\
    \testing \(\mathbf{tailcall}\) & (same as for \(\mathbf{call}\)) \\
    \hline
    \multicolumn{2}{|c|}{{\it Pointer provenance operations}} \\
    \hline
    \theory \(\mathbf{promote}\) & register, offset \& size \\
    \theory \(\mathbf{propagate}\) & source register/address \\
    \theory & destination register/address \\
    \theory \(\mathbf{clear}\) & target register/address \\
    \hline
  \end{tabular}
\end{center}
  \caption{Security-relevant Operations and their Parameters}
  \label{tab:psi}
\end{table}

Our security semantics has similarities to the overlay semantics that have been proposed
to characterize stack safety in the past~\cite{SkorstengaardSTK}, but it does not restrict
the behavior of the underlying machine in any way. Rather, it tracks additional context
about the history of security-relevant operations, which inform the criterion a machine
must satisfy in order to correctly obey the properties.

The security principals in our account of stack safety are function
activations: the context tracks, for the active function and all pending
functions, their {\em view} of the security state of the system.

\paragraph*{Views}

The security context consists of a stack of views: functions that map
each state element to a {\it security class}. The security classes are
\(\public\), \(\unsealed\), \(\object\), and \(\sealed\).

State elements that are outside of the stack---general-purpose memory used for
globals and the heap, as well as the code region and globally-relevant
registers---are always labeled \(\public\). We place security requirements on some
\(\public\) elements for purposes of \(\wbcf\), and a given enforcement mechanism
might restrict their access (e.g., by rendering code immutable) but for integrity
and confidentiality purposes they are considered accessible at all times.

For a newly active function, every element that is available for use but uninitialized
is seen as \(\unsealed\). From the perspective of the caller, the callee has no obligations
regarding its use of \(\unsealed\) elements, but the callee itself \apt{???}

Arguments are seen as \(\object\), meaning that their contents may be safely used.
When the active function allocates memory, that memory will also be \(\object\).
Then, on a call, \(\object\) elements that are not being used to communicate with
the new callee will become \(\sealed\)---reserved for an inactive principal,
and expected to be unchanged when it becomes active again.

\paragraph*{Events}

It is common to specify the behavior of a system, including security properties, in
terms of traces of observable {\em events} rather than details of the machine state.
Lazy enforcement works because the deferred check catches errors before they appear
in the trace. The nature of events is a parameter of the machine, and can be specialized
to any notion of observable behavior.

\paragraph*{Variants and Non-interference}

To characterize confidentiality, we borrow the idea of \emph{variant} states
from the theory of non-interference. Non-interference is a way of expressing
the flow of information through the system. If we begin with a state \(\mach\)
and modify it to \(\nach = \mach[\component \mapsto v]\) for arbitrary \(v\),
then execute both \(\mach\) and \(\nach\), any differences in the outputs they
produce must be the result of accessing \(\component\).

\section{Properties by Example}
\label{sec:example}

In this section we introduce our security property definitions by means
of small code examples, using a simplified set of security-relevent operations for
or calls, returns, and private allocations.
Figure \ref{fig:main} gives C code and corresponding compiled 32-bit RISC-V code
for a function {\tt main}, which
takes an argument {\tt secret} and initializes a local variable {\tt sensitive} to contain
potentially sensitive data.
Then {\tt main} calls another function {\tt f},
and afterward performs a test on {\tt sensitive} to decide whether
to output {\tt secret}.  Since {\tt sensitive} is initialized to 0,
the test should fail, and {\tt main} should instead output the return value of {\tt f}.
Output is performed by writing to the special global {\tt out},
and we assume that such writes are the only observable events in the system.

The C code is compiled using the standard RISC-V calling conventions~\cite{RISC-V-CC}.
In particular, the first function argument and the function
return value are both passed in {\tt a0}.
Memory is byte-addressed and the stack grows towards
lower addresses. We assume that {\tt main} begins at address 0 and its callee {\tt f} at address 100.

\begin{figure}
  \begin{subfigure}{\columnwidth}
    {\tt
      volatile int out;

      void main(int secret) \{

      ~ ~ int sensitive = 0;

      ~ ~ int res = f();

      ~ ~ if (sensitive == 42)

      ~ ~ ~ ~ out = secret;

      ~ ~ else

      ~ ~ ~ ~ out = res;

      \}}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \begin{tabular}{r l | l}
      \labeledrow{0:}{addi sp,sp,-20}{\(\mathbf{alloc} ~ (-20,20)\)}
      \labeledrow{4:}{sd ra,12(sp)}{}
      \labeledrow{8:}{sw a0,8(sp)}{}
      \labeledrow{12:}{sw zero,4(sp)}{}
      \labeledrow{16:}{jal f,ra}{\(\mathbf{call} ~ \emplist \)}
      \labeledrow{20:}{sw a0,0(sp)}{}
      \labeledrow{24:}{lw a4,4(sp)}{}
      \labeledrow{28:}{li a5,42}{}
      \labeledrow{32:}{bne a4,a5,L1}{}
      \labeledrow{36:}{lw a0,8(sp)}{}
      \labeledrow{40:}{sw a0,out}{}
      \labeledrow{44:}{j L2:}{}
      \labeledrow{L1, 48:}{lw a0,0(sp)}{}
      \labeledrow{52:}{sw a0,out}{}
      \labeledrow{L2, 56:}{ld ra,12(sp)}{}
      \labeledrow{60:}{addi sp,sp,20}{\(\mathbf{dealloc} ~ (0,20)\)}
      \labeledrow{64:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
  \end{subfigure}
  \begin{subfigure}{\columnwidth}
    \includegraphics[width=\columnwidth]{stacklayout.png}
  \end{subfigure}

\caption{Example: C and assembly code for {\tt main}, and layout of its stack frame; addresses increase to the right and the stadk grows to the left.\apt{Needs machine-set diagram.}}
\label{fig:main}
\end{figure}

%Figure \ref{fig:RISCVregs} gives an example division of the RISC-V registers into
%security classes.  APT: Don't think we need this figure.

We now consider how {\tt f} might misbehave and violate desirable
stack safety properties associated with {\tt main}. To put the violations in a
security framework, suppose that {\tt f} is actually an attacker seeking
to leak {\tt secret}. It might do so in a number of ways, shown as snippets of
assembly code in Figure \ref{fig:f}.
%
Leakage is most obviously viewed as a violation of {\tt main}'s {\it confidentiality}.
In Figure \ref{subfig:direct}, {\tt f} takes an offset from the stack
pointer, accesses {\tt secret}, and directly outputs it. But more
subtly, even if somehow prevented from outputting {\tt secret} directly, {\tt f}
can instead return that value so that {\tt main} stores it to {\tt out},
as in Figure \ref{subfig:indirect}.
%
Beyond simply reading {\tt secret}, the attacker might overwrite {\tt sensitive}
with 42, guaranteeing that {\tt main} publishes its own secret unintentionally
(Figure \ref{subfig:integrity}).
Attacks of this kind do not violate {\tt main}'s confidentiality, but
rather its {\it integrity}.
In Figure \ref{subfig:WBCF}, the attacker arranges to return to the
wrong instruction, thereby bypassing the check and publishing {\tt secret} regardless,
violating the program's {\it well-bracketed control flow} (\(\wbcf\).)
%
In Figure \ref{subfig:WBCF2}, a different attack violates \(\wbcf\), this time
by returning to the correct program counter but with the wrong stack pointer.
\footnote{We pad the last two variants with {\tt nop}s just so that all the
snippets have the same length, which simplifies some later explanations.}

\begin{figure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{lw a4,8(sp)}{}
      \labeledrow{104:}{sw a4,out}{}
      \labeledrow{108:}{li a0,1}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \caption{Leaking {\tt secret} directly}
    \label{subfig:direct}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{lw a4,8(sp)}{}
      \labeledrow{104:}{mov a4,a0}{}
      \labeledrow{108:}{sw zero,-4(sp)}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \caption{Leaking {\tt secret} indirectly}
    \label{subfig:indirect}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{li a5,42}{}
      \labeledrow{104:}{sw a5,4(sp)}{}
      \labeledrow{108:}{li a0,1}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking {\tt sensitive}}
    \label{subfig:integrity}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{addi ra,ra,16}{}
      \labeledrow{104:}{nop}{}
      \labeledrow{108:}{nop}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking control flow}
    \label{subfig:WBCF}
  \end{subfigure}
  \begin{subfigure}[b]{\columnwidth}
    \vspace{\abovedisplayskip}
    \begin{tabular}{r l | l}
      \labeledrow{100:}{addi sp,sp,8}{}
      \labeledrow{104:}{nop}{}
      \labeledrow{108:}{nop}{}
      \labeledrow{112:}{jalr ra}{\(\mathbf{return}\)}
    \end{tabular}
    \subcaption{Attacking stack pointer integrity}
    \label{subfig:WBCF2}
  \end{subfigure}

  \caption{Example: assembly code alternatives for {\tt f} as an attacker.
  }
  \label{fig:f}
\end{figure}

The security semantics for this program is based
on the security-relevant events noted in the right columns of Figures~\ref{fig:main}
and~\ref{fig:f}, namely execution of instructions that allocate or deallocate space,
make a call, or make a return.

Our security semantics attach a security context to the machine state,
which consists of a view \(V\) and a stack \(\sigma\) of pending activations' views.
Figure \ref{fig:exec1} shows how the security context evolves over the first few
steps of the program.  (The formal details of the security semantics are described in
\cref{sec:machine}, and the context evolution rules are formalized in \cref{fig:basicops}.)
Execution begins at the start of {\tt main}, where the program counter (\(\PCname\)) is zero,
and with the stack pointer (\(\SP\)) at address 1000.
State transitions are numbered and labeled with a list of security operations, written
\(\downarrow \overline{\psi}\) between steps.

The initial view \(V_0\) maps all stack addresses below \(\SP\) to \(\unsealed\) and the remainder of
memory to \(\public\). The sole used argument register, {\tt a0}, is mapped to \(\object\);
other caller-save registers are mapped to \(\unsealed\) and callee-save registers to \(\sealed\).
Step 1 allocates a word each for {\tt secret}, {\tt sensitive}, and {\tt res}, as well
as two words for the return address. This has the
effect of marking those bytes \(\object\).
(We use \(V\llbracket\cdot\rrbracket\) to denote updates to \(V\).)
%

\newcommand{\freebox}{\tikz \filldraw[fill=blue] (0,0) rectangle (10px,10px);}
\newcommand{\pubbox}{\tikz \filldraw[fill=lightgray] (0,0) rectangle (10px,10px);}
\newcommand{\objbox}{\tikz \filldraw[fill=yellow] (0,0) rectangle (10px,10px);}
\newcommand{\sealbox}{\tikz \filldraw[fill=red] (0,0) rectangle (10px,10px);}

\begin{figure*}
  \begin{tabular}{|r|r||l|r}
    \cline{1-3}
    \(\PCname\) & \(\SP\) & Context &
    \multirow{3}{*}{\(\underbrace{\dots \freebox \freebox \freebox \freebox \freebox
        \freebox \freebox \freebox \freebox \freebox}_\unsealed
      \! \underbrace{\stackrel{\stackrel{\SP}{\downarrow}}{\pubbox} \!\! \pubbox \pubbox \dots}_\public
      ~ \stackrel{\mathtt{a0}}{\pubbox\pubbox} ~ \stackrel{\mathtt{a1}}{\freebox\freebox}
      ~ \stackrel{\mathtt{s0}}{\sealbox\sealbox}
      \)} \\
    \cline{1-3}
    0 & 1000 & \(V_0, \emplist\)
    \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{\(1 \Big\downarrow [\mathbf{alloc} ~ (-20,20)]\)}} & \\
    \multicolumn{3}{l}{} &
    \multirow{3}{*}{\(\underbrace{\dots \freebox \freebox \freebox \freebox \freebox}_\unsealed
      \! \underbrace{\stackrel{\stackrel{\SP}{\downarrow}}{\objbox} \!\! \objbox \objbox \objbox \objbox}_\object
      \! \underbrace{\pubbox \pubbox \pubbox \dots}_\public
      ~ \stackrel{\mathtt{a0}}{\pubbox\pubbox} ~ \stackrel{\mathtt{a1}}{\freebox\freebox}
      ~ \stackrel{\mathtt{s0}}{\sealbox\sealbox}
      \)}
    \\
    \cline{1-3}
    4 & 980 & \(V_1 = V_0 \llbracket 980..999 \mapsto \object\rrbracket, \emplist\) &
    \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{2-4 \(\Big\downarrow \emplist\)}} \\ \multicolumn{3}{l}{} \\
    \cline{1-3}
    16 & 980 & \(V_1, \emplist\) & \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{\(5 \Big\downarrow [\mathbf{call} ~ 100 ~ \emplist]\)}} & \\
    \multicolumn{3}{l}{} &
    \multirow{3}{*}{\(\underbrace{\dots \freebox \freebox \freebox \freebox \freebox}_\unsealed
      \! \underbrace{\stackrel{\stackrel{\SP}{\downarrow}}{\sealbox} \!\! \sealbox \sealbox \sealbox \sealbox}_\sealed
      \! \underbrace{\pubbox \pubbox \pubbox \dots}_\public
      ~ \stackrel{\mathtt{a0}}{\freebox\freebox} ~ \stackrel{\mathtt{a1}}{\freebox\freebox}
      ~ \stackrel{\mathtt{s0}}{\sealbox\sealbox}
      \)}
    \\
    \cline{1-3}
    100 & 980 & \(V_2 = V_1 \llbracket 980..999 \mapsto \sealed, \mathtt{a0} \mapsto \unsealed\rrbracket,[V_1]\) & \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{6-8 \(\Big\downarrow \emplist\)}} \\ \multicolumn{3}{l}{} \\
    \cline{1-3}
    112 & 980 & \(V_2,[V_1]\) \\
    \cline{1-3}
    \multicolumn{3}{l}{\multirow{2}{*}{\(9 \Big\downarrow [\mathbf{return}]\)}} & \\
    \multicolumn{3}{l}{} & \multirow{3}{*}{\(\underbrace{\dots \freebox \freebox \freebox \freebox \freebox}_\unsealed
      \! \underbrace{\stackrel{\stackrel{\SP}{\downarrow}}{\objbox} \!\! \objbox \objbox \objbox \objbox}_\object
      \! \underbrace{\pubbox \pubbox \pubbox \dots}_\public
      ~ \stackrel{\mathtt{a0}}{\pubbox\pubbox} ~ \stackrel{\mathtt{a1}}{\freebox\freebox}
      ~ \stackrel{\mathtt{s0}}{\sealbox\sealbox}
      \)}
    \\
    \cline{1-3}
    20 & 980  & \(V_1, \emplist\) &
    \\
    \cline{1-3}
    \multicolumn{2}{l}{} \\
  \end{tabular}
  \caption{Execution of example up through the return from {\tt f}. In stack diagram, addresses increase to the right, stack grows to the left, and boxes represent 4-byte words.}
\label{fig:exec1}
\end{figure*}
%
At step 5, the current principal's record is pushed onto the inactive list.
Its return target is the return address of the call,
and the stack pointer target is the stack pointer at the moment of call.
The callee's view is updated from the caller's such that all \(\object\) memory locations
become \(\sealed\). (For now we assume no sharing of memory between activations; data is
passed only through argument registers, which remain active. In the presence of memory
sharing, some memory would remain active, too.)
Function {\tt f} does not take any arguments; if it did, any registers containing them would be 
mapped to \(\object\), while any non-argument, caller-saved 
registers are mapped to \(\unsealed\). In the current example, only register {\tt a0} has a change in
security class. All callee-save registers remain \(\sealed\) for all calls.
Thus, if we varied the assembly code for the example slightly so that {\tt sensitive} was stored
in a callee-save register (e.g. {\tt s0}) rather than in memory, its security class would still be \(\sealed\)
at the entry to {\tt f}.
%
At step 9, {\tt f} returns, and the topmost inactive view, that of {\tt main}, is restored.

We now show how this security semantics can be used to define notions of confidentiality,
integrity, and correct control flow in such a way that many classes of
bad behavior, including the attacks in Figure~\ref{fig:f}, are
detected as security violations.

\paragraph*{Well-bracketed Control Flow}

To begin with, what if {\tt f} returns to an unexpected place (i.e. \(\PCname \neq 20\) or
\(\SP \neq 980\))? We consider this to violate \(\wbcf\). \(\wbcf\) is a relationship between
call steps and their corresponding return steps: just after the return, the program
counter should be at the next instruction following the call,
and the stack pointer should be the same as it was before the call.
Both of these are essential. In Figure \ref{subfig:WBCF}, the attacker adds
16 to the return address and then returns, thus bypassing the {\tt if}-test in the code and outputting
{\tt secret}.
In Figure \ref{subfig:WBCF2}, the attacker returns with \(\SP' = 998\) instead of the
correct \(\SP = 980\). In this scenario, given the layout of {\tt main}'s frame,
\begin{center}
\begin{tabular}{| l | l | l | l | l |}
  \multicolumn{1}{r}{\(\SP \downarrow\)} &
  \multicolumn{2}{r}{\(\SP' \downarrow\)} \\
  \hline
  res & sens & sec & ra & ra \\
  \hline
\end{tabular}
\end{center}

\vspace{\abovedisplayskip}

\noindent
{\tt main}'s attempt to read {\tt sensitive} will instead
read part of the return address, and its attempt to output
{\tt res} will instead output {\tt secret}.

Before the call, the program counter is 16 and the stack pointer is 980.
So we define a predicate on states that should hold just after the return:
\(\ret\ \mach \triangleq \mach[\PCname] = 20 \wedge \mach[\SP] = 980\).
%
We can identify the point just after the return (if a return occurs)
as the first state in which the pending call stack is smaller than it was
just after the call.
\(\wbcf\) requires that if \(\mach\) is the state at that point, then \(\ret ~ \mach\) holds.
%For nested calls, where the pending stack is initially larger, the same principle
%applies: \(\ret ~ \mach\) must hold the next time the pending stack is the same size or smaller.

% Even absent other
% kinds of data protection, the stack pointer {\it must} be restored
% for the program to behave predictably.

\paragraph*{Stack Integrity}

Like \(\wbcf\), stack integrity defines a condition at the call that must hold upon
return. This time the condition applies to all of the memory in the function's
frame. In Figure \ref{fig:exec1} we see the lifecycle of an allocated frame:
upon allocation, the view labels it \(\object\), and when a call is made, it instead
becomes \(\sealed\). Intuitively, the integrity of {\tt main}
is preserved if, when control returns to it, any \(\sealed\) elements
are identical to when it made the call.
%
Again, we need to know when a caller has been returned to,
and we use the same mechanism of checking the depth of the call stack.
%
In the case of the call from {\tt main} to {\tt f}, the \(\sealed\) elements are the
addresses 980 through 999 and callee-saved registers such as
the stack pointer. Note that callee-saved registers often change
during the call---but if the caller accesses them after the call, it should find them
restored to their prior value.

While it would be simple to define integrity as ``all sealed elements retain their
values after the call,'' this would be stricter than necessary. Suppose that
a callee overwrites some data of its caller, but the caller never accesses that data
(or only does so after re-initializing it.) This would be harmless, with the callee
essentially using the caller's memory as scratch space, but the caller never seeing any change.

For a set of elements \(\components\),
a pair of states \(\mach\) and \(\nach\) are {\em \(\components\)-variants} if
their values an only disagree on elements in \(\components\).
We say that the elements of \(\components\) are \emph{irrelevant}
in \(\mach\) if they can be replaced by arbitrary other values without changing the
observable behavior of the machine. All other elements are \emph{relevant}.

(This story is slightly over-simplified. If an enforcement mechanism maintains
additional state associated with elements, such as tags, we don't want that
state to vary; only the payloads should. Formal definitions of variants and
relevance that incorporate this wrinkle are given in \cref{sec:props}.)

We define \emph{caller integrity} (\(\clri\))  as the property that
every relevant element that is \(\sealed\) under the callee's view is restored
to its original value at the return point.

\begin{figure}
  \includegraphics[width=\columnwidth]{variants.png}
  \caption{Integrity Violation\apt{Needs a machine-drawn version, as do the rest. Use colors consistently. No tags!}}
  \label{fig:variant}
\end{figure}

In our example setting, the observation trace consists of the sequence
of values written to {\tt out}.
The example in Figure \ref{subfig:integrity} modifies the value of {\tt sensitive},
which is \(\sealed\). Figure \ref{fig:variant} shows the state just after the call at step 5,
assuming that {\tt sec} is 5. Similar to \(\wbcf\), we define
\(\intProp\) as a predicate on states that holds if
all relevant sealed addresses in \(\mach\) are the same as after step 5.
We require that \(\intProp\) hold on the state following the matching return,
which is reached by step 9. Here {\tt sensitive} has obviously changed, but is it relevant?
Consider a variant state in which {\tt sensitive} has any other value, arbitrarily
choosing 43. As execution continues after the return
from the original state, it passes the {\tt if}-test on {\tt sensitive}, whereas the execution
from the variant does not, resulting in differing outputs. Therefore {\tt sensitive} \emph{is} relevant,
so \(\intProp\) does not hold, and integrity has indeed been violated.

\paragraph*{Caller Confidentiality}

We treat confidentiality as a form of non-interference as well: the confidentiality of a caller
means that its callee's behavior is dependent only on publicly visible data,
not the caller's private state. This also requires that the callee initialize
memory before reading it.
As we saw in the examples, we must consider both the observable events
that the callee produces during the call and the changes that the callee makes to the state that might
affect the caller after the callee returns.

Consider the state \(\mach\) after step 5, whose context is \((V_2,\sigma)\), with the attacker from
Figure \ref{subfig:direct}. We take a variant state over the set of element(s) that are
\(\sealed\) in \(V_2\) (see Figure \ref{fig:variant2}.)
            [TODO: fix figure to avoid varying the entire stack.]
If we take a trace of execution from each state until it returns,
the traces may differ, in this case outputting 5 (the original value of {\tt secret}) and
4 (its value in the variant) respectively. This is a violation of
{\it internal confidentiality} (formalized in \cref{tab:props}, line 3a).

\begin{figure}
  \includegraphics[width=\columnwidth]{variants2.png}
  \caption{Internal Confidentiality Violation}
  \label{fig:variant2}
\end{figure}

But, in Figure \ref{subfig:indirect}, we also saw an attacker that exfiltrated the secret
by reading it and then returning it, in a context where the caller would output the returned
value. Figure \ref{fig:variant3} shows the behavior of the same variants under this attacker,
but in this case, there is no output during the call. Instead the value of {\tt secret} is
extracted and placed in {\tt a0}, the return value register. We wish to identify this as
a confidentiality violation, again by considering variants of the \(\sealed\)
elements in \(V_2\), but capturing the required property is subtle.

To illustrate the issues, note that {\tt f} has also stores a 0 below the stack pointer.
%
\begin{figure}
  \includegraphics[width=\columnwidth]{variants3.png}
  \caption{Return-time Confidentiality Violation}
  \label{fig:variant3}
\end{figure}
%
Now consider three elements: the address \(\SP - 4\), the address \(\SP + 12\),
and the register {\tt a0}. The execution from the right-hand state to its return
has changed the value at \(\SP - 4\), but that value matches that of the
left-hand variant at its return. Therefore, the change does not represent a leak.

The return states disagree on the value of \(\SP + 12\). But in neither
case has that value changed since the original variants. So the difference is inherited from
the original variation, and does not represent a leak either. We do not continue executing the
variant state after return, so these values will not cause the caller to behave differently.

But in the case of {\tt a0}, the value has changed during the call (in both the original
and the variant, although only one of these would be necessary), and its final value
differs between the variants.
Therefore, it must depend on a secret (in fact, the variable {\tt secret}).
Unless {\tt a0} happens to be irrelevant to the caller, this is a violation of what
we term {\it return-time confidentiality} (formalized in \cref{tab:props}, line 3b).

Structurally, return-time confidentiality resembles integrity, but now dealing with
variants. We begin with a state immediately following
a call, \(\mach\). We consider an arbitrary variant state,
\(\nach\), which may vary any element that is \(\sealed\) or \(\unsealed\),
i.e., any element that is not used legitimately to pass arguments. Caller confidentiality
therefore can be thought of as the callee's insensitivity to elements in its initial state
that are not part of the caller-callee interface.

We define a binary relation \(\confProp\) on pairs of states,
which holds on eventual return states \(\mach'\) and \(\nach'\)
if all relevant elements are {\em uncorrupted} relative to \(\mach\) and \(\nach\).
An element is {\em corrupted} if it differs between \(\mach'\) and \(\nach'\),
and it either changed between \(\mach\) and \(\mach'\) or between \(\nach\) and \(\nach'\).

Finally, we define \emph{caller confidentiality} (\(\clrc\)) as the
combination of internal and return-time confidentiality (\cref{tab:props}, line 3).

\paragraph*{The Callee's Perspective}

We presented our initial example from the perspective of the caller, but a callee
may also have privilege that its caller lacks, and which must be protected from the
caller. Consider a function that makes a privileged system call to obtain a secret key,
and uses that key to perform a specific task. An untrustworthy or erroneous caller might
attempt to read the key out of the callee's memory after return, or to influence the callee
to cause it to misuse the key itself!

Where the caller's confidentiality and integrity are concerned with protecting specific,
identifiable state---the caller's stack frame---their callee equivalents are concerned
with enforcing the expected interface between caller and callee. Communication between
the principals should occur only through the state elements that are designated for the
purpose: those which we label \(\public\) and (in the presence of memory sharing) \(\object\).

Applying this intuition using our framework, \(\clec\) turns out to resemble
\(\clri\), extended to every element that is not marked \(\object\) at call-time. The callee's
internal behavior is represented by those elements that change over the course of its
execution, and which are not part of the interface with the caller. At return, those
elements should become irrelevant.

Similarly, in \(\clei\), only elements marked \(\object\) at the call should influence the
behavior of the callee. This is the same principle as \(\clrc\), extended from \(\sealed\)
elements to all non-\(\object\) ones.

\section{Formalization}
\label{sec:machine}

% APT: I don't think we need this.
%% \begin{figure}
%%   \begin{tabular}{| l | l | l |}
%%     \hline
%%     Set / & Names & Purpose \\
%%     Class & & \\
%%     \hline
%%     \(\mathit{CLR}\) / & {\tt t0} -- {\tt t6} & Caller-saved temps \\
%%     \(\unsealed\) & {\tt a0} -- {\tt a1} & Caller-saved args / return vals \\
%%     & {\tt a2} -- {\tt a7} & Caller-saved args \\
%%     & {\tt ra} & Return Address \\
%%     \hline
%%     \(\mathit{CLE}\) / & {\tt s0} -- {\tt s11} & Callee-saved \\
%%     \(\sealed\) & {\tt sp} & Stack Pointer \\
%%     \hline
%%     \(\mathit{PUBLIC}\) / & {\tt gp} & Global Pointer  \\
%%     \(\public\) & {\tt tp} & Thread Pointer \\
%%     \hline
%%   \end{tabular}
%%   \caption{RISC-V integer register set}
%%   \label{fig:RISCVregs}
%% \end{figure}

We now give a formal description of our machine model, security semantics,
and properties. Our definitions abstract over the details of the target machine
architecture and ABI, the set of security-relevant operations, the rules that
govern evolution of the security context, the set of observable events, and
a notion of value compatibility.
\apt{Not quite true: the property definitions require a notion of
  call entry.}
\apt{Any more? Give a more listlike presentation?
  Need to clarify exactly what the parameters are, both in the initial
  presentation and in the various extensions, including the appendix.}

\subsection{Machine}
The building blocks of a machine are {\em words} and {\em registers}.
Words are ranged over by \(\word\) and, when used as addresses, \(\addr\),
and are drawn from the set \(\WORDS\).
Registers in the set \(\REGS\) are ranged over by \(\reg\), with the stack pointer
given the special name \(\SP\);
some registers may be classified as caller-saved or callee-saved.
Along with the program counter, \(\PCname\), these are referred to as
{\em state elements} \(\component\) in the set \(\COMPONENTS ::= \PCname | \WORDS | \REGS\).

A {\em machine state} \(\mach \in \MACHS\) is a map from state elements to a set \(\mathcal{V}\) of
\emph{values}.
Each value \(v\) contains a \emph{payload} word, written \(|v|\).
We write \(\mach[\component]\) to denote the value of \(\mach\) at
\(\component\)  and \(\mach[v]\) as shorthand for \(\mach[|v|]\).
The details of value structure depend on the specific machine being modelled;
intuitively, the payload represents the part of the value that is relevant to
the behavior of the basic machine, while the rest of the value may contain
information relevant to a hardware enforcement mechanism (such as a tag).
We assume a given \emph{compatibility} equivalence relation \(\sim\) on values,
and lift it element-wise to states.
Values are compatible if their non-payload information is identical, 
even though their payloads may differ.
\apt{This is still not very convincing!
  empting to say that only the payloads can affect the step function, but that is
  too strong, right? (OK for tags, but not, e.g. capabilities?}

The machine has a step function \(\mach \xrightarrow{\bar{\psi},\obs} \mach'\).
Except for the annotations over the arrow, this function just encodes the usual
ISA description of the machine's instruction set. The annotations serve to connect
the machine's operation to our security setting: 
\(\bar{\psi}\) is a list of security-relevant operations drawn from an assumed set \(\Psi\),
and \(\obs\) is an (potentially silent) observable event; these are described further below.

\subsection{Security semantics}

The security semantics operates in parallel with the 
Each state element (memory word or register) is given a \emph{security class} 
\(l \in \{\public, \object, \sealed, \unsealed\}\).
A \emph{view} \(V \in \mathit{VIEW}\) maps elements to security classes.
For any security class \(l\), we write \(l(V)\)
to denote the set of elements \(\component\) such that \(V ~ \component = l\).
The {\it initial view} \(V_0\) maps all stack locations to \(\unsealed\),
all other locations to \(\public\), and registers based on which set they
belong to: \(\sealed\) for callee-saved, \(\unsealed\) for caller-saved, and \(\public\) otherwise.
\apt{This doesn't quite match example, where there is magically an initial argument register labeled \(\active\).}

A (security) \emph{context} is
a pair of the current activation's view and
a list of views representing the call stack (pending inactive
principals), ranged over by \(\sigma\).
%
\[\context \in \CONTEXTS ::= \mathit{VIEW \times list ~ VIEW}\]
%
The initial context is \(\context_0 = (V_0, \emplist)\).

\cref{sec:example} describes informally how the security context evolves as the system performs
security-relevant operations. Formally, we combine each machine state with a context
to create a {\it combined state} \(s = (\mach,\context)\) and a lift the transition
to \(\stepstounder{}\) on combined states. 
At each step, the context updates based on the function
\(Op : \MACHS \rightarrow \CONTEXTS \rightarrow \psi \rightarrow \CONTEXTS\).
(Note that \(Op\) takes as its first argument the state {\it before} the step.)
Since a single step might correspond to multiple operations, we apply
\(Op\) as many times as needed, using \(\mathit{foldl}\).

\judgmenttwo{\(\mach \xrightarrow{\overline{\psi},\obs} \mach' \)}
            {\(\mathit{foldl} ~ (Op ~ \mach) ~ \context ~ \overline{\psi} = \context'\)}
            {\((\mach,\context) \stepstounder{\overline{\psi},\obs} (\mach', \context')\)}

A definition of \(Op\) is most convenient to present decomposed into
rules for each operation. We have already seen the intuition behind the rules for
\(\mathbf{alloc}\), \(\mathbf{call}\), and \(\mathbf{ret}\).
For the machine described in the example, the \(Op\) rules would be those
found in \cref{fig:basicops}.

\begin{figure}
    \[\mathit{range} ~ \reg ~ \mathit{off} ~ \mathit{sz} ~ \mach \triangleq
    \{\mach[\reg]+i | \mathit{off} \leq i < \mathit{off+sz}\}\]

  \judgmentbr{\(\components = \mathit{range} ~ \SP ~ \mathit{off} ~ \mathit{sz} ~ \mach \cap
    \{\addr \mid V ~ \addr = \unsealed\}\)}
             {\(V' = V \llbracket \addr \mapsto \sealed \mid \addr \in \components \rrbracket\)}
             {\(Op ~ \mach ~ (\mathbf{alloc} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}
    %
    \judgmentbr{\(b = \mach[\SP] + \mathit{off}\)}
               {\(V' = V \llbracket \addr \mapsto \unsealed |
                 b \leq a < b+\mathit{sz} \land V ~ \addr = \object \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{dealloc} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}
    %
    \judgmentbr{\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
                 \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
               {\(V'' = V'\llbracket \addr \mapsto \sealed | V' ~ \addr = \object \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}})
                 ~ (V,\sigma) = (V'',V::\sigma)\)}
    %
    \judgment{\(\sigma = (V,\addr_{ret},\addr_{sp})::\sigma'\)}
             {\(Op ~ \mach ~ \mathbf{return} ~ (\_, \sigma) = (V, \sigma')\)}
  \caption{Basic Operations}
  \label{fig:basicops}
\end{figure}

\subsection{Events and Traces}
\label{sec:events}

We abstract over the events that can be observed in the system, defining them
only as a set \(\OBSS\) that contains at least the element \(\tau\), the silent
event. Other events might represent certain function calls (i.e., system calls)
or writes to special addresses representing mmapped regions.
A {\em trace} is a nonempty, finite or infinite sequence
of events, ranged over by \(\obsT\).
We use ``\(\notfinished{}{}\)'' to represent ``cons'' for traces, reserving ``::''
for list-cons.

We are particular interested in traces that end just after a function returns.
We define these in terms of the depth \(d\) of the security context's call stack \(\sigma\).
We write \(d \hookrightarrow s\) for the trace of execution from a state \(s\)
up to the first point where the stack depth is smaller than \(d\), defined
coinductively by these rules:

\judgment{\(|\sigma| < d\)}
         {\(d \hookrightarrow (\mach,(V,\sigma)) = \tau\)}

\judgmentthree{\((\mach,(V,\sigma)) \stepstounder{} (\mach',\context',\obs)\)}
              {\(|\sigma| \geq d\)}
              {\(d \hookrightarrow (\mach',\context') = \obsT\)}
              {\(d \hookrightarrow (\mach,(V,\sigma)) = \notfinished{\obs}{\obsT}\)}

\noindent
When \(d = 0\), the trace will always be infinite; in this case we
omit \(d\) and just write \(\hookrightarrow s\).

Two event traces $\obsT_1$ and $\obsT_2$ are {\em similar},
written \(\obsT_1 \eqsim \obsT_2\), if the sequence of non-silent events
is the same. That is, we compare up to deletion of \(\tau\) events.
\apt{Didn't we use to say something about the (arguably) odd nature of this definition?}

\begin{minipage}{.4\columnwidth}
  \judgment{}{\(\obsT \eqsim \obsT\)}
\end{minipage}
\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\notfinished{\obs}{\obsT_1} \eqsim \notfinished{\obs}{\obsT_2}\)}
\end{minipage}

\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\notfinished{\tau}{\obsT_1} \eqsim \obsT_2\)}
\end{minipage}
\begin{minipage}{.4\columnwidth}
  \judgment{\(\obsT_1 \eqsim \obsT_2\)}
           {\(\obsT_1 \eqsim \notfinished{\tau}{\obsT_2}\)}
\end{minipage}

\subsection{Variants, corrupted sets, and ``on-return'' assertions}
\label{sec:props}

Two (compatible) states are variants with respect to a set of elements \(\components\)
if they agree on the value of every element not in \(\components\). 
Our notion of non-interference involves comparing the traces of such
\(\components\)-variants. We use this to define sets of irrelevant elements,
and secrets in general.\apt{last phrase is cryptic}

\definition The \emph{difference set} of two machine states \(\mach\) and \(\mach'\), written \(\Delta(\mach,\mach')\),
is the set of elements \(\component\) such that \(\mach[\component] \not = \mach'[\component]\).

\definition Machine states \(\mach\) and \(\nach\) are {\em \(\components\)-variants},
written \(\mach \approx_\components \nach\), if \(\mach\sim\nach\) and
\(\Delta(\mach,\nach) \subseteq \components\). 
%if all \(\component \not \in \components\), \(\mach[\component] = \nach[\component]\)
%and for all \(\component \in \components\), \(\mach[\component] \sim \nach[\component]\).

\definition An element set \(\components\) is \emph{irrelevant} to state \((\mach,\context)\),
written \((\mach,\context) \parallel \components\), if for all
\(\nach\) such that \(\mach \approx_{\components} \nach\),
\(\hookrightarrow (\mach,\context)  \eqsim \hookrightarrow (\nach,\context)\).

\definition The {\em corrupted set} \(\bar{\Diamond}(\mach,\mach',\nach,\nach')\)
is the set \((\Delta(\mach,\mach') \cup \Delta(\nach,\nach')) \cap \Delta(\mach',\nach')\).

If we consider two execution sequences, one from \(\mach\) to \(\mach'\)
and the other from \(\nach\) to \(\nach'\),
then \(\bar{\Diamond}(\mach,\mach',\nach,\nach')\) is the set of elements that
change in one or both executions and end up with different values. Intuitively,
this captures the effect of any differences between \(\mach\) and \(\nach\), i.e.,
the set of values that are ``corrupted'' by those differences.

Our ``on-return'' assertions are defined using a second-order logical operator
\(d \uparrow P\), pronounced ``\(P\) holds on return from depth \(d\),''
where \(P\) is a predicate on machine states. This is a coinductive relation
similar to ``weak until'' in temporal logic---it also holds if the program never
returns from depth \(d\).

\judgmenttwo[Returned]
            {\(|\sigma| < d\)}
            {\(P ~ \mach\)}
            {\((d \uparrow P) ~ (\mach, (V,\sigma))\)}

\judgmenttwobrlong[Step]
                  {\(|\sigma| \geq d\)}
                  {\((d \uparrow P) ~ (\mach', \context')\)}
                  {\((\mach, (V,\sigma)) \stepstounder{\overline{\psi},\obs} (\mach', \context')\)}
                  {\((d \uparrow P) ~ (\mach, (V,\sigma))\)}

Similarly, we give a binary equivalent for use in confidentiality. We define \(\Uparrow\) so that
\((\mach,\context) ~ (d \Uparrow R) ~ (\mach',\context')\) holds if \(R\) holds on the
first states that return from depth \(d\) after \((\mach,\context)\) and \((\mach',\context')\),
respectively. Once again, \(\Uparrow\) is coinductive.

\judgmentthree[Returned]
              {\(|\sigma_1| < d\)}
              {\(|\sigma_2| < d\)}
              {\(\mach_1 ~ R ~ \mach_2\)}
              {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\judgmenttwobrlong[Left]
                  {\(|\sigma_1| \geq d\)}
                  {\((\mach_1,(V_1,\sigma_1)) \stepstounder{\overline{\psi},\obs} (\mach_1',\context_1')\)}
                  {\((\mach_1',\context_1') ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}
                  {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

\judgmenttwobrlong[Right]
                  {\(|\sigma_2| \geq d\)}
                  {\((\mach_2,(V_2,\sigma_2)) \stepstounder{\overline{\psi},\obs} (\mach_2',\context_2')\)}
                  {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2',\context_2')\)}
                  {\((\mach_1,(V_1,\sigma_1)) ~ (d \Uparrow R) ~ (\mach_2,(V_2,\sigma_2))\)}

Finally, we can define our core properties. They can be found in \cref{tab:props},
arranged to show their commonalities and distinctions. Each definition gives a criterion
quantified over states \(s\) that immediately follow call (or tail-call) steps.
If an execution can reach a state \(s'\) such that \(s' \stepstounder{\overline{\psi}} s\)
where \(\mathbf{call} ~ \addr ~ \overline{\reg} \in \bar{\psi}\), then \(s\) is the target
of a call.
%Likewise, if \(\mathbf{tailcall} ~ \addr ~ \overline{\reg} \in \bar{\psi}\), then
%\(s\) is the target of a tailcall.
As a shorthand, we write that each property is defined
by a criterion that must hold ``for all call targets \(s\).''

\(\wbcf\), \(\clri\), and \(\clec\) are each straightforward: they define predicates that must
hold on return from the call. \(\clrc\) and \(\clei\) both compare the execution of the
call state with that of a variant state, and are concerned with both their internal and
their return-time behavior. So, for some \(\nach\) that is an appropriate variant,
all states must fulfill an ``internal'' clause that their traces are similar,
and a ``return-time'' clause, that any potentially corrupted memory at their returns
is irrelevant.
\apt{This paragraph needs major expansion, to paraphrase the definitions informally,
  and TO SAY MUCH MORE ABOUT THE CALLEE PROPERTIES, which are not intuitive at all (to me)!}

\begin{table*}[h]
  \setlength{\tabcolsep}{1pt}
  \center
  \begin{tabular}{l r l l l}
    \rowcolor{black!20}
    1
    & \(\wbcf \triangleq\) & \((|\sigma| \uparrow \ret) ~ ????\)
    & \(\textnormal{ where } \ret ~ \mach' \triangleq \)
    \(\mach'[\SP] = \mach[\SP] \land \mach'[\PCname] = \mach[\PCname]+4\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
    %
    \rowcolor{black!10}
    2
    & \(\clri \triangleq\) & \((|\sigma| \uparrow \intProp) ~ (\mach,(V,\sigma))\)
    & \(\textnormal{ where } \intProp ~ \mach' \triangleq
    \mach' \parallel (\sealed(V) \cap \Delta(\mach,\mach'))\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
    %
    \rowcolor{black!20}
    3
    & \(\clrc \triangleq\) & \(\forall \nach \textnormal{ s.t. } \mach \approx_{\components} \nach,\)
    & \(\textnormal{ where } \components = \sealed(V)\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is (tail)called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
    \rowcolor{black!20}
    3a & & \(|\sigma| \hookrightarrow (\mach,(V,\sigma)) \simeq |\sigma| \hookrightarrow (\nach,(V,\sigma))\) & & \\
    \rowcolor{black!20}
    3b & & \(\textnormal{ and } (\mach,(V,\sigma)) ~ (|\sigma| \Uparrow \confProp) ~ (\nach,(V,\sigma))\)
    & \(\textnormal{ where } (\mach' ~ \confProp ~ \nach') \triangleq
    \mach' \parallel \bar{\Diamond}(\mach,\nach,\mach',\nach')\) & \\
    %
    \rowcolor{black!10}
    4
    & \(\clec \triangleq\) & \((|\sigma| \uparrow \cconfProp) ~ (\mach,(V,\sigma))\)
    & \(\textnormal{ where } \cconfProp ~ \mach' \triangleq
    \mach' \parallel (\Delta(\mach,\mach') - \components)\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is (tail)called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
    \rowcolor{black!10}
    & & & \(\textnormal{ where } \components = \public(V) \cup \object(V)\) & \\
    %
    \rowcolor{black!20}
    5
    & \(\clei \triangleq\) & \(\forall \nach \textnormal{ s.t. } \mach \approx_{\components} \nach,\)
    & \(\textnormal{ where } \components = \public(V) \cup \object(V)\)
%    & \(\textnormal{ when } (\mach,(V,\sigma)) \textnormal{ is (tail)called}\) \\
    & \(\textnormal{ for all call targets } (\mach,(V,\sigma))\) \\
   \rowcolor{black!20}
    5a & & \(|\sigma| \hookrightarrow (\mach,(V,\sigma)) \simeq |\sigma| \hookrightarrow (\nach,(V,\sigma))\) & & \\
    \rowcolor{black!20}
    5b & & \(\textnormal{ and } (\mach,(V,\sigma)) ~ (|\sigma| \Uparrow \cintProp) ~ (\nach,(V,\sigma))\)
    & \(\textnormal{ where } (\mach' ~ \cintProp ~ \nach') \triangleq
    \mach' \parallel \bar{\Diamond}(\mach,\nach,\mach',\nach')\) & \\
  \end{tabular}
  \caption{Properties\apt{In WBCF, surely we need a state arg? But it needs to be the state \emph{before} the call,
      doesn't it?}}
  \label{tab:props}
\end{table*}

\section{Expanding the Machine}

The system we've modeled so far has been very simple, but our properties
support an important dimension of generality: adding features.\apt{Just what needs to be changed
to add these features?}
We now briefly introduce the features of our primary testing machine.\apt{Make sure
testing has been introduced properly before now. And why ``primary''?}

The full machine defines additional operations, and extends some of the old ones.
We add a field \(\overline{sa}\) to the \(\mathbf{call}\) operation: a set of
triples of a register \(\reg\), a base offset \(\mathit{off}\), and a size \(sz\), denoting
that the value at the offset \(\mathit{off}\) from \(\reg\) is to be passed as an argument.
The new \(\mathbf{tailcall}\) operation has the same fields, but deals with tail-call
optimizations in which the callee will reuse the caller's stack. The \(\mathbf{alloc}\) operation
gains a boolen flag indicating whether the allocation is to be public, accessible to
all functions until deallocated. The rules are given in \cref{fig:advops}; the
rules in \cref{fig:basicops} can be recaptured by instantiating
\(\mathbf{call}\) with \(\overline{sa}\) as the empty set, and \(\mathbf{alloc}\)
with flag \(\mathbf{f}\).

\subsection{Sharing Stack Memory}
In our examples, we have presented a vision of stack safety in which
the interface between caller and callee is in the registers that pass
arguments and return values. This is frequently not the case in a realistic
setting. Arguments may be passed on the stack (e.g. because there are too many
to pass in registers), as an implementation
of variadic arguments, or because they are reference objects \apt{??} that inherently have
pass-by-reference semantics. The caller may also take the address of a local object and
pass that as a pointer to its callee.

We refine our call operation to make use of the information that we have about
which memory contain arguments, \(\overline{sa}\). \(\overline{sa}\) is a set of
triples of a register, an offset from the value of that register, and a size.
We first define the helpful set \(\mathit{passed} ~ \overline{sa} ~ \mach\),
then extend the call operation to keep all objects in \(\mathit{passed}\) marked
as \(\object\) and seal everything else (\cref{sfig:stkargs}).

Using this mechanism, we can describe an argument being passed on the stack
by giving its \(\SP\)-relative offset,
with the operation \(\mathbf{call} ~ \dots ~ (\SP, \mathit{off}, \mathit{sz})\).
In the case of pass-by-reference semantics, if the compiler designates register
\(\reg\) as holding the reference, then the operations is
\(\mathbf{call} ~ \dots ~ (\reg, 0, \mathit{sz})\).\apt{This is unclear to me. 
  The compiler knows where the cbr parameter lives in the caller's stack
  frame, so why wouldn't it use a sp-relative spec for that too? Does it have
  to do with passing a CBR parameter further down to another callee as you allude
  to in the next para?}

In both cases, the immediate callee retains access to the passed object, but deeper
callees will not---unless the object is passed-by-reference further down the stack.
On the other hand, in the case of a caller explicitly taking the address of an
object and passing it as a pointer, we have no way obvious way to limit the scope
of such sharing.

Our primary \apt{again...} machine treats this scenario as the caller allocating a ``public''
object that can be accessed by anyone until it's deallocated. We extend the
\(\mathbf{alloc}\) operation with a boolean flag, where {\bf t} indicates
that the allocation is public, and {\bf f} is private.
In the likely event that the space for multiple objects is allocated in a single step,
that step can make multiple allocation operations, each labeled appropriately.
Allocating public objects affects the call stack very similarly to private ones,
but instead of labeling them \(\object\) they become \(\public\), so they are
never sealed at a call (\cref{sfig:publicalloc}).

\begin{figure*}
  \begin{subfigure}{0.4\textwidth}
    \[\mathit{range} ~ \reg ~ \mathit{off} ~ \mathit{sz} ~ \mach \triangleq
    \{\mach[\reg]+i | \mathit{off} \leq i < \mathit{off+sz}\}\]

    \judgmentbr{\(\components = \mathit{range} ~ \SP ~ \mathit{off} ~ \mathit{sz} ~ \mach \cap
                 \{\addr \mid V ~ \addr = \unsealed\}\)}
               {\(V' = V \llbracket \addr \mapsto \sealed \mid \addr \in \components \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{alloc} ~ \mathbf{f} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}

    \judgmentbr{\(\components = \mathit{range} ~ \SP ~ \mathit{off} ~ \mathit{sz} ~ \mach \cap
                 \{\addr \mid V ~ \addr = \unsealed\}\)}
               {\(V' = V \llbracket \addr \mapsto \public \mid \addr \in \components \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{alloc} ~ \mathbf{t} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}
    %
    \judgmentbr{\(b = \mach[\SP] + \mathit{off}\)}
               {\(V' = V \llbracket \addr \mapsto \unsealed |
                 b \leq a < b+\mathit{sz} \land V ~ \addr = \object \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{dealloc} ~ \mathit{off, sz}) ~ (V,\sigma) = (V',\sigma)\)}

    \caption{Memory Allocation}
    \label{sfig:publicalloc}
  \end{subfigure}
  \begin{subfigure}{0.6\textwidth}
    %
    \[\mathit{passed} ~ \overline{sa} ~ \mach = \bigcup_{(\reg,\mathit{off},\mathit{sz}) \in \overline{sa}}
    \mathit{range} ~ \reg ~ \mathit{off} ~ \mathit{sz} ~ \mach\]
    %
    \judgmentbr{\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
                 \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
               {\(V'' = V'\llbracket \addr \mapsto \sealed | V' ~ \addr = \object \land \addr \not \in (\mathit{passed} ~ \overline{sa} ~ \mach) \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa})
                 ~ (V,\sigma) = (V'',V::\sigma)\)}
    %
    \judgmentbr{\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
                 \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
               {\(V'' = V'\llbracket \addr \mapsto \unsealed | V' ~ \addr = \object \land \addr \not \in (\mathit{passed} ~ \overline{sa} ~ \mach) \rrbracket\)}
               {\(Op ~ \mach ~ (\mathbf{tailcall} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa})
                 ~ (V,\sigma) = (V',\sigma)\)}
    %
    \judgment{\(\sigma = (V,\addr_{ret},\addr_{sp})::\sigma'\)}
             {\(Op ~ \mach ~ \mathbf{return} ~ (\_, \sigma) = (V, \sigma')\)}

    \caption{Calls with Argument Passing on the Stack}
    \label{sfig:stkargs}
  \end{subfigure}
  \caption{Operations supporting tailcalls and argument passing on stack.}
  \label{fig:advops}
\end{figure*}

\subsection{Tail Calls}

The operation rule for a tailcall is similar to that for a normal call.
We do not push the caller's view onto the stack,
but replace it outright. This means that a tailcall does not increase the size of
the call stack, and therefore for purposes of our properties, all tailcalls will
be considered to return simultaneously when the eventual {\bf return} operation
pops the top of the stack.

Since the caller will not be returned to, it does not need integrity, though
it should still enjoy confidentiality. We set its frame to \(\unsealed\) rather
than \(\sealed\) to express this. Of course, some objects may be passed as arguments
in place\apt{??}, easily modeled by including them in \(\overline{sa}\). More sophisticated
behaviors may require sequences of {\bf dealloc}, {\bf tailcall}, then {\bf alloc}.

\section{Enforcement}

\bcp{Bits missing here!}

\paragraph{Depth Isolation}

\paragraph{Lazy Tagging and Clearing}

\subsection{Enforcing \(\clei\) and \(\clec\)}

\section{Validation through Random Testing}
\label{sec:testing}

There are several ways to evaluate whether an enforcement mechanism enforces the above
stack safety properties. Ideally such validation would be done through formal proof over
the semantics of the enforcement-augmented machine.
However, while there are no fundamental barriers to producing such a proof,
it would be considerable work to carry out for a full ISA like RISC-V and
complex enforcement mechanisms like Roessler and DeHon's micro-policies.
We therefore choose to systematically \emph{test} their micro-policies.
\leo{moved the following here, I think it makes more sense here.}
Our primary testing targets are the eager {\em Depth Isolation}
and the {\em Lazy Per-Activation Tagging and Clearing} micro-policies.


We use a Coq specification of the RISC-V architecture~\cite{Bourgeat2021AMF},
and extend it with a runtime monitor implementing a stack safety
micro-policy. We choose the Coq proof assistant as the setting for our implementation
to ensure that our coinductive trace definitions are well-formed, to reason about
them\leo{where do we mention reasoning about them?}, and to leverage the power of the QuickChick property-based testing framework~\cite{Pierce:SF4}.

\subsection{Test Generation}
To use QuickChick, we developed random test-case generators that produce
(1) an initial RISC-V machine state, including the program to be executed;
(2)
  an initial policy state, tagging instructions and the stack regions appropriately
  for the micro-policy (see \cref{subsec:policies})
(3)
  a map from instructions to lists of their operations in the security semantics.

To write such generators we build on the work of Hri\c{t}cu et
al. \cite{TestingNI:ICFP, DBLP:journals/jfp/HritcuLSADHPV16}, which
introduced {\em generation by execution}, a technique that produces
programs that lead to longer executions---and hopefully towards more
interesting behaviors as a result. We extend their technique with an additional
degree of statefulness: when complex calling conventions need to be
observed to avoid early failstops, extra care must be taken to ensure
that any such calls are well behaved. For example, immediately
following a call we need to initialize any stack-allocated variables
with an increased probability. However, at the same time, we need to
allow for the possibility of {\em not} performing such an initialization
to allow for potential attack vectors to manifest.

Moreover, we need to further extend typical hyperproperty testing
schemes to handle the nested nature of confidentiality: rather than
just generating two initial machines that are variants of one another
and letting them execute to test for noninterference, we generate a
new variant {\em every time a call is made} and check confidentiality
for the subtrace produced from that variant state until its
corresponding return. As a result, a ``single'' confidentiality test
compactly checks multiple nested calls.


\subsection{Mutation Testing}

To ensure the effectiveness of testing against our formal properties, we
use {\em mutation testing}~\cite{JiaH11}. In mutation testing, we inject errors
(mutations) in a program that should cause the property of interest (here,
stack safety) to fail, and ensure that the testing framework can find
them. The bugs we use for our evaluation are either artificially generated
by us (deliberately weakening the micro-policy in ways that we expect
should break its guarantees), or actual bugs that we discovered through
testing our implementation. We elaborate on some such bugs below.

For example, when loading from a stack location, {\em Depth Isolation}
needs to enforce that the tag on the location being read
is $\tagStackDepth{n}$ for some number $n$ and that the tag of the
current $\PCname$ is $\tagPCDepth{n}$ for the same depth $n$. We can relax
that restriction by not checking the depth equality (row {\em
  LOAD\_NO\_CHECK\_DI}).

Similarly, when storing to a stack location, the correct micro-policy
needs to ensure that the tag on the memory location is either
$\tagNoDepth$ or has again the same depth as the current $\PCname$
tag. Relaxing that constraint causes violations to the integrity
property (row {\em STORE\_NO\_CHECK}).

\begin{table}[]
\centering
\begin{tabular}{c|c|c|c}
  Bug & Property Violated & MTTF (s) & Tests \\
  \hline
      {\em LOAD\_NO\_CHECK\_DI}  & Confidentiality & 24.2 & 13.3 \\
      {\em STORE\_NO\_CHECK} & Integrity & 26.9 & 26 \\
      {\em HEADER\_NO\_INIT} & Integrity & 69.5 & 76.3 \\
  \hline
  \hline
      {\em PER\_DEPTH\_TAG} & Obs. Integrity & 189.7 & 8342.5  \\
      {\em LOAD\_NO\_CHECK\_LT}  & Obs. Integrity & 23.5 & 12.0 \\
      {\em LOAD\_NO\_CHECK\_LT}  & Confidentiality & 19.2 & 695.5 \\
      {\em STORE\_NO\_UPDATE} & Obs. Integrity & 70 & 80.6  \\
      {\em STORE\_NO\_UPDATE} & Confidentiality & 4.9 & 88.5 \\
  \hline
\end{tabular}
\vspace*{1em}
\caption{MTTF for finding bugs in erroneous policy enforcement mechanisms}
\vspace*{-2em}
\label{tab:bug-table}
\end{table}

\subsection{Results}

The mean-time-to-failure (MTTF) and average number of tests for various bugs can be found in
Table~\ref{tab:bug-table}, along with the average number of tests
it took to find the failure. Experiments were run in a desktop
machine equipped with i7-4790K CPU @ 4.0GHz with 32GB RAM.

Naturally, testing also revealed a number of errors in our
implementation of the enforcement mechanism (the original was written in C++
and targeted ARM machine code;
%\bcp{right?}\leo{yeah}
we re-implemented it in Coq targeting RISC-V).  These errors range
from trivial typos to ones that require an intriguingly complex setup
to provoke.  The most interesting bug (included in the table as row
{\em HEADER\_NO\_INIT}) was that, on our first try, the blessed call
sequence %/policy combination\apt{??}
did not initialize all locations for the
newly allocated stack frame correctly, but left some of them as
$\tagNoDepth$. This allowed for a potential integrity violation, but
only if a rather complicated sequence of events occured.
The smallest counterexample requires calling a function {\tt f},
which fails to initialize some of its frame,
but writes into an uninitialized location $l$ later, treating \(l\) as outside
the stack. Then {\tt f} calls a further function {\tt g} (which should have
the effect of sealing $l$ for integrity purposes). {\tt g} attempts to write to $l$,
which is allowed because the enforcement mechanism still has
$l$ tagged as $\tagNoDepth$, but violates the integrity property on {\tt f}'s data.
%\sna{I believe what went wrong was that we were off-by-one in {\tt main}'s initialization,
%  and the write from {\tt f} was already a violation.}

As for \(LTC\), the original micro-policy, implemented as {\em PER\_DEPTH\_TAG},
fails in testing, in cases where data is leaked between sequential calls.
To round out our mutation testing we also check {\em LOAD\_NO\_CHECK\_LT},
equivalent to its counterpart in depth isolation,
and a version where stores succeed but fail to propagate the PC tag, {\em STORE\_NO\_UPDATE}.
It turns out that {\em PER\_DEPTH\_TAG} is a comparatively subtle bug,
taking twice as long to catch as the next longest.

Our properties have allowed us to identify an enforcement mechanism as
not really stack safe, and to validate a possible fix.

\section{Related Work}

\sna{Not really sure what goes here anymore.
  We've discussed Cerise and Nick and Andre quite a bit throughout.}

\section{Future Work}
\label{sec:future}

\subsection{Testing The Stack Safety Menagerie}

In \cref{sec:intro}, we discussed several dimensions of flexibility
that our properties should exhibit: applicability to different system
features and different security requirements. Another important dimension
is applicability to multiple enforcement mechanisms.
In the following, we discuss how different existing mechanisms
might fit into our model.

\paragraph{Stack Canaries and Shadow Stacks}
%
Canaries are special values inserted by the caller at certain points of the
stack and checked for corruption on return, for protection against
some classes of control-flow hijacking (i.e., overwriting the caller's
return address through a buffer overflow that would also overwrite the canary
detectably). A shadow stack is a mirror image of the control-relevant parts of the stack,
maintained by the program to attempt to detect attempts to hijack its control
flow and ``restrict the flexibility available in creating gadget chains''
\cite{Shanbhogue+19}.
%
Interestingly, these are lazy enforcement mechanisms, in that
the attack may occur and be detected some time later, as long as
it is detected before it can become dangerous. That would make our
formalism a good fit for defining their security, except that
they are chiefly hardening techniques: they increase the difficulty
of some control-flow attacks on the stack, but cannot provide absolute
guarantees on \(\wbcf\) under a normal attacker model.

\paragraph{Split Stacks}
%
Code-pointer integrity seeks to prevent control-flow hijacking attacks by
splitting the program memory, including the stack, into a regular region and a
safe region; objects that require protection are placed in the latter and
all accesses to them protected by static or dynamic checks.
%
Like shadow stacks, bounds checking is a high-level technique designed to
protect code written in a high-level language, but it is designed to
offer protection under a stronger threat model that also considers the presence
of untrusted code. As with most other software-based methods, it purports to
protect exclusively the control-relevant parts of the stack, making no claims
about its data-relevant parts---we would solely be testing \(\wbcf\).

\paragraph{Bounds Checking}
%
Under a bounds checking discipline such as SoftBound \cite{NagarakatteZMZ09}, all the pointers
in a program are extended with some disjoint metadata, and these are combined
to gate memory accesses. These approaches enforce a form of \emph{memory safety},
and we would therefore expect them to enforce \(\clri\) and \(\clrc\). They aim
to enforce \(\wbcf\) by cutting off attacks that involve memory-safety violations,
but that may not be sufficient.

Bounds checking approaches require substantial compiler cooperation. This is not a
problem for our properties in general, but it is not very compatible with
generation-by-execution of low-level code. A better choice would be to generate
high-level code using a tool like CSmith \cite{DBLP:conf/pldi/YangCER11}, or prove the properties instead.

\paragraph{Capabilities}

Capability machines can represent and manipulate
unforgeable tokens of authority over specific regions of
memory using a combination of dedicated out-of-band memory and
specialized ISA instructions.
%
The most prominent contemporary representative of this long line of work is
CHERI \cite{DBLP:conf/sp/WatsonWNMACDDGL15}, a modern architecture designed to provide efficient fine-grained
memory protection and compartmentalization.
%
Previous work has uses these principles in simplified models to implement
\emph{secure calling conventions} for the call
stack \cite{SkorstengaardLocal,SkorstengaardSTKJFP,Georges22:TempsDesCerises}. Those conventions can be seen as combining the
protections of bounds-checking with software-based methods of control-flow protection.

There are several proposals around the use of CHERI capabilities to enforce stack safety,
including mechanisms that use the standard CHERI hardware (which includes local
capabilities) \cite{SkorstengaardLocal},
and others that propose entirely new types of capabilities, such as linear
\cite{SkorstengaardSTK}, uninitialized \cite{Georges+21}, lifetime
\cite{Tsampas+19}, and directed \cite{Georges22:TempsDesCerises} capabilities.
It is a high priority to test this most recent work by Georges et al., as it is designed
to be similar to all of our properties except for \(\clei\).

\subsection{More Features}

Several common features did not make it into this work: most importantly concurrency and
exceptions. We plan to extend the model to include them, and then test them. We would also
like to test the model given in \cref{app:ptr} for arbitrary memory-safe pointer sharing.

\subsection{Proofs}

\bibliographystyle{IEEEtran}
\bibliography{bcp.bib,local.bib}

\appendix

\subsection{Provenance, Capabilities, and Protecting Objects}
\label{app:ptr}

What if we want to express a finer-grained notion of safety, in which
stack objects are protected unless the function that owns them intentionally
passes a pointer to them? This can be thought of as a {\it capability}-based
notion of security. Capabilities are unforgeable tokens that grant access to
a region of memory, typically corresponding to valid pointers to that region.
So, in order to express such a property, we need our machine to carry some notion
of {\it pointer provenance}---a distinction between a pointer that is intended to
point to a given object, and non-pointer integers as well as pointers to other objects.

One such provenance model is Memarian et al.'s PVI \cite{provenance}, in which pointers are
annotated with the identity of the object they first pointed to. This annotation is
propagated when the pointer is copied and when operations are performed on it---even
integer-only operations.

We can model this as a trio of additional security-relevant operations: one which
declares a register to contain a valid pointer, one which transmits the provenance
of a pointer from one element to another, and one which clears the provenance
(for instance, when a pointer is modified in place in a way that makes it invalid.)

In addition to the normal call stack, our security context will carry a map \(\rho\) from
elements to memory regions, represented as a base and a bound \(\context = (V, \sigma, \rho)\).
Existing operations are extended to keep the value of \(\rho\) the same, and the new operations
work as follows:

\judgmentbr[Promote]
           {\(\psi = \mathbf{promote} ~ \reg_{dst} ~ (\reg_{base},\mathit{off},\mathit{sz})\)}
           {\(\rho' = \rho[\reg_{dst} \mapsto \mathit{range} ~ \reg_{base} ~ \mathit{off} ~ \mathit{sz}]\)}
           {\(Op ~ \mach ~ \psi ~ (V,\sigma,\rho) = (V,\sigma,\rho')\)}

\judgmentbr[Propagate]
           {\(\psi = \mathbf{propagate} ~ \component_{src} ~ \component_{dst}\)}
           {\(\rho' = \rho[\component_{dst} \mapsto \rho[\component_{src}]]\)}
           {\(Op ~ \mach ~ \psi ~ (V,\sigma,\rho) = (V,\sigma,\rho')\)}

\judgment[Clear]
         {\(\psi = \mathbf{clear} ~ \component\)}
         {\(Op ~ \mach ~ \psi ~ (V,\sigma,\rho) = (V,\sigma,\rho[\component \mapsto \emptyset])\)}

We now have a notion of provenance, and must integrate it into the definition of
stack safety. We essentially generalize the above notion of passing: we will consider
a caller to have intentionally passed an object if that object is reachable by
a capability that has been passed to the callee. This includes capabilities passed
indirectly, by being stored in an object that is in turn passed. Formally, we call
this set \(\mathit{capped}\), and define it recursively:
%
\[\begin{split}
& \mathit{capped} ~ \components ~ \rho \triangleq \bigcup_{\component \in \components} \{\component\} \cup \mathit{capped} ~ \components' ~ \rho \textnormal{ where} \\
& \components' = \{\component' |\rho[\component] = (\mathit{base},\mathit{bound})
\land \mathit{base} \leq \component' < \mathit{bound}\} \\
\end{split}\]

We then tweak the call operation to seal only objects that are in \(\mathit{capped}\), or
the previously defined \(\mathit{passed}\).

\judgmentbrbrbr[]
               {\(\psi = \mathbf{call} ~ \addr_{target} ~ \overline{\reg_{args}} ~ \overline{sa}\)}
               {\(V' = V \llbracket \reg \mapsto \unsealed | \reg \in \mathit{CLR} \rrbracket
                 \llbracket \reg \mapsto \public | \reg \in \overline{\reg_{args}} \rrbracket\)}
               {\(\components = \{\addr | V' ~ \addr = \object \land \addr \not \in (\mathit{passed} ~ \overline{sa} ~ \mach) \cup (\mathit{capped} ~ \overline{\reg_{args}} ~ \rho) \}\)}
               {\(V'' = V'\llbracket \components \mapsto \sealed \rrbracket\)}
               {\(Op ~ \mach ~ \psi ~ (V,\sigma,\rho) =
                 (V'',V::\sigma,\rho)\)}

Note that we have a degree of monotonicity here. Once an object is sealed (because its
capability has not been passed to a callee), subsequent nested calls can never unseal it.
On the other hand, an object that is passed via a pointer may be passed on indefinitely.

\end{document}
